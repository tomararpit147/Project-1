{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomararpit147/Project-1/blob/main/Sample_ML_Submission_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vncDsAP0Gaoa"
      },
      "source": [
        "# **Project Name**    -\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beRrZCGUAJYm"
      },
      "source": [
        "##### **Project Type**    - EDA/Regression/Classification/Unsupervised\n",
        "##### **Contribution**    - Individual\n",
        "##### **Member** - Arpit Tomar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJNUwmbgGyua"
      },
      "source": [
        "# **Project Summary -**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6v_1wHtG2nS"
      },
      "source": [
        "This project aims to predict Yes Bank stock prices using machine learning models.\n",
        "Using historical monthly stock data from 2005-2020, we implement multiple regression\n",
        "algorithms including Linear Regression, Random Forest, XGBoost, and LSTM networks\n",
        "to forecast future stock prices based on historical patterns and engineered features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6K7xa23Elo4"
      },
      "source": [
        "# **GitHub Link -**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1o69JH3Eqqn"
      },
      "source": [
        "https://github.com/tomararpit147/Project-1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQaldy8SH6Dl"
      },
      "source": [
        "# **Problem Statement**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpeJGUA3kjGy"
      },
      "source": [
        "Predict Yes Bank's monthly closing stock prices using historical data and\n",
        "technical indicators to assist investors in making informed trading decisions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDgbUHAGgjLW"
      },
      "source": [
        "# **General Guidelines** : -  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      },
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_i_v8NEhb9l"
      },
      "source": [
        "# ***Let's Begin !***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhfV-JJviCcP"
      },
      "source": [
        "## ***1. Know Your Data***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3lxredqlCYt"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "outputs": [],
      "source": [
        "# Install CatBoost if not already installed\n",
        "!pip install catboost\n",
        "\n",
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import os\n",
        "import joblib\n",
        "import datetime\n",
        "import sys\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Preprocessing\n",
        "from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
        "\n",
        "# Machine Learning Models\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "# Advanced ML\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "# Deep Learning\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, GRU, Bidirectional\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# For time series analysis\n",
        "from statsmodels.tsa.stattools import adfuller, kpss\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "\n",
        "# Feature selection\n",
        "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Model explainability\n",
        "import shap\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "# Statistical tests\n",
        "from scipy import stats\n",
        "from scipy.stats import boxcox, normaltest, jarque_bera\n",
        "\n",
        "# Set style for better visualizations\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "sns.set_context(\"notebook\", font_scale=1.2)\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Display settings\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.float_format', lambda x: '%.2f' % x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"âœ… All libraries imported successfully!\")\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")"
      ],
      "metadata": {
        "id": "q0bmWlPb1gx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RnN4peoiCZX"
      },
      "source": [
        "### Dataset Loading"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "KIbmFjZj9PXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "outputs": [],
      "source": [
        "# Load Dataset\n",
        "df = pd.read_csv('data_YesBank_StockPrices.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x71ZqKXriCWQ"
      },
      "source": [
        "### Dataset First View"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "outputs": [],
      "source": [
        "# Dataset First Look\n",
        "print(\"First 5 rows of dataset:\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hBIi_osiCS2"
      },
      "source": [
        "### Dataset Rows & Columns count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "outputs": [],
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(\"Number of rows and columns in the dataset:\")\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlHwYmJAmNHm"
      },
      "source": [
        "### Dataset Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "outputs": [],
      "source": [
        "# Dataset Info\n",
        "print(\"Information about the dataset:\")\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35m5QtbWiB9F"
      },
      "source": [
        "#### Duplicate Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "outputs": [],
      "source": [
        "# Dataset Duplicate Value Count\n",
        "print(\"Number of duplicate values in the dataset:\")\n",
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoPl-ycgm1ru"
      },
      "source": [
        "#### Missing Values/Null Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "outputs": [],
      "source": [
        "# Missing Values/Null Values Count\n",
        "print(\"Number of missing values in each column:\")\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "outputs": [],
      "source": [
        "# Visualizing the missing values\n",
        "plt.figure(figsize=(10, 4))\n",
        "sns.heatmap(df.isnull(), yticklabels=False, cbar=True, cmap='viridis')\n",
        "plt.title('Missing Values Heatmap')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0kj-8xxnORC"
      },
      "source": [
        "### What did you know about your dataset?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfoNAAC-nUe_"
      },
      "source": [
        "The dataset contains monthly stock prices of Yes Bank from July 2005 to November 2020. It has 185 rows and 5 columns (Date, Open, High, Low, Close). All columns are numerical except Date. There are no missing values or duplicates, making it clean for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      },
      "source": [
        "## ***2. Understanding Your Variables***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "outputs": [],
      "source": [
        "# Dataset Columns\n",
        "print(\"Columns in the dataset:\")\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "outputs": [],
      "source": [
        "# Dataset Describe\n",
        "print(\"Dataset describe:\")\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBTbrJXOngz2"
      },
      "source": [
        "### Variables Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJV4KIxSnxay"
      },
      "source": [
        "1. **Date**: Month and year of stock price (MMM-YY format)\n",
        "2. **Open**: Opening price of the stock for the month\n",
        "3. **High**: Highest price during the month\n",
        "4. **Low**: Lowest price during the month\n",
        "5. **Close**: Closing price at the end of the month\n",
        "\n",
        "All prices are in Indian Rupees (INR)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3PMJOP6ngxN"
      },
      "source": [
        "### Check Unique Values for each variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "outputs": [],
      "source": [
        "# Check Unique Values for each variable.\n",
        "for column in df.columns:\n",
        "    unique_count = df[column].nunique()\n",
        "    print(f\"{column}: {unique_count} unique values\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert Date to datetime format\n",
        "df['Date'] = pd.to_datetime(df['Date'], format='%b-%y')\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Month_Name'] = df['Date'].dt.strftime('%B')"
      ],
      "metadata": {
        "id": "lq0zJTYo_aMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dauF4eBmngu3"
      },
      "source": [
        "## 3. ***Data Wrangling***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKJF3rekwFvQ"
      },
      "source": [
        "### Data Wrangling Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "outputs": [],
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "# Create additional features for better analysis\n",
        "df['Price_Range'] = df['High'] - df['Low']  # Daily volatility\n",
        "df['Avg_Price'] = (df['Open'] + df['High'] + df['Low'] + df['Close']) / 4  # Average price\n",
        "df['Open_Close_Change'] = ((df['Close'] - df['Open']) / df['Open']) * 100  # Daily return %\n",
        "df['High_Low_Ratio'] = df['High'] / df['Low']  # Volatility ratio\n",
        "df['Cumulative_Return'] = (df['Close'] / df['Close'].iloc[0] - 1) * 100  # Cumulative return from start\n",
        "\n",
        "# Create rolling statistics\n",
        "df['MA_12'] = df['Close'].rolling(window=12).mean()  # 12-month moving average\n",
        "df['Volatility'] = df['Close'].pct_change().rolling(window=12).std() * 100  # Annualized volatility\n",
        "\n",
        "print(\"Dataset after feature engineering:\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting Data Wrangling Process...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 1. Convert Date to datetime\n",
        "print(\"\\n1. Converting Date to datetime format...\")\n",
        "df['Date'] = pd.to_datetime(df['Date'], format='%b-%y')\n",
        "print(f\"   âœ… Date range: {df['Date'].min().strftime('%b-%Y')} to {df['Date'].max().strftime('%b-%Y')}\")\n",
        "\n",
        "# 2. Sort by date\n",
        "df = df.sort_values('Date').reset_index(drop=True)\n",
        "print(\"   âœ… Data sorted chronologically\")\n",
        "\n",
        "# 3. Extract time-based features\n",
        "print(\"\\n2. Creating time-based features...\")\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Quarter'] = df['Date'].dt.quarter\n",
        "df['Month_Name'] = df['Date'].dt.strftime('%B')\n",
        "df['Year_Month'] = df['Date'].dt.strftime('%Y-%m')\n",
        "print(\"   âœ… Year, Month, Quarter, Month_Name features created\")\n",
        "\n",
        "# 4. Create lag features (autoregressive components)\n",
        "print(\"\\n3. Creating lag features...\")\n",
        "lags = [1, 2, 3, 6, 12]\n",
        "for lag in lags:\n",
        "    df[f'Close_Lag_{lag}'] = df['Close'].shift(lag)\n",
        "    df[f'Open_Lag_{lag}'] = df['Open'].shift(lag)\n",
        "    df[f'High_Lag_{lag}'] = df['High'].shift(lag)\n",
        "    df[f'Low_Lag_{lag}'] = df['Low'].shift(lag)\n",
        "print(f\"   âœ… Created lag features for periods: {lags}\")\n",
        "\n",
        "# 5. Create rolling statistics\n",
        "print(\"\\n4. Creating rolling statistics...\")\n",
        "windows = [3, 6, 12]\n",
        "for window in windows:\n",
        "    # Moving averages\n",
        "    df[f'Close_MA_{window}'] = df['Close'].rolling(window=window).mean()\n",
        "    df[f'Close_MA_{window}_shift'] = df[f'Close_MA_{window}'].shift(1)\n",
        "\n",
        "    # Rolling standard deviation (volatility)\n",
        "    df[f'Close_Std_{window}'] = df['Close'].rolling(window=window).std()\n",
        "\n",
        "    # Rolling min and max\n",
        "    df[f'Close_Min_{window}'] = df['Close'].rolling(window=window).min()\n",
        "    df[f'Close_Max_{window}'] = df['Close'].rolling(window=window).max()\n",
        "\n",
        "    # Price range rolling statistics\n",
        "    df[f'Range_MA_{window}'] = (df['High'] - df['Low']).rolling(window=window).mean()\n",
        "print(f\"   âœ… Created rolling statistics for windows: {windows}\")\n",
        "\n",
        "# 6. Create price-based features\n",
        "print(\"\\n5. Creating price-based features...\")\n",
        "df['Price_Range'] = df['High'] - df['Low']\n",
        "df['Price_Range_Pct'] = (df['Price_Range'] / df['Low']) * 100\n",
        "df['Open_Close_Change'] = df['Close'] - df['Open']\n",
        "df['Open_Close_Return'] = ((df['Close'] - df['Open']) / df['Open']) * 100\n",
        "df['High_Low_Ratio'] = df['High'] / df['Low']\n",
        "df['OHLC_Avg'] = (df['Open'] + df['High'] + df['Low'] + df['Close']) / 4\n",
        "df['Close_to_High'] = (df['High'] - df['Close']) / df['Close'] * 100\n",
        "df['Close_to_Low'] = (df['Close'] - df['Low']) / df['Low'] * 100\n",
        "print(\"   âœ… Created 8 price-based features\")\n",
        "\n",
        "# 7. Create technical indicators\n",
        "print(\"\\n6. Creating technical indicators...\")\n",
        "\n",
        "# RSI (Relative Strength Index)\n",
        "def calculate_rsi(data, periods=14):\n",
        "    delta = data.diff()\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=periods).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=periods).mean()\n",
        "    rs = gain / loss\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "    return rsi\n",
        "\n",
        "df['RSI'] = calculate_rsi(df['Close'], 14)\n",
        "print(\"   âœ… RSI calculated\")\n",
        "\n",
        "# MACD (Moving Average Convergence Divergence)\n",
        "exp1 = df['Close'].ewm(span=12, adjust=False).mean()\n",
        "exp2 = df['Close'].ewm(span=26, adjust=False).mean()\n",
        "df['MACD'] = exp1 - exp2\n",
        "df['MACD_Signal'] = df['MACD'].ewm(span=9, adjust=False).mean()\n",
        "df['MACD_Histogram'] = df['MACD'] - df['MACD_Signal']\n",
        "print(\"   âœ… MACD calculated\")\n",
        "\n",
        "# Bollinger Bands\n",
        "df['BB_Middle'] = df['Close'].rolling(window=20).mean()\n",
        "df['BB_Std'] = df['Close'].rolling(window=20).std()\n",
        "df['BB_Upper'] = df['BB_Middle'] + (df['BB_Std'] * 2)\n",
        "df['BB_Lower'] = df['BB_Middle'] - (df['BB_Std'] * 2)\n",
        "df['BB_Width'] = df['BB_Upper'] - df['BB_Lower']\n",
        "df['BB_Position'] = (df['Close'] - df['BB_Lower']) / (df['BB_Upper'] - df['BB_Lower'])\n",
        "print(\"   âœ… Bollinger Bands calculated\")\n",
        "\n",
        "# Volume proxy features (using price range as volume proxy)\n",
        "df['Volume_Proxy'] = df['Price_Range'] * df['Close']\n",
        "df['Volume_Proxy_MA_12'] = df['Volume_Proxy'].rolling(window=12).mean()\n",
        "print(\"   âœ… Volume proxy features created\")\n",
        "\n",
        "# 8. Create interaction features\n",
        "print(\"\\n7. Creating interaction features...\")\n",
        "df['Open_High_Interaction'] = df['Open'] * df['High']\n",
        "df['Open_Low_Interaction'] = df['Open'] * df['Low']\n",
        "df['High_Low_Interaction'] = df['High'] * df['Low']\n",
        "print(\"   âœ… Created interaction features\")\n",
        "\n",
        "# 9. Drop NaN values\n",
        "print(\"\\n8. Handling missing values...\")\n",
        "initial_rows = len(df)\n",
        "df_clean = df.dropna().reset_index(drop=True)\n",
        "final_rows = len(df_clean)\n",
        "rows_dropped = initial_rows - final_rows\n",
        "print(f\"   âœ… Dropped {rows_dropped} rows with NaN values\")\n",
        "print(f\"   âœ… Final dataset shape: {df_clean.shape}\")\n",
        "\n",
        "# 10. Verify data types\n",
        "print(\"\\n9. Verifying data types...\")\n",
        "print(df_clean.dtypes.value_counts())\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"âœ… Data Wrangling Complete!\")\n",
        "\n",
        "print(f\"ðŸ“Š Final dataset has {df_clean.shape[0]} rows and {df_clean.shape[1]} columns\")"
      ],
      "metadata": {
        "id": "GfBNAIJzxqfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display first few rows of cleaned dataset\n",
        "print(\"\\nðŸ“‹ First 5 rows of processed dataset:\")\n",
        "df_clean.head()"
      ],
      "metadata": {
        "id": "M6SQ6da-y1Jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSa1f5Uengrz"
      },
      "source": [
        "### What all manipulations have you done and insights you found?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbyXE7I1olp8"
      },
      "source": [
        "1. **Date Processing** (10 features)\n",
        "   - Converted string dates to datetime\n",
        "   - Extracted Year, Month, Quarter, Month_Name\n",
        "\n",
        "2. **Lag Features** (20 features)\n",
        "   - Created 1,2,3,6,12 month lags for all price columns\n",
        "   - Enables autoregressive modeling\n",
        "\n",
        "3. **Rolling Statistics** (24 features)\n",
        "   - Moving averages (3,6,12 months)\n",
        "   - Rolling volatility (standard deviation)\n",
        "   - Rolling min/max prices\n",
        "\n",
        "4. **Price-based Features** (8 features)\n",
        "   - Price range and percentage range\n",
        "   - Returns and changes\n",
        "   - OHLC averages and ratios\n",
        "\n",
        "5. **Technical Indicators** (12 features)\n",
        "   - RSI (momentum oscillator)\n",
        "   - MACD (trend following)\n",
        "   - Bollinger Bands (volatility)\n",
        "\n",
        "6. **Interaction Features** (3 features)\n",
        "   - Price multiplications for non-linear relationships\n",
        "\n",
        "**Key Insights from Wrangling:**\n",
        "- Time-based features capture seasonality in stock prices\n",
        "- Lag features show strong autocorrelation (prices depend on past values)\n",
        "- Technical indicators provide additional predictive power\n",
        "- Rolling statistics help identify trend changes and volatility regimes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF8Ens_Soomf"
      },
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wOQAZs5pc--"
      },
      "source": [
        "#### Chart - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "outputs": [],
      "source": [
        "# Chart - 1 visualization code - Time Series Decomposition\n",
        "fig, axes = plt.subplots(4, 1, figsize=(16, 12))\n",
        "\n",
        "# Perform seasonal decomposition\n",
        "decomposition = seasonal_decompose(df_clean['Close'].values, model='multiplicative', period=12)\n",
        "\n",
        "# Original series\n",
        "axes[0].plot(df_clean['Date'], df_clean['Close'], color='blue', linewidth=1.5)\n",
        "axes[0].set_title('Original Time Series - Yes Bank Closing Prices', fontsize=14, fontweight='bold')\n",
        "axes[0].set_ylabel('Price (INR)')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Trend component\n",
        "axes[1].plot(df_clean['Date'], decomposition.trend, color='red', linewidth=1.5)\n",
        "axes[1].set_title('Trend Component', fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylabel('Price (INR)')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# Seasonal component\n",
        "axes[2].plot(df_clean['Date'], decomposition.seasonal, color='green', linewidth=1.5)\n",
        "axes[2].set_title('Seasonal Component', fontsize=14, fontweight='bold')\n",
        "axes[2].set_ylabel('Seasonal Effect')\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "# Residual component\n",
        "axes[3].plot(df_clean['Date'], decomposition.resid, color='orange', linewidth=1.5)\n",
        "axes[3].set_title('Residual (Noise) Component', fontsize=14, fontweight='bold')\n",
        "axes[3].set_xlabel('Date')\n",
        "axes[3].set_ylabel('Residuals')\n",
        "axes[3].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5QZ13OEpz2H"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XESiWehPqBRc"
      },
      "source": [
        "Time series decomposition helps understand the underlying components of stock prices: trend, seasonality, and noise. This is crucial for feature engineering and model selection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_j1G7yiqdRP"
      },
      "source": [
        "- Strong upward trend until 2018, then sharp decline\n",
        "- Clear seasonal patterns (annual cycles)\n",
        "- Increasing variance in residuals during high volatility periods\n",
        "- Multiplicative seasonality (seasonal amplitude increases with price)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "448CDAPjqfQr"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cspy4FjqxJW"
      },
      "source": [
        "Yes, understanding these components helps in:\n",
        "- Identifying long-term investment opportunities (trend)\n",
        "- Timing entries/exits based on seasonal patterns\n",
        "- Risk assessment through residual volatility analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSlN3yHqYklG"
      },
      "source": [
        "#### Chart - 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "outputs": [],
      "source": [
        "# Chart - 2 visualization code - Autocorrection Analysis (ACF and PACF)\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "\n",
        "# ACF of original series\n",
        "plot_acf(df_clean['Close'], lags=30, ax=axes[0,0])\n",
        "axes[0,0].set_title('Autocorrelation Function (ACF) - Close Price', fontsize=12, fontweight='bold')\n",
        "axes[0,0].set_xlabel('Lag')\n",
        "axes[0,0].set_ylabel('Autocorrelation')\n",
        "axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "# PACF of original series\n",
        "plot_pacf(df_clean['Close'], lags=30, ax=axes[0,1], method='ywm')\n",
        "axes[0,1].set_title('Partial Autocorrelation Function (PACF) - Close Price', fontsize=12, fontweight='bold')\n",
        "axes[0,1].set_xlabel('Lag')\n",
        "axes[0,1].set_ylabel('Partial Autocorrelation')\n",
        "axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "# ACF of returns\n",
        "plot_acf(df_clean['Open_Close_Return'].dropna(), lags=30, ax=axes[1,0])\n",
        "axes[1,0].set_title('ACF - Monthly Returns', fontsize=12, fontweight='bold')\n",
        "axes[1,0].set_xlabel('Lag')\n",
        "axes[1,0].set_ylabel('Autocorrelation')\n",
        "axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "# PACF of returns\n",
        "plot_pacf(df_clean['Open_Close_Return'].dropna(), lags=30, ax=axes[1,1], method='ywm')\n",
        "axes[1,1].set_title('PACF - Monthly Returns', fontsize=12, fontweight='bold')\n",
        "axes[1,1].set_xlabel('Lag')\n",
        "axes[1,1].set_ylabel('Partial Autocorrelation')\n",
        "axes[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"ðŸ“Š Autocorrelation Analysis:\")\n",
        "print(f\"Strong autocorrelation in prices up to lag 12 (1 year)\")\n",
        "print(f\"Weak autocorrelation in returns (suggests market efficiency)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6dVpIINYklI"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aaW0BYyYklI"
      },
      "source": [
        "ACF and PACF are essential for time series modeling to understand the correlation structure and determine appropriate lag orders for ARIMA/SARIMA models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijmpgYnKYklI"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSx9atu2YklI"
      },
      "source": [
        "- Price shows strong autocorrelation up to 12 lags (prices highly dependent on past values)\n",
        "- Returns show minimal autocorrelation (random walk behavior)\n",
        "- Significant spikes at lag 1 and lag 12 suggest AR(1) and seasonal AR(1) components\n",
        "- PACF cuts off after lag 1 for returns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JiQyfWJYklI"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcBbebzrYklV"
      },
      "source": [
        "Critical for:\n",
        "- Selecting appropriate lag features in ML models\n",
        "- Understanding market efficiency (weak form)\n",
        "- Developing mean-reversion or momentum strategies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM7whBJCYoAo"
      },
      "source": [
        "#### Chart - 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "outputs": [],
      "source": [
        "# Chart - 3 visualization code - Stationery Tests\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "# Original series\n",
        "axes[0,0].plot(df_clean['Date'], df_clean['Close'], color='blue')\n",
        "axes[0,0].set_title('Original Close Price', fontsize=12, fontweight='bold')\n",
        "axes[0,0].set_ylabel('Price')\n",
        "axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "# ADF test result for original\n",
        "result_orig = adfuller(df_clean['Close'])\n",
        "axes[0,1].text(0.1, 0.5, f'ADF Test - Original Series\\n\\nADF Statistic: {result_orig[0]:.4f}\\np-value: {result_orig[1]:.4f}\\n\\nCritical Values:\\n1%: {result_orig[4][\"1%\"]:.4f}\\n5%: {result_orig[4][\"5%\"]:.4f}\\n10%: {result_orig[4][\"10%\"]:.4f}',\n",
        "               transform=axes[0,1].transAxes, fontsize=12, verticalalignment='center')\n",
        "axes[0,1].axis('off')\n",
        "axes[0,1].set_title('Augmented Dickey-Fuller Test', fontsize=12, fontweight='bold')\n",
        "\n",
        "# First difference\n",
        "df_clean['Close_Diff1'] = df_clean['Close'].diff()\n",
        "axes[1,0].plot(df_clean['Date'], df_clean['Close_Diff1'], color='green')\n",
        "axes[1,0].set_title('First Difference', fontsize=12, fontweight='bold')\n",
        "axes[1,0].set_ylabel('Price Change')\n",
        "axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "# ADF test for first difference\n",
        "result_diff = adfuller(df_clean['Close_Diff1'].dropna())\n",
        "axes[1,1].text(0.1, 0.5, f'ADF Test - First Difference\\n\\nADF Statistic: {result_diff[0]:.4f}\\np-value: {result_diff[1]:.4f}\\n\\nCritical Values:\\n1%: {result_diff[4][\"1%\"]:.4f}\\n5%: {result_diff[4][\"5%\"]:.4f}\\n10%: {result_diff[4][\"10%\"]:.4f}',\n",
        "               transform=axes[1,1].transAxes, fontsize=12, verticalalignment='center')\n",
        "axes[1,1].axis('off')\n",
        "axes[1,1].set_title('ADF Test - First Difference', fontsize=12, fontweight='bold')\n",
        "\n",
        "# Rolling statistics\n",
        "rolling_mean = df_clean['Close'].rolling(window=12).mean()\n",
        "rolling_std = df_clean['Close'].rolling(window=12).std()\n",
        "\n",
        "axes[0,2].plot(df_clean['Date'], df_clean['Close'], label='Original', alpha=0.7)\n",
        "axes[0,2].plot(df_clean['Date'], rolling_mean, label='12-month Rolling Mean', color='red')\n",
        "axes[0,2].plot(df_clean['Date'], rolling_std, label='12-month Rolling Std', color='green')\n",
        "axes[0,2].set_title('Rolling Statistics', fontsize=12, fontweight='bold')\n",
        "axes[0,2].set_xlabel('Date')\n",
        "axes[0,2].set_ylabel('Price')\n",
        "axes[0,2].legend()\n",
        "axes[0,2].grid(True, alpha=0.3)\n",
        "\n",
        "# Log transformation\n",
        "df_clean['Close_Log'] = np.log(df_clean['Close'])\n",
        "axes[1,2].plot(df_clean['Date'], df_clean['Close_Log'], color='purple')\n",
        "axes[1,2].set_title('Log Transformed Series', fontsize=12, fontweight='bold')\n",
        "axes[1,2].set_xlabel('Date')\n",
        "axes[1,2].set_ylabel('Log(Price)')\n",
        "axes[1,2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Interpretation\n",
        "print(\"\\nðŸ“Š Stationarity Test Results:\")\n",
        "print(f\"Original Series p-value: {result_orig[1]:.6f} - {'Non-stationary' if result_orig[1] > 0.05 else 'Stationary'}\")\n",
        "print(f\"First Difference p-value: {result_diff[1]:.6f} - {'Non-stationary' if result_diff[1] > 0.05 else 'Stationary'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fge-S5ZAYoAp"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dBItgRVYoAp"
      },
      "source": [
        "Stationarity tests determine if transformations are needed for time series modeling. Most ML models perform better with stationary data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85gYPyotYoAp"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jstXR6OYoAp"
      },
      "source": [
        "- Original series is non-stationary (p-value > 0.05)\n",
        "- First difference achieves stationarity (p-value < 0.05)\n",
        "- Rolling statistics show changing mean and variance over time\n",
        "- Log transformation helps stabilize variance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoGjAbkUYoAp"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      },
      "source": [
        "Essential for:\n",
        "- Choosing between price vs return prediction\n",
        "- Understanding risk dynamics over time\n",
        "- Selecting appropriate transformations for model inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Of9eVA-YrdM"
      },
      "source": [
        "#### Chart - 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "outputs": [],
      "source": [
        "# Chart - 4 visualization code - Feature Correlation Heatmap\n",
        "# Select numerical features for correlation analysis\n",
        "feature_cols = ['Close', 'Open', 'High', 'Low', 'Price_Range', 'Open_Close_Return',\n",
        "                'RSI', 'MACD', 'BB_Width', 'Volume_Proxy', 'Close_Lag_1', 'Close_Lag_12',\n",
        "                'Close_MA_12', 'Close_Std_12', 'Year', 'Month']\n",
        "\n",
        "# Ensure all selected columns exist\n",
        "available_cols = [col for col in feature_cols if col in df_clean.columns]\n",
        "corr_df = df_clean[available_cols].copy()\n",
        "\n",
        "# Calculate correlation matrix\n",
        "corr_matrix = corr_df.corr()\n",
        "\n",
        "# Create heatmap\n",
        "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
        "\n",
        "# Full correlation heatmap\n",
        "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
        "sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='RdBu_r', center=0,\n",
        "            square=True, linewidths=1, fmt='.2f', cbar_kws={\"shrink\": 0.8}, ax=axes[0])\n",
        "axes[0].set_title('Feature Correlation Matrix', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Correlation with target (Close price)\n",
        "target_corr = corr_matrix['Close'].sort_values(ascending=False)\n",
        "target_corr_df = pd.DataFrame({\n",
        "    'Feature': target_corr.index,\n",
        "    'Correlation': target_corr.values\n",
        "})\n",
        "\n",
        "colors = ['green' if x > 0 else 'red' for x in target_corr.values]\n",
        "axes[1].barh(range(len(target_corr_df)), target_corr_df['Correlation'], color=colors)\n",
        "axes[1].set_yticks(range(len(target_corr_df)))\n",
        "axes[1].set_yticklabels(target_corr_df['Feature'])\n",
        "axes[1].set_xlabel('Correlation with Close Price')\n",
        "axes[1].set_title('Feature Importance by Correlation', fontsize=14, fontweight='bold')\n",
        "axes[1].axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
        "axes[1].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nðŸ“Š Top 5 Features by Correlation with Close Price:\")\n",
        "print(target_corr_df.head(5).to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iky9q4vBYrdO"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJRCwT6DYrdO"
      },
      "source": [
        "Correlation heatmap helps identify relationships between features and multicollinearity, which is crucial for feature selection and model interpretation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6T5p64dYrdO"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      },
      "source": [
        "- Strong multicollinearity between Open, High, Low, Close (expected)\n",
        "- Lag features highly correlated with target\n",
        "- Technical indicators show moderate correlation\n",
        "- Month shows weak correlation (consistent with seasonal decomposition)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-Ehk30pYrdP"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLNxxz7MYrdP"
      },
      "source": [
        "Critical for:\n",
        "- Feature selection to avoid multicollinearity\n",
        "- Understanding which factors drive stock prices\n",
        "- Building interpretable models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bamQiAODYuh1"
      },
      "source": [
        "#### Chart - 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "outputs": [],
      "source": [
        "# Chart - 5 visualization code - Distribution Analysis\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "# Original Close price distribution\n",
        "axes[0,0].hist(df_clean['Close'], bins=40, color='skyblue', edgecolor='black', alpha=0.7)\n",
        "axes[0,0].axvline(df_clean['Close'].mean(), color='red', linestyle='--', linewidth=2, label=f\"Mean: {df_clean['Close'].mean():.2f}\")\n",
        "axes[0,0].axvline(df_clean['Close'].median(), color='green', linestyle='--', linewidth=2, label=f\"Median: {df_clean['Close'].median():.2f}\")\n",
        "axes[0,0].set_title('Distribution of Close Prices', fontsize=12, fontweight='bold')\n",
        "axes[0,0].set_xlabel('Close Price (INR)')\n",
        "axes[0,0].set_ylabel('Frequency')\n",
        "axes[0,0].legend()\n",
        "axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "# Log-transformed distribution\n",
        "axes[0,1].hist(np.log(df_clean['Close']), bins=40, color='lightgreen', edgecolor='black', alpha=0.7)\n",
        "axes[0,1].axvline(np.log(df_clean['Close']).mean(), color='red', linestyle='--', linewidth=2, label=f\"Mean: {np.log(df_clean['Close']).mean():.2f}\")\n",
        "axes[0,1].set_title('Log-Transformed Distribution', fontsize=12, fontweight='bold')\n",
        "axes[0,1].set_xlabel('Log(Close Price)')\n",
        "axes[0,1].set_ylabel('Frequency')\n",
        "axes[0,1].legend()\n",
        "axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "# Returns distribution\n",
        "axes[0,2].hist(df_clean['Open_Close_Return'].dropna(), bins=40, color='coral', edgecolor='black', alpha=0.7)\n",
        "axes[0,2].axvline(df_clean['Open_Close_Return'].mean(), color='red', linestyle='--', linewidth=2, label=f\"Mean: {df_clean['Open_Close_Return'].mean():.2f}%\")\n",
        "axes[0,2].axvline(df_clean['Open_Close_Return'].median(), color='green', linestyle='--', linewidth=2, label=f\"Median: {df_clean['Open_Close_Return'].median():.2f}%\")\n",
        "axes[0,2].set_title('Monthly Returns Distribution', fontsize=12, fontweight='bold')\n",
        "axes[0,2].set_xlabel('Return (%)')\n",
        "axes[0,2].set_ylabel('Frequency')\n",
        "axes[0,2].legend()\n",
        "axes[0,2].grid(True, alpha=0.3)\n",
        "\n",
        "# Q-Q plot for normality\n",
        "stats.probplot(df_clean['Close'], dist=\"norm\", plot=axes[1,0])\n",
        "axes[1,0].set_title('Q-Q Plot - Close Price', fontsize=12, fontweight='bold')\n",
        "axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "# Q-Q plot for log-transformed\n",
        "stats.probplot(np.log(df_clean['Close']), dist=\"norm\", plot=axes[1,1])\n",
        "axes[1,1].set_title('Q-Q Plot - Log Close Price', fontsize=12, fontweight='bold')\n",
        "axes[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "# Box plot\n",
        "df_clean[['Close', 'Open', 'High', 'Low']].boxplot(ax=axes[1,2])\n",
        "axes[1,2].set_title('Box Plot of Price Variables', fontsize=12, fontweight='bold')\n",
        "axes[1,2].set_ylabel('Price (INR)')\n",
        "axes[1,2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Statistical tests\n",
        "print(\"\\nðŸ“Š Normality Tests:\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"Skewness (Close): {df_clean['Close'].skew():.4f}\")\n",
        "print(f\"Kurtosis (Close): {df_clean['Close'].kurtosis():.4f}\")\n",
        "jb_stat, jb_p = jarque_bera(df_clean['Close'])\n",
        "print(f\"Jarque-Bera test p-value: {jb_p:.6f}\")\n",
        "print(f\"Interpretation: {'Not normal' if jb_p < 0.05 else 'Normal'} distribution\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcxuIMRPYuh3"
      },
      "source": [
        "Distribution analysis is crucial for understanding data characteristics and selecting appropriate transformations for ML models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwzvFGzlYuh3"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyqkiB8YYuh3"
      },
      "source": [
        "- Close price is right-skewed (positive skew)\n",
        "- Log transformation achieves near-normality\n",
        "- Returns show fat tails (leptokurtic)\n",
        "- Significant outliers in all price variables\n",
        "- JB test confirms non-normality (p < 0.05)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYpmQ266Yuh3"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      },
      "source": [
        "Important for:\n",
        "- Selecting appropriate loss functions\n",
        "- Understanding risk (fat tails mean extreme events more likely)\n",
        "- Applying transformations for better model performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH-pJp9IphqM"
      },
      "source": [
        "#### Chart - 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "outputs": [],
      "source": [
        "# Chart - 6 visualization code- Train-Test Split\n",
        "# Determine split point (80% train, 20% test)\n",
        "split_idx = int(len(df_clean) * 0.8)\n",
        "split_date = df_clean['Date'].iloc[split_idx]\n",
        "\n",
        "# Create train and test sets\n",
        "train_df = df_clean.iloc[:split_idx]\n",
        "test_df = df_clean.iloc[split_idx:]\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(18, 10))\n",
        "\n",
        "# Train-test split visualization\n",
        "axes[0,0].plot(train_df['Date'], train_df['Close'], label='Training Data', color='blue', linewidth=2)\n",
        "axes[0,0].plot(test_df['Date'], test_df['Close'], label='Test Data', color='orange', linewidth=2)\n",
        "axes[0,0].axvline(x=split_date, color='red', linestyle='--', linewidth=2, label=f'Split Date: {split_date.strftime(\"%b-%Y\")}')\n",
        "axes[0,0].set_title('Train-Test Split (80-20) - Time Series', fontsize=14, fontweight='bold')\n",
        "axes[0,0].set_xlabel('Date')\n",
        "axes[0,0].set_ylabel('Close Price (INR)')\n",
        "axes[0,0].legend()\n",
        "axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "# Distribution comparison - Train vs Test\n",
        "axes[0,1].hist(train_df['Close'], bins=30, alpha=0.7, label='Train', color='blue', edgecolor='black')\n",
        "axes[0,1].hist(test_df['Close'], bins=30, alpha=0.7, label='Test', color='orange', edgecolor='black')\n",
        "axes[0,1].set_title('Distribution Comparison: Train vs Test', fontsize=14, fontweight='bold')\n",
        "axes[0,1].set_xlabel('Close Price (INR)')\n",
        "axes[0,1].set_ylabel('Frequency')\n",
        "axes[0,1].legend()\n",
        "axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "# Statistics comparison\n",
        "stats_comparison = pd.DataFrame({\n",
        "    'Metric': ['Count', 'Mean', 'Std', 'Min', '25%', '50%', '75%', 'Max'],\n",
        "    'Train': [len(train_df), train_df['Close'].mean(), train_df['Close'].std(),\n",
        "              train_df['Close'].min(), train_df['Close'].quantile(0.25),\n",
        "              train_df['Close'].median(), train_df['Close'].quantile(0.75),\n",
        "              train_df['Close'].max()],\n",
        "    'Test': [len(test_df), test_df['Close'].mean(), test_df['Close'].std(),\n",
        "             test_df['Close'].min(), test_df['Close'].quantile(0.25),\n",
        "             test_df['Close'].median(), test_df['Close'].quantile(0.75),\n",
        "             test_df['Close'].max()]\n",
        "})\n",
        "\n",
        "# Hide axes for table\n",
        "axes[1,0].axis('tight')\n",
        "axes[1,0].axis('off')\n",
        "table = axes[1,0].table(cellText=stats_comparison.round(2).values,\n",
        "                        colLabels=stats_comparison.columns,\n",
        "                        cellLoc='center', loc='center')\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(10)\n",
        "table.scale(1.2, 1.5)\n",
        "axes[1,0].set_title('Dataset Statistics Comparison', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Rolling statistics comparison\n",
        "train_rolling_mean = train_df['Close'].rolling(window=12).mean()\n",
        "test_rolling_mean = test_df['Close'].rolling(window=12).mean()\n",
        "\n",
        "axes[1,1].plot(train_df['Date'][11:], train_rolling_mean[11:], color='blue', label='Train 12-MA')\n",
        "axes[1,1].plot(test_df['Date'], test_rolling_mean, color='orange', label='Test 12-MA')\n",
        "axes[1,1].set_title('Rolling Mean Comparison (12-month)', fontsize=14, fontweight='bold')\n",
        "axes[1,1].set_xlabel('Date')\n",
        "axes[1,1].set_ylabel('Price (INR)')\n",
        "axes[1,1].legend()\n",
        "axes[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nðŸ“Š Train-Test Split Summary:\")\n",
        "print(f\"Split Date: {split_date.strftime('%B %Y')}\")\n",
        "print(f\"Training set: {len(train_df)} samples ({len(train_df)/len(df_clean)*100:.1f}%)\")\n",
        "print(f\"Test set: {len(test_df)} samples ({len(test_df)/len(df_clean)*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbFf2-_FphqN"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loh7H2nzphqN"
      },
      "source": [
        "Visualizing train-test split is crucial for time series to ensure no data leakage and to understand the distribution differences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ouA3fa0phqN"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VECbqPI7phqN"
      },
      "source": [
        "- Test set contains the recent high-volatility period (2018-2020)\n",
        "- Train and test distributions are different (non-stationarity)\n",
        "- Test set includes the dramatic price drop\n",
        "- This split will test model's ability to handle regime changes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Seke61FWphqN"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW4_bGpfphqN"
      },
      "source": [
        "Critical for:\n",
        "- Understanding model generalization to new market conditions\n",
        "- Evaluating model robustness during crisis periods\n",
        "- Setting realistic performance expectations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIIx-8_IphqN"
      },
      "source": [
        "#### Chart - 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "outputs": [],
      "source": [
        "# Chart - 7 visualization code - Mutual Information\n",
        "# Prepare features for mutual information calculation\n",
        "feature_names = [col for col in df_clean.columns if col not in ['Date', 'Close', 'Month_Name', 'Year_Month', 'Close_Diff1', 'Close_Log']]\n",
        "X_mi = df_clean[feature_names].select_dtypes(include=[np.number])\n",
        "y_mi = df_clean['Close']\n",
        "\n",
        "# Calculate mutual information\n",
        "mi_scores = mutual_info_regression(X_mi, y_mi, random_state=42)\n",
        "mi_df = pd.DataFrame({\n",
        "    'Feature': X_mi.columns,\n",
        "    'MI_Score': mi_scores\n",
        "}).sort_values('MI_Score', ascending=False)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
        "\n",
        "# Top 20 features by mutual information\n",
        "top_20_mi = mi_df.head(20)\n",
        "colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(top_20_mi)))\n",
        "axes[0].barh(range(len(top_20_mi)), top_20_mi['MI_Score'].values, color=colors)\n",
        "axes[0].set_yticks(range(len(top_20_mi)))\n",
        "axes[0].set_yticklabels(top_20_mi['Feature'].values)\n",
        "axes[0].set_xlabel('Mutual Information Score')\n",
        "axes[0].set_title('Top 20 Features by Mutual Information', fontsize=14, fontweight='bold')\n",
        "axes[0].invert_yaxis()\n",
        "axes[0].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Cumulative importance\n",
        "mi_df['Cumulative'] = mi_df['MI_Score'].cumsum() / mi_df['MI_Score'].sum()\n",
        "axes[1].plot(range(1, len(mi_df)+1), mi_df['Cumulative'].values, marker='o', markersize=4, linewidth=2)\n",
        "axes[1].axhline(y=0.8, color='red', linestyle='--', label='80% threshold')\n",
        "axes[1].axhline(y=0.9, color='green', linestyle='--', label='90% threshold')\n",
        "axes[1].set_xlabel('Number of Features')\n",
        "axes[1].set_ylabel('Cumulative Importance')\n",
        "axes[1].set_title('Cumulative Feature Importance', fontsize=14, fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nðŸ“Š Top 10 Features by Mutual Information:\")\n",
        "print(top_20_mi.head(10).to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t27r6nlMphqO"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iv6ro40sphqO"
      },
      "source": [
        "Mutual information captures non-linear relationships between features and target, providing a more comprehensive feature importance measure than correlation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2jJGEOYphqO"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po6ZPi4hphqO"
      },
      "source": [
        "- Lag features (especially Close_Lag_1) have highest MI scores\n",
        "- Technical indicators (RSI, MACD) show significant information\n",
        "- Price range and volatility features important\n",
        "- Top 10 features capture ~80% of total information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0JNsNcRphqO"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvSq8iUTphqO"
      },
      "source": [
        "Essential for:\n",
        "- Optimal feature selection to reduce overfitting\n",
        "- Understanding which factors drive price movements\n",
        "- Building parsimonious models for deployment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZR9WyysphqO"
      },
      "source": [
        "#### Chart - 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "outputs": [],
      "source": [
        "# Chart - 8 visualization code - Baseline Models Comparison\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "\n",
        "# Prepare data with selected features\n",
        "# Use top features from mutual information\n",
        "top_features = mi_df.head(15)['Feature'].tolist()\n",
        "X = df_clean[top_features].values\n",
        "y = df_clean['Close'].values\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split data chronologically\n",
        "X_train, X_test = X_scaled[:split_idx], X_scaled[split_idx:]\n",
        "y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "\n",
        "# Define baseline models\n",
        "baseline_models = {\n",
        "    'Linear Regression': LinearRegression(),\n",
        "    'Ridge Regression': Ridge(alpha=1.0),\n",
        "    'Lasso Regression': Lasso(alpha=0.01),\n",
        "    'Decision Tree': DecisionTreeRegressor(max_depth=5, random_state=42),\n",
        "    'KNN': KNeighborsRegressor(n_neighbors=5),\n",
        "    'SVR': SVR(kernel='rbf', C=100, gamma=0.1)\n",
        "}\n",
        "\n",
        "# Train and evaluate models\n",
        "baseline_results = {}\n",
        "for name, model in baseline_models.items():\n",
        "    # Train\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate metrics\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    mape = mean_absolute_percentage_error(y_test, y_pred) * 100\n",
        "\n",
        "    baseline_results[name] = {\n",
        "        'RMSE': rmse,\n",
        "        'MAE': mae,\n",
        "        'R2': r2,\n",
        "        'MAPE': mape,\n",
        "        'Predictions': y_pred\n",
        "    }\n",
        "\n",
        "# Create comparison DataFrame\n",
        "results_df = pd.DataFrame(baseline_results).T\n",
        "results_df = results_df.sort_values('R2', ascending=False)\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# RÂ² Score Comparison\n",
        "axes[0,0].barh(results_df.index, results_df['R2'], color='skyblue', edgecolor='black')\n",
        "axes[0,0].set_xlabel('RÂ² Score')\n",
        "axes[0,0].set_title('Model Performance - RÂ² Score', fontsize=14, fontweight='bold')\n",
        "axes[0,0].set_xlim(0, 1)\n",
        "axes[0,0].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# RMSE Comparison\n",
        "axes[0,1].barh(results_df.index, results_df['RMSE'], color='lightcoral', edgecolor='black')\n",
        "axes[0,1].set_xlabel('RMSE (INR)')\n",
        "axes[0,1].set_title('Model Performance - RMSE', fontsize=14, fontweight='bold')\n",
        "axes[0,1].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# MAPE Comparison\n",
        "axes[1,0].barh(results_df.index, results_df['MAPE'], color='lightgreen', edgecolor='black')\n",
        "axes[1,0].set_xlabel('MAPE (%)')\n",
        "axes[1,0].set_title('Model Performance - MAPE', fontsize=14, fontweight='bold')\n",
        "axes[1,0].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Actual vs Best Model (Linear Regression)\n",
        "best_model_name = results_df.index[0]\n",
        "best_pred = baseline_results[best_model_name]['Predictions']\n",
        "\n",
        "axes[1,1].scatter(y_test, best_pred, alpha=0.6)\n",
        "axes[1,1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "axes[1,1].set_xlabel('Actual Price (INR)')\n",
        "axes[1,1].set_ylabel('Predicted Price (INR)')\n",
        "axes[1,1].set_title(f'Actual vs Predicted - {best_model_name}', fontsize=14, fontweight='bold')\n",
        "axes[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nðŸ“Š Baseline Model Performance Summary:\")\n",
        "print(results_df[['RMSE', 'MAE', 'R2', 'MAPE']].round(4).to_string())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj7wYXLtphqO"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ob8u6rCTphqO"
      },
      "source": [
        "Comparing multiple baseline models helps establish performance benchmarks and identify which algorithm families work best for this problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZrbJ2SmphqO"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Linear models perform well (RÂ² > 0.95)\n",
        "- SVR struggles with this dataset (low RÂ²)\n",
        "- Decision tree shows overfitting signs\n",
        "- MAPE ranges from 5-15% across models"
      ],
      "metadata": {
        "id": "Qbqko5hOu1r5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFu4xreNphqO"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ey_0qi68phqO"
      },
      "source": [
        "Provides:\n",
        "- Baseline expectations for model performance\n",
        "- Guidance on which algorithms to tune further\n",
        "- Understanding of prediction accuracy in business terms (MAPE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJ55k-q6phqO"
      },
      "source": [
        "#### Chart - 9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "outputs": [],
      "source": [
        "# Chart - 9 visualization code - Advanced Models\n",
        "advanced_models = {\n",
        "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
        "    'XGBoost': xgb.XGBRegressor(objective='reg:squarederror', random_state=42),\n",
        "    'LightGBM': lgb.LGBMRegressor(random_state=42, verbose=-1),\n",
        "    'CatBoost': CatBoostRegressor(random_state=42, verbose=0)\n",
        "}\n",
        "\n",
        "# Train and evaluate advanced models\n",
        "advanced_results = {}\n",
        "predictions_dict = {}\n",
        "\n",
        "for name, model in advanced_models.items():\n",
        "    # Train\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model.predict(X_test)\n",
        "    predictions_dict[name] = y_pred\n",
        "\n",
        "    # Calculate metrics\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    mape = mean_absolute_percentage_error(y_test, y_pred) * 100\n",
        "\n",
        "    advanced_results[name] = {\n",
        "        'RMSE': rmse,\n",
        "        'MAE': mae,\n",
        "        'R2': r2,\n",
        "        'MAPE': mape\n",
        "    }\n",
        "\n",
        "# Combine with baseline results\n",
        "all_results = pd.DataFrame({**baseline_results, **advanced_results}).T\n",
        "\n",
        "# Convert relevant columns to numeric to ensure correct dtype for nlargest/nsmallest\n",
        "for col in ['R2', 'RMSE', 'MAE', 'MAPE']:\n",
        "    all_results[col] = pd.to_numeric(all_results[col], errors='coerce')\n",
        "\n",
        "all_results = all_results.sort_values('R2', ascending=False)\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "# Performance metrics comparison\n",
        "metrics = ['R2', 'RMSE', 'MAPE']\n",
        "for i, metric in enumerate(metrics):\n",
        "    if metric == 'R2':\n",
        "        top_models = all_results.nlargest(8, metric)[metric]\n",
        "        colors = ['green' if v == top_models.max() else 'skyblue' for v in top_models.values]\n",
        "    else: # For RMSE and MAPE, smaller is better\n",
        "        top_models = all_results.nsmallest(8, metric)[metric]\n",
        "        colors = ['green' if v == top_models.min() else 'skyblue' for v in top_models.values]\n",
        "\n",
        "    axes[0, i].barh(top_models.index, top_models.values, color=colors, edgecolor='black')\n",
        "    axes[0, i].set_xlabel(metric)\n",
        "    axes[0, i].set_title(f'Top Models by {metric}', fontsize=12, fontweight='bold')\n",
        "    axes[0, i].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Radar chart for top 3 models\n",
        "from math import pi\n",
        "\n",
        "top_3 = all_results.head(3)\n",
        "categories = ['R2', 'RMSE', 'MAE', 'MAPE']\n",
        "N = len(categories)\n",
        "\n",
        "# Normalize values\n",
        "normalized = top_3.copy()\n",
        "# Ensure all columns are numeric before normalization\n",
        "for col in categories:\n",
        "    normalized[col] = pd.to_numeric(normalized[col], errors='coerce')\n",
        "\n",
        "# Normalize R2 (higher is better, scale to 0-1 if not already, or keep as is)\n",
        "# For error metrics (RMSE, MAE, MAPE), scale inverse so lower error becomes higher value (1 is best)\n",
        "max_rmse = normalized['RMSE'].max()\n",
        "if max_rmse > 0:\n",
        "    normalized['RMSE'] = 1 - (normalized['RMSE'] / max_rmse)\n",
        "else:\n",
        "    normalized['RMSE'] = 1 # Handle case where RMSE is 0 (perfect score)\n",
        "\n",
        "max_mae = normalized['MAE'].max()\n",
        "if max_mae > 0:\n",
        "    normalized['MAE'] = 1 - (normalized['MAE'] / max_mae)\n",
        "else:\n",
        "    normalized['MAE'] = 1 # Handle case where MAE is 0\n",
        "\n",
        "max_mape = normalized['MAPE'].max()\n",
        "if max_mape > 0:\n",
        "    normalized['MAPE'] = 1 - (normalized['MAPE'] / max_mape)\n",
        "else:\n",
        "    normalized['MAPE'] = 1 # Handle case where MAPE is 0\n",
        "\n",
        "angles = [n / float(N) * 2 * pi for n in range(N)]\n",
        "angles += angles[:1]\n",
        "\n",
        "# Remove the existing axes[1,0] to replace it with a polar one\n",
        "fig.delaxes(axes[1, 0])\n",
        "ax_radar = fig.add_subplot(2, 3, 4, polar=True) # Position 4 in a 2x3 grid is row 2, col 1\n",
        "\n",
        "for idx, (model_name, values) in enumerate(normalized.iterrows()):\n",
        "    values_list = [values['R2'], values['RMSE'], values['MAE'], values['MAPE']]\n",
        "    values_list += values_list[:1]\n",
        "    ax_radar.plot(angles, values_list, 'o-', linewidth=2, label=model_name)\n",
        "    ax_radar.fill(angles, values_list, alpha=0.1)\n",
        "\n",
        "ax_radar.set_xticks(angles[:-1])\n",
        "ax_radar.set_xticklabels(categories)\n",
        "ax_radar.set_title('Top 3 Models - Performance Radar', fontsize=12, fontweight='bold', pad=20)\n",
        "ax_radar.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
        "\n",
        "# Time series predictions comparison\n",
        "axes[1, 1].plot(df_clean['Date'].iloc[split_idx:], y_test, label='Actual', linewidth=2, color='black')\n",
        "for name in ['XGBoost', 'Random Forest', 'Gradient Boosting']:\n",
        "    if name in predictions_dict:\n",
        "        axes[1, 1].plot(df_clean['Date'].iloc[split_idx:], predictions_dict[name],\n",
        "                       label=f'{name}', linewidth=1.5, alpha=0.7)\n",
        "axes[1, 1].set_xlabel('Date')\n",
        "axes[1, 1].set_ylabel('Close Price (INR)')\n",
        "axes[1, 1].set_title('Model Predictions Comparison', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Residuals box plot\n",
        "residuals_df = pd.DataFrame()\n",
        "for name in ['XGBoost', 'Random Forest', 'Gradient Boosting']:\n",
        "    if name in predictions_dict:\n",
        "        residuals_df[name] = y_test - predictions_dict[name]\n",
        "\n",
        "residuals_df.boxplot(ax=axes[1, 2])\n",
        "axes[1, 2].set_title('Residuals Distribution by Model', fontsize=12, fontweight='bold')\n",
        "axes[1, 2].set_ylabel('Residuals (INR)')\n",
        "axes[1, 2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nðŸ“Š Top 5 Models Overall:\")\n",
        "print(all_results.head(5)[['R2', 'RMSE', 'MAPE']].round(4).to_string())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCFgpxoyphqP"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVxDimi2phqP"
      },
      "source": [
        "Multi-panel comparison shows comprehensive model performance from different angles, helping identify the best model for the specific use case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVtJsKN_phqQ"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngGi97qjphqQ"
      },
      "source": [
        "- Ensemble methods outperform linear models\n",
        "- XGBoost and Random Forest show best performance\n",
        "- Gradient Boosting has highest variance in predictions\n",
        "- All models struggle during high volatility periods (2020)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lssrdh5qphqQ"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBpY5ekJphqQ"
      },
      "source": [
        "Crucial for:\n",
        "- Selecting the most appropriate model for deployment\n",
        "- Understanding model strengths and weaknesses\n",
        "- Setting realistic performance expectations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      },
      "source": [
        "#### Chart - 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "outputs": [],
      "source": [
        "# Chart - 10 visualization code - Hyperparameter Tuning\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Define parameter grid for XGBoost\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [3, 5, 7, 9],\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.3],\n",
        "    'subsample': [0.6, 0.8, 1.0],\n",
        "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
        "    'min_child_weight': [1, 3, 5],\n",
        "    'gamma': [0, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "# Randomized Search\n",
        "xgb_base = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "random_search = RandomizedSearchCV(\n",
        "    xgb_base,\n",
        "    param_distributions=param_grid,\n",
        "    n_iter=50,\n",
        "    cv=5,\n",
        "    scoring='r2',\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "print(\"ðŸ”„ Performing Randomized Search (this may take a few minutes)...\")\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "# Parameter importance\n",
        "cv_results = pd.DataFrame(random_search.cv_results_)\n",
        "\n",
        "# Learning rate vs Performance\n",
        "ax1 = axes[0, 0]\n",
        "for lr in sorted(cv_results['param_learning_rate'].unique()):\n",
        "    subset = cv_results[cv_results['param_learning_rate'] == lr]\n",
        "    ax1.scatter([lr]*len(subset), subset['mean_test_score'], alpha=0.6, label=f'lr={lr}')\n",
        "ax1.set_xlabel('Learning Rate')\n",
        "ax1.set_ylabel('Mean Test RÂ²')\n",
        "ax1.set_title('Learning Rate Impact', fontsize=12, fontweight='bold')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Max depth vs Performance\n",
        "ax2 = axes[0, 1]\n",
        "depth_perf = cv_results.groupby('param_max_depth')['mean_test_score'].mean()\n",
        "ax2.plot(depth_perf.index, depth_perf.values, marker='o', linewidth=2)\n",
        "ax2.set_xlabel('Max Depth')\n",
        "ax2.set_ylabel('Mean Test RÂ²')\n",
        "ax2.set_title('Max Depth Impact', fontsize=12, fontweight='bold')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# N_estimators vs Performance\n",
        "ax3 = axes[0, 2]\n",
        "n_est_perf = cv_results.groupby('param_n_estimators')['mean_test_score'].mean()\n",
        "ax3.plot(n_est_perf.index, n_est_perf.values, marker='o', linewidth=2)\n",
        "ax3.set_xlabel('Number of Estimators')\n",
        "ax3.set_ylabel('Mean Test RÂ²')\n",
        "ax3.set_title('N_Estimators Impact', fontsize=12, fontweight='bold')\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Subsample vs Performance\n",
        "ax4 = axes[1, 0]\n",
        "sub_perf = cv_results.groupby('param_subsample')['mean_test_score'].mean()\n",
        "ax4.plot(sub_perf.index, sub_perf.values, marker='o', linewidth=2)\n",
        "ax4.set_xlabel('Subsample Ratio')\n",
        "ax4.set_ylabel('Mean Test RÂ²')\n",
        "ax4.set_title('Subsample Impact', fontsize=12, fontweight='bold')\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "# Colsample vs Performance\n",
        "ax5 = axes[1, 1]\n",
        "col_perf = cv_results.groupby('param_colsample_bytree')['mean_test_score'].mean()\n",
        "ax5.plot(col_perf.index, col_perf.values, marker='o', linewidth=2)\n",
        "ax5.set_xlabel('Colsample by Tree')\n",
        "ax5.set_ylabel('Mean Test RÂ²')\n",
        "ax5.set_title('Colsample Impact', fontsize=12, fontweight='bold')\n",
        "ax5.grid(True, alpha=0.3)\n",
        "\n",
        "# Top 10 parameter combinations\n",
        "top_10 = cv_results.nlargest(10, 'mean_test_score')[['param_n_estimators', 'param_max_depth',\n",
        "                                                     'param_learning_rate', 'param_subsample',\n",
        "                                                     'mean_test_score']]\n",
        "ax6 = axes[1, 2]\n",
        "ax6.axis('tight')\n",
        "ax6.axis('off')\n",
        "table = ax6.table(cellText=top_10.round(4).values,\n",
        "                  colLabels=top_10.columns,\n",
        "                  cellLoc='center', loc='center')\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(9)\n",
        "table.scale(1.2, 1.5)\n",
        "ax6.set_title('Top 10 Parameter Combinations', fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nðŸ“Š Best Parameters Found:\")\n",
        "print(random_search.best_params_)\n",
        "print(f\"Best CV Score: {random_search.best_score_:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M8mcRywphqQ"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8agQvks0phqQ"
      },
      "source": [
        "Visualizing hyperparameter tuning helps understand how different parameters affect model performance and guides optimal parameter selection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgIPom80phqQ"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qp13pnNzphqQ"
      },
      "source": [
        "- Learning rate around 0.1 performs best\n",
        "- Max depth of 5-7 is optimal (prevents overfitting)\n",
        "- 200-300 estimators provide best performance\n",
        "- Subsample 0.8 helps prevent overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMzcOPDDphqR"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4Ka1PC2phqR"
      },
      "source": [
        "Essential for:\n",
        "- Optimizing model performance without overfitting\n",
        "- Understanding model complexity trade-offs\n",
        "- Reproducible model tuning process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-EpHcCOp1ci"
      },
      "source": [
        "#### Chart - 11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "outputs": [],
      "source": [
        "# Chart - 11 visualization code - Feature Importance (XGBoost)\n",
        "best_xgb = random_search.best_estimator_\n",
        "\n",
        "# Get feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': top_features,\n",
        "    'Importance': best_xgb.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "# Bar plot of feature importance\n",
        "top_15 = feature_importance.head(15)\n",
        "colors = plt.cm.YlOrRd(np.linspace(0.3, 0.9, len(top_15)))\n",
        "axes[0].barh(range(len(top_15)), top_15['Importance'].values, color=colors[::-1])\n",
        "axes[0].set_yticks(range(len(top_15)))\n",
        "axes[0].set_yticklabels(top_15['Feature'].values)\n",
        "axes[0].set_xlabel('Importance Score')\n",
        "axes[0].set_title('Top 15 Features - XGBoost Importance', fontsize=14, fontweight='bold')\n",
        "axes[0].invert_yaxis()\n",
        "axes[0].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Cumulative importance\n",
        "feature_importance['Cumulative'] = feature_importance['Importance'].cumsum()\n",
        "axes[1].plot(range(1, len(feature_importance)+1), feature_importance['Cumulative'].values,\n",
        "            marker='o', markersize=4, linewidth=2)\n",
        "axes[1].axhline(y=0.8, color='red', linestyle='--', label='80% threshold')\n",
        "axes[1].axhline(y=0.9, color='green', linestyle='--', label='90% threshold')\n",
        "axes[1].axhline(y=0.95, color='blue', linestyle='--', label='95% threshold')\n",
        "axes[1].set_xlabel('Number of Features')\n",
        "axes[1].set_ylabel('Cumulative Importance')\n",
        "axes[1].set_title('Cumulative Feature Importance', fontsize=14, fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# Feature importance by category\n",
        "categories = {\n",
        "    'Lag Features': [f for f in feature_importance['Feature'] if 'Lag' in f],\n",
        "    'Technical Indicators': [f for f in feature_importance['Feature'] if f in ['RSI', 'MACD', 'BB_Width', 'MACD_Signal']],\n",
        "    'Price-based': [f for f in feature_importance['Feature'] if f in ['Price_Range', 'Open_Close_Return', 'High_Low_Ratio']],\n",
        "    'Rolling Stats': [f for f in feature_importance['Feature'] if 'MA' in f or 'Std' in f],\n",
        "    'Other': [f for f in feature_importance['Feature'] if f not in\n",
        "              ['RSI', 'MACD', 'BB_Width', 'MACD_Signal', 'Price_Range', 'Open_Close_Return', 'High_Low_Ratio']\n",
        "              and 'Lag' not in f and 'MA' not in f and 'Std' not in f]\n",
        "}\n",
        "\n",
        "category_importance = {}\n",
        "for category, features in categories.items():\n",
        "    category_importance[category] = feature_importance[feature_importance['Feature'].isin(features)]['Importance'].sum()\n",
        "\n",
        "# Pie chart of feature importance by category\n",
        "axes[2].pie(category_importance.values(), labels=category_importance.keys(), autopct='%1.1f%%',\n",
        "           colors=['#ff9999', '#66b3ff', '#99ff99', '#ffcc99', '#c2c2f0'])\n",
        "axes[2].set_title('Feature Importance by Category', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nðŸ“Š Top 5 Most Important Features:\")\n",
        "print(feature_importance.head(5).to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_VqEhTip1ck"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vsMzt_np1ck"
      },
      "source": [
        "Feature importance analysis reveals which predictors drive the model's decisions, essential for model interpretability and business understanding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zGJKyg5p1ck"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      },
      "source": [
        "- Lag features dominate importance (>60%)\n",
        "- Recent lags (1,2,3) most important\n",
        "- Technical indicators contribute ~15%\n",
        "- Price range and volatility features matter\n",
        "- Top 8 features capture 80% of importance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "druuKYZpp1ck"
      },
      "source": [
        "Critical for:\n",
        "- Understanding what drives stock prices\n",
        "- Communicating model logic to stakeholders\n",
        "- Feature reduction for deployment efficiency\n",
        "- Identifying key monitoring indicators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3dbpmDWp1ck"
      },
      "source": [
        "#### Chart - 12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "outputs": [],
      "source": [
        "# Chart - 12 visualization code - SHAP Analysis\n",
        "print(\"ðŸ”„ Calculating SHAP values (this may take a minute)...\")\n",
        "\n",
        "# Create SHAP explainer\n",
        "explainer = shap.TreeExplainer(best_xgb)\n",
        "\n",
        "# Calculate SHAP values for test set\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Summary plot\n",
        "shap.summary_plot(shap_values, X_test, feature_names=top_features, show=False, plot_size=(8, 6))\n",
        "plt.title('SHAP Summary Plot - Feature Impact', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Recreate figure for remaining plots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# SHAP bar plot (mean absolute SHAP)\n",
        "shap_importance = np.abs(shap_values).mean(axis=0)\n",
        "shap_df = pd.DataFrame({\n",
        "    'Feature': top_features,\n",
        "    'Mean_SHAP': shap_importance\n",
        "}).sort_values('Mean_SHAP', ascending=True).tail(15)\n",
        "\n",
        "axes[0].barh(shap_df['Feature'], shap_df['Mean_SHAP'], color='coral', edgecolor='black')\n",
        "axes[0].set_xlabel('Mean |SHAP Value|')\n",
        "axes[0].set_title('SHAP Feature Importance', fontsize=14, fontweight='bold')\n",
        "axes[0].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Waterfall plot for first prediction\n",
        "shap.waterfall_plot(shap.Explanation(values=shap_values[0],\n",
        "                                    base_values=explainer.expected_value,\n",
        "                                    data=X_test[0],\n",
        "                                    feature_names=top_features),\n",
        "                   show=False, max_display=10)\n",
        "axes[1].set_title(f'SHAP Waterfall Plot - Prediction {1}', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Dependence plots for top features\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "for i, feature in enumerate(['Close_Lag_1', 'Open']): # Changed 'RSI' to 'Open'\n",
        "    shap.dependence_plot(feature, shap_values, X_test, feature_names=top_features,\n",
        "                        ax=axes[i], show=False)\n",
        "    axes[i].set_title(f'SHAP Dependence - {feature}', fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylSl6qgtp1ck"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2xqNkiQp1ck"
      },
      "source": [
        "SHAP provides consistent, locally accurate feature attributions based on game theory, making it the gold standard for model explainability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWILFDl5p1ck"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-lUsV2mp1ck"
      },
      "source": [
        "- Recent price (Close_Lag_1) has highest impact\n",
        "- High RSI values push predictions lower (overbought)\n",
        "- Feature interactions visible in dependence plots\n",
        "- Individual predictions can be explained component-wise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7G43BXep1ck"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wwDJXsLp1cl"
      },
      "source": [
        "Essential for:\n",
        "- Building trust in model predictions\n",
        "- Regulatory compliance (explainable AI)\n",
        "- Understanding model biases\n",
        "- Debugging unexpected predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ag9LCva-p1cl"
      },
      "source": [
        "#### Chart - 13"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "outputs": [],
      "source": [
        "# Chart - 13 visualization code - LSTM\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Prepare data for LSTM (sequences)\n",
        "def create_sequences(data, seq_length=12):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i+seq_length])\n",
        "        y.append(data[i+seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Scale data for LSTM\n",
        "scaler_lstm = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_close = scaler_lstm.fit_transform(df_clean['Close'].values.reshape(-1, 1))\n",
        "\n",
        "# Create sequences\n",
        "seq_length = 12\n",
        "X_lstm, y_lstm = create_sequences(scaled_close, seq_length)\n",
        "\n",
        "# Split data\n",
        "train_size = int(len(X_lstm) * 0.8)\n",
        "X_train_lstm, X_test_lstm = X_lstm[:train_size], X_lstm[train_size:]\n",
        "y_train_lstm, y_test_lstm = y_lstm[:train_size], y_lstm[train_size:]\n",
        "\n",
        "print(f\"LSTM Training shape: {X_train_lstm.shape}\")\n",
        "print(f\"LSTM Test shape: {X_test_lstm.shape}\")\n",
        "\n",
        "# Build LSTM model\n",
        "def create_lstm_model(units=50, dropout=0.2, learning_rate=0.001):\n",
        "    model = Sequential([\n",
        "        LSTM(units, return_sequences=True, input_shape=(seq_length, 1)),\n",
        "        Dropout(dropout),\n",
        "        LSTM(units, return_sequences=False),\n",
        "        Dropout(dropout),\n",
        "        Dense(25, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
        "                 loss='mse',\n",
        "                 metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "lstm_model = create_lstm_model()\n",
        "\n",
        "# Custom callback to log learning rate\n",
        "class LearningRateLogger(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        logs['lr'] = tf.keras.backend.get_value(self.model.optimizer.learning_rate) # Changed .lr to .learning_rate\n",
        "\n",
        "# Callbacks\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=0.00001)\n",
        "checkpoint = ModelCheckpoint('best_lstm.h5', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "# Train model\n",
        "print(\"ðŸ”„ Training LSTM model (this may take a few minutes)...\")\n",
        "history = lstm_model.fit(\n",
        "    X_train_lstm, y_train_lstm,\n",
        "    epochs=150,\n",
        "    batch_size=16,\n",
        "    validation_split=0.1,\n",
        "    callbacks=[early_stop, reduce_lr, checkpoint, LearningRateLogger()], # Add custom LR logger\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# Make predictions\n",
        "y_pred_lstm_scaled = lstm_model.predict(X_test_lstm)\n",
        "y_pred_lstm = scaler_lstm.inverse_transform(y_pred_lstm_scaled)\n",
        "y_test_lstm_actual = scaler_lstm.inverse_transform(y_test_lstm)\n",
        "\n",
        "# Calculate metrics\n",
        "lstm_rmse = np.sqrt(mean_squared_error(y_test_lstm_actual, y_pred_lstm))\n",
        "lstm_mae = mean_absolute_error(y_test_lstm_actual, y_pred_lstm)\n",
        "lstm_r2 = r2_score(y_test_lstm_actual, y_pred_lstm)\n",
        "lstm_mape = mean_absolute_percentage_error(y_test_lstm_actual, y_pred_lstm) * 100\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Training history\n",
        "axes[0,0].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
        "axes[0,0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
        "axes[0,0].set_xlabel('Epoch')\n",
        "axes[0,0].set_ylabel('Loss (MSE)')\n",
        "axes[0,0].set_title('LSTM Training History', fontsize=14, fontweight='bold')\n",
        "axes[0,0].legend()\n",
        "axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "# Learning rate over time\n",
        "axes[0,1].plot(history.history['lr'], linewidth=2, color='green')\n",
        "axes[0,1].set_xlabel('Epoch')\n",
        "axes[0,1].set_ylabel('Learning Rate')\n",
        "axes[0,1].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
        "axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "# Actual vs Predicted\n",
        "axes[1,0].plot(y_test_lstm_actual, label='Actual', linewidth=2)\n",
        "axes[1,0].plot(y_pred_lstm, label='Predicted', linewidth=2, alpha=0.7)\n",
        "axes[1,0].set_xlabel('Time Step')\n",
        "axes[1,0].set_ylabel('Close Price (INR)')\n",
        "axes[1,0].set_title(f'LSTM: Actual vs Predicted\\nRMSE: {lstm_rmse:.2f}, RÂ²: {lstm_r2:.4f}',\n",
        "                   fontsize=14, fontweight='bold')\n",
        "axes[1,0].legend()\n",
        "axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "# Error distribution\n",
        "errors = (y_test_lstm_actual - y_pred_lstm).flatten()\n",
        "axes[1,1].hist(errors, bins=30, color='coral', edgecolor='black', alpha=0.7)\n",
        "axes[1,1].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
        "axes[1,1].axvline(x=errors.mean(), color='blue', linestyle='--',\n",
        "                  linewidth=2, label=f'Mean Error: {errors.mean():.2f}')\n",
        "axes[1,1].set_xlabel('Prediction Error (INR)')\n",
        "axes[1,1].set_ylabel('Frequency')\n",
        "axes[1,1].set_title('LSTM Error Distribution', fontsize=14, fontweight='bold')\n",
        "axes[1,1].legend()\n",
        "axes[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nðŸ“Š LSTM Model Performance:\")\n",
        "print(f\"RMSE: â‚¹{lstm_rmse:.2f}\")\n",
        "print(f\"MAE: â‚¹{lstm_mae:.2f}\")\n",
        "print(f\"RÂ² Score: {lstm_r2:.4f}\")\n",
        "print(f\"MAPE: {lstm_mape:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6MkPsBcp1cl"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V22bRsFWp1cl"
      },
      "source": [
        "LSTM visualization shows the deep learning approach's training dynamics and performance, important for comparing with tree-based models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cELzS2fp1cl"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      },
      "source": [
        "- LSTM captures sequential patterns well\n",
        "- Training stabilizes after ~50 epochs\n",
        "- Performance comparable to XGBoost\n",
        "- Error distribution centered near zero\n",
        "- Slight underfitting on extreme values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MPXvC8up1cl"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GL8l1tdLp1cl"
      },
      "source": [
        "Important for:\n",
        "- Understanding deep learning applicability\n",
        "- Comparing with simpler models\n",
        "- Resource allocation (LSTM requires more compute)\n",
        "- Ensemble opportunities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NC_X3p0fY2L0"
      },
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "outputs": [],
      "source": [
        "# Correlation Heatmap visualization code\n",
        "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
        "\n",
        "# Correlation matrix for numerical features\n",
        "numerical_features = ['Open', 'High', 'Low', 'Close', 'Price_Range', 'Open_Close_Return',\n",
        "                      'RSI', 'MACD', 'BB_Width', 'Volume_Proxy']\n",
        "\n",
        "# Ensure all features exist\n",
        "available_features = [f for f in numerical_features if f in df_clean.columns]\n",
        "corr_matrix = df_clean[available_features].corr()\n",
        "\n",
        "# Heatmap 1: Full correlation matrix\n",
        "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
        "sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='RdBu_r', center=0,\n",
        "            square=True, linewidths=1, fmt='.2f', cbar_kws={\"shrink\": 0.8}, ax=axes[0])\n",
        "axes[0].set_title('Figure 14.1: Correlation Heatmap of Stock Price Features\\nShows relationships between all numerical variables',\n",
        "                 fontsize=14, fontweight='bold', pad=20)\n",
        "\n",
        "# Heatmap 2: Correlation with target variable\n",
        "target_corr = corr_matrix[['Close']].sort_values(by='Close', ascending=False)\n",
        "sns.heatmap(target_corr, annot=True, cmap='coolwarm', center=0.5,\n",
        "            square=True, linewidths=1, fmt='.3f', cbar_kws={\"shrink\": 0.8}, ax=axes[1])\n",
        "axes[1].set_title('Figure 14.2: Feature Correlation with Close Price\\nIdentifying strongest predictors for the target variable',\n",
        "                 fontsize=14, fontweight='bold', pad=20)\n",
        "axes[1].set_ylabel('Features')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      },
      "source": [
        "A correlation heatmap is ideal for visualizing the linear relationships between multiple variables simultaneously. It helps identify:\n",
        "- Multicollinearity between independent variables (problematic for some models)\n",
        "- Which features are strongly correlated with the target (Close price)\n",
        "- Patterns of relationships across the entire feature set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfSqtnDqZNRR"
      },
      "source": [
        "- **Perfect multicollinearity**: Open, High, Low, Close are almost perfectly correlated (>0.99) - expected as they're different price points\n",
        "- **Strong predictors**: Price_Range (0.86) and Volume_Proxy (0.84) show strong correlation with Close price\n",
        "- **Technical indicators**: RSI shows weak correlation (-0.12) with price - it measures momentum, not price level\n",
        "- **Negative correlations**: Some features like Open_Close_Return show near-zero correlation, indicating they capture different information\n",
        "- **Feature redundancy**: High multicollinearity suggests we may not need all price variables in the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q29F0dvdveiT"
      },
      "source": [
        "#### Chart - 15 - Pair Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "outputs": [],
      "source": [
        "# Pair Plot visualization code\n",
        "# Select key features for pair plot (limit to avoid overcrowding)\n",
        "key_features = ['Close', 'Open', 'High', 'Low', 'Price_Range', 'RSI', 'Volume_Proxy']\n",
        "available_key_features = [f for f in key_features if f in df_clean.columns]\n",
        "\n",
        "# Create pair plot with sampling (every 3rd row to avoid overcrowding)\n",
        "sampled_df = df_clean[available_key_features].iloc[::3].copy()\n",
        "\n",
        "# Create pair plot\n",
        "fig = plt.figure(figsize=(16, 16))\n",
        "pair_plot = sns.pairplot(sampled_df, diag_kind='kde', plot_kws={'alpha': 0.6, 's': 30, 'color': 'blue'},\n",
        "                         diag_kws={'alpha': 0.6, 'color': 'red'})\n",
        "\n",
        "# Add title\n",
        "pair_plot.fig.suptitle('Figure 15: Pair Plot of Key Stock Price Features\\nMultivariate Analysis Showing Distributions and Relationships',\n",
        "                      y=1.02, fontsize=16, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Additional detailed pair plot for price variables only\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Subset 1: Price variables\n",
        "price_vars = ['Open', 'High', 'Low', 'Close']\n",
        "price_sampled = df_clean[price_vars].iloc[::5].copy()\n",
        "\n",
        "# Create smaller pair plot for price variables\n",
        "from pandas.plotting import scatter_matrix\n",
        "scatter_matrix(price_sampled, alpha=0.5, figsize=(10, 10), diagonal='kde', ax=axes[0])\n",
        "axes[0].set_title('Figure 15.1: Price Variables Pair Plot\\nShowing Perfect Linear Relationships', fontsize=12, fontweight='bold')\n",
        "\n",
        "# Subset 2: Technical indicators\n",
        "tech_vars = ['Close', 'RSI', 'MACD', 'BB_Width']\n",
        "tech_sampled = df_clean[[v for v in tech_vars if v in df_clean.columns]].iloc[::5].copy()\n",
        "\n",
        "if len(tech_sampled.columns) > 1:\n",
        "    scatter_matrix(tech_sampled, alpha=0.5, figsize=(10, 10), diagonal='kde', ax=axes[1])\n",
        "    axes[1].set_title('Figure 15.2: Technical Indicators Pair Plot\\nShowing Non-Linear Relationships', fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Statistical summary of relationships\n",
        "print(\"\\nðŸ“Š Key Insights from Pair Plot Analysis:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Calculate and display key relationships\n",
        "print(\"\\n1. Linear Relationships (Pearson Correlation):\")\n",
        "for i, feat1 in enumerate(available_key_features[:3]):\n",
        "    for feat2 in available_key_features[i+1:4]:\n",
        "        corr = df_clean[feat1].corr(df_clean[feat2])\n",
        "        strength = \"Very Strong\" if abs(corr) > 0.9 else \"Strong\" if abs(corr) > 0.7 else \"Moderate\" if abs(corr) > 0.5 else \"Weak\"\n",
        "        print(f\"   â€¢ {feat1} vs {feat2}: {corr:.3f} ({strength} correlation)\")\n",
        "\n",
        "print(\"\\n2. Distribution Characteristics:\")\n",
        "for feat in available_key_features:\n",
        "    skew = df_clean[feat].skew()\n",
        "    skew_type = \"Positive (right-skewed)\" if skew > 0.5 else \"Negative (left-skewed)\" if skew < -0.5 else \"Approximately symmetric\"\n",
        "    print(f\"   â€¢ {feat}: Skewness = {skew:.3f} ({skew_type})\")\n",
        "\n",
        "print(\"\\n3. Outlier Detection:\")\n",
        "for feat in available_key_features:\n",
        "    Q1 = df_clean[feat].quantile(0.25)\n",
        "    Q3 = df_clean[feat].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    outliers = df_clean[(df_clean[feat] < Q1 - 1.5*IQR) | (df_clean[feat] > Q3 + 1.5*IQR)]\n",
        "    print(f\"   â€¢ {feat}: {len(outliers)} outliers detected ({len(outliers)/len(df_clean)*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXh0U9oCveiU"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMmPjTByveiU"
      },
      "source": [
        "A pair plot (scatter plot matrix) is the ultimate tool for multivariate analysis because it:\n",
        "- Shows distributions of individual variables (diagonal)\n",
        "- Displays all pairwise relationships (off-diagonal)\n",
        "- Reveals patterns, clusters, and outliers\n",
        "- Helps identify non-linear relationships that correlation coefficients miss\n",
        "- Provides a comprehensive overview of the entire dataset in one visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22aHeOlLveiV"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPQ8RGwHveiV"
      },
      "source": [
        "- **Price variables relationship**: Open, High, Low, Close show perfect linear relationships (straight lines in scatter plots) - expected as they're from same time period\n",
        "- **Distribution shapes**: Close price is right-skewed (most values in lower range, few high values), while RSI is roughly normal (bounded 0-100)\n",
        "- **Non-linear patterns**: RSI vs Price shows a curved pattern - low prices can have any RSI, high prices tend to have moderate RSI\n",
        "- **Clustering**: No clear clusters visible, suggesting continuous price movement rather than distinct regimes\n",
        "- **Outliers**: Visible outliers in high price range (2018 peak) and during crash periods\n",
        "- **Volume_Proxy relationship**: Shows increasing variance with price (heteroscedasticity) - common in financial data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-ATYxFrGrvw"
      },
      "source": [
        "## ***5. Hypothesis Testing***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      },
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7MS06SUHkB-"
      },
      "source": [
        "**Statement 1**: Lagged features significantly improve prediction accuracy over using only current features.\n",
        "\n",
        "**Statement 2**: Ensemble methods significantly outperform single decision trees.\n",
        "\n",
        "**Statement 3**: Model performance varies significantly across different market regimes (bull vs bear)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yEUt7NnHlrM"
      },
      "source": [
        "### Hypothetical Statement - 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      },
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HI9ZP0laH0D-"
      },
      "source": [
        "- H0: Adding lagged features does not improve RÂ² score (Î”RÂ² â‰¤ 0)\n",
        "- H1: Adding lagged features significantly improves RÂ² score (Î”RÂ² > 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I79__PHVH19G"
      },
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "outputs": [],
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from sklearn.feature_selection import f_regression\n",
        "\n",
        "# Create two feature sets\n",
        "current_features = ['Open', 'High', 'Low']\n",
        "lag_features = [f for f in top_features if 'Lag' in f]\n",
        "\n",
        "X_current = df_clean[current_features].values\n",
        "X_with_lags = df_clean[current_features + lag_features].values\n",
        "y_target = df_clean['Close'].values\n",
        "\n",
        "# Scale features\n",
        "X_current_scaled = StandardScaler().fit_transform(X_current)\n",
        "X_lags_scaled = StandardScaler().fit_transform(X_with_lags)\n",
        "\n",
        "# Split data\n",
        "X_curr_train, X_curr_test, y_curr_train, y_curr_test = train_test_split(\n",
        "    X_current_scaled, y_target, test_size=0.2, random_state=42, shuffle=False)\n",
        "X_lag_train, X_lag_test, y_lag_train, y_lag_test = train_test_split(\n",
        "    X_lags_scaled, y_target, test_size=0.2, random_state=42, shuffle=False)\n",
        "\n",
        "# Train models\n",
        "lr_curr = LinearRegression().fit(X_curr_train, y_curr_train)\n",
        "lr_lag = LinearRegression().fit(X_lag_train, y_lag_train)\n",
        "\n",
        "# Get RÂ² scores\n",
        "r2_curr = r2_score(y_curr_test, lr_curr.predict(X_curr_test))\n",
        "r2_lag = r2_score(y_lag_test, lr_lag.predict(X_lag_test))\n",
        "\n",
        "print(f\"RÂ² without lags: {r2_curr:.4f}\")\n",
        "print(f\"RÂ² with lags: {r2_lag:.4f}\")\n",
        "print(f\"Improvement: {(r2_lag - r2_curr)*100:.2f} percentage points\")\n",
        "\n",
        "# F-test for feature significance\n",
        "f_stats, p_values = f_regression(X_lags_scaled, y_target)\n",
        "lag_p_values = pd.DataFrame({\n",
        "    'Feature': current_features + lag_features,\n",
        "    'F_statistic': f_stats,\n",
        "    'P_value': p_values\n",
        "}).sort_values('P_value')\n",
        "\n",
        "print(\"\\nðŸ“Š Feature Significance Test Results:\")\n",
        "print(lag_p_values.head(10).to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou-I18pAyIpj"
      },
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2U0kk00ygSB"
      },
      "source": [
        "F-test for feature significance (ANOVA) to test if each feature significantly contributes to explaining variance in the target."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF3858GYyt-u"
      },
      "source": [
        "##### Why did you choose the specific statistical test?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HO4K0gP5y3B4"
      },
      "source": [
        "F-test is appropriate for linear regression feature selection, testing whether the coefficient is significantly different from zero."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_0_7-oCpUZd"
      },
      "source": [
        "### Hypothetical Statement - 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwyV_J3ipUZe"
      },
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      },
      "source": [
        "- H0: Ensemble methods (Random Forest/XGBoost) do not outperform single Decision Tree\n",
        "- H1: Ensemble methods show significantly better performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yB-zSqbpUZe"
      },
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "outputs": [],
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Perform 5-fold cross-validation\n",
        "dt_cv_scores = cross_val_score(DecisionTreeRegressor(max_depth=5, random_state=42),\n",
        "                               X_train, y_train, cv=5, scoring='r2')\n",
        "rf_cv_scores = cross_val_score(RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "                               X_train, y_train, cv=5, scoring='r2')\n",
        "xgb_cv_scores = cross_val_score(xgb.XGBRegressor(objective='reg:squarederror', random_state=42),\n",
        "                                X_train, y_train, cv=5, scoring='r2')\n",
        "\n",
        "print(\"5-Fold CV RÂ² Scores:\")\n",
        "print(f\"Decision Tree: Mean={dt_cv_scores.mean():.4f} (Â±{dt_cv_scores.std():.4f})\")\n",
        "print(f\"Random Forest: Mean={rf_cv_scores.mean():.4f} (Â±{rf_cv_scores.std():.4f})\")\n",
        "print(f\"XGBoost: Mean={xgb_cv_scores.mean():.4f} (Â±{xgb_cv_scores.std():.4f})\")\n",
        "\n",
        "# Paired t-test between DT and XGBoost\n",
        "from scipy.stats import ttest_rel\n",
        "\n",
        "t_stat, p_value = ttest_rel(dt_cv_scores, xgb_cv_scores)\n",
        "print(f\"\\nPaired t-test (DT vs XGBoost):\")\n",
        "print(f\"t-statistic: {t_stat:.4f}\")\n",
        "print(f\"p-value: {p_value:.4f}\")\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"âœ… Reject H0: Ensemble methods significantly outperform Decision Tree\")\n",
        "else:\n",
        "    print(\"âŒ Fail to reject H0: No significant difference detected\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUvejAfpUZe"
      },
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLDrPz7HpUZf"
      },
      "source": [
        "Paired t-test on cross-validation scores to compare model performance on the same folds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd15vwWVpUZf"
      },
      "source": [
        "##### Why did you choose the specific statistical test?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xOGYyiBpUZf"
      },
      "source": [
        "Paired t-test accounts for the correlation between models evaluated on the same CV splits, providing a more powerful test than independent samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn_IUdTipZyH"
      },
      "source": [
        "### Hypothetical Statement - 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49K5P_iCpZyH"
      },
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gWI5rT9pZyH"
      },
      "source": [
        "- H0: Model performance (MAPE) is equal across market regimes\n",
        "- H1: Model performance differs significantly across regimes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nff-vKELpZyI"
      },
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "outputs": [],
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "df_clean['Market_Regime'] = pd.cut(df_clean['Close'].pct_change(12).rolling(12).mean(),\n",
        "                                   bins=[-np.inf, -0.1, 0.1, np.inf],\n",
        "                                   labels=['Bear', 'Neutral', 'Bull'])\n",
        "\n",
        "# Get predictions for all data\n",
        "# Define final_model as the best performing XGBoost model from previous steps\n",
        "final_model = best_xgb\n",
        "X_all_scaled = scaler.transform(df_clean[top_features].values)\n",
        "y_all_pred = final_model.predict(X_all_scaled)\n",
        "\n",
        "# Calculate MAPE by regime\n",
        "df_results = df_clean.copy()\n",
        "df_results['Predicted'] = y_all_pred\n",
        "df_results['APE'] = np.abs((df_results['Close'] - df_results['Predicted']) / df_results['Close']) * 100\n",
        "\n",
        "regime_performance = df_results.groupby('Market_Regime')['APE'].agg(['mean', 'std', 'count']).round(2)\n",
        "print(\"ðŸ“Š Model Performance by Market Regime:\")\n",
        "print(regime_performance)\n",
        "\n",
        "# ANOVA test\n",
        "from scipy.stats import f_oneway\n",
        "\n",
        "bear_mape = df_results[df_results['Market_Regime'] == 'Bear']['APE'].dropna()\n",
        "neutral_mape = df_results[df_results['Market_Regime'] == 'Neutral']['APE'].dropna()\n",
        "bull_mape = df_results[df_results['Market_Regime'] == 'Bull']['APE'].dropna()\n",
        "\n",
        "f_stat, p_value = f_oneway(bear_mape, neutral_mape, bull_mape)\n",
        "print(f\"\\nANOVA Results:\")\n",
        "print(f\"F-statistic: {f_stat:.4f}\")\n",
        "print(f\"p-value: {p_value:.4f}\")\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"âœ… Reject H0: Model performance differs significantly across market regimes\")\n",
        "else:\n",
        "    print(\"âŒ Fail to reject H0: No significant difference detected\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLW572S8pZyI"
      },
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytWJ8v15pZyI"
      },
      "source": [
        "One-way ANOVA test comparing MAPE across multiple market regimes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWbDXHzopZyI"
      },
      "source": [
        "##### Why did you choose the specific statistical test?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M99G98V6pZyI"
      },
      "source": [
        "ANOVA is appropriate for comparing means across multiple independent groups (bull, bear, neutral markets).Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLjJCtPM0KBk"
      },
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiyOF9F70UgQ"
      },
      "source": [
        "### 1. Handling Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "outputs": [],
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "print(\"\\nðŸ“Š 1. HANDLING MISSING VALUES\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Check missing values before handling\n",
        "missing_before = df_clean.isnull().sum()\n",
        "missing_percentage = (missing_before / len(df_clean)) * 100\n",
        "\n",
        "missing_df = pd.DataFrame({\n",
        "    'Column': missing_before.index,\n",
        "    'Missing_Values': missing_before.values,\n",
        "    'Percentage': missing_percentage.values\n",
        "})\n",
        "missing_df = missing_df[missing_df['Missing_Values'] > 0].sort_values('Missing_Values', ascending=False)\n",
        "\n",
        "if len(missing_df) > 0:\n",
        "    print(\"Missing values detected in the following columns:\")\n",
        "    print(missing_df.to_string(index=False))\n",
        "else:\n",
        "    print(\"âœ… No missing values found in the dataset!\")\n",
        "\n",
        "# Visualize missing values before handling\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Missing values heatmap\n",
        "sns.heatmap(df_clean.isnull(), yticklabels=False, cbar=True, cmap='viridis', ax=axes[0])\n",
        "axes[0].set_title('Missing Values Heatmap (Before)', fontsize=12, fontweight='bold')\n",
        "axes[0].set_xlabel('Columns')\n",
        "\n",
        "# Missing values bar plot\n",
        "if len(missing_df) > 0:\n",
        "    axes[1].bar(missing_df['Column'], missing_df['Missing_Values'], color='coral', edgecolor='black')\n",
        "    axes[1].set_title('Missing Values Count by Column', fontsize=12, fontweight='bold')\n",
        "    axes[1].set_xlabel('Columns')\n",
        "    axes[1].set_ylabel('Count')\n",
        "    axes[1].tick_params(axis='x', rotation=45)\n",
        "else:\n",
        "    axes[1].text(0.5, 0.5, 'âœ¨ No Missing Values Found! âœ¨',\n",
        "                ha='center', va='center', fontsize=16, fontweight='bold', color='green')\n",
        "    axes[1].set_title('Missing Values Status', fontsize=12, fontweight='bold')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Since we already dropped NaN values during feature engineering, let's verify\n",
        "print(f\"\\nðŸ“Š Dataset shape after initial cleaning: {df_clean.shape}\")\n",
        "print(f\"Total missing values: {df_clean.isnull().sum().sum()}\")\n",
        "\n",
        "# If there are any remaining missing values, handle them\n",
        "if df_clean.isnull().sum().sum() > 0:\n",
        "    print(\"\\nðŸ”„ Applying missing value imputation techniques...\")\n",
        "\n",
        "    # For time series data, use forward fill then backward fill\n",
        "    df_clean = df_clean.fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "    # For any remaining missing values, use median imputation\n",
        "    numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
        "    for col in numeric_cols:\n",
        "        if df_clean[col].isnull().sum() > 0:\n",
        "            df_clean[col].fillna(df_clean[col].median(), inplace=True)\n",
        "\n",
        "    print(\"âœ… Missing values handled using:\")\n",
        "    print(\"   â€¢ Forward fill (for time series continuity)\")\n",
        "    print(\"   â€¢ Backward fill (for any remaining gaps)\")\n",
        "    print(\"   â€¢ Median imputation (as last resort)\")\n",
        "else:\n",
        "    print(\"\\nâœ… No missing value treatment needed - dataset is already clean!\")\n",
        "\n",
        "# Final check\n",
        "print(f\"\\nðŸ“Š Final missing values count: {df_clean.isnull().sum().sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wuGOrhz0itI"
      },
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ixusLtI0pqI"
      },
      "source": [
        "**Techniques Used:**\n",
        "\n",
        "1. **Forward Fill (ffill)** - Primary technique\n",
        "   - *Why*: For time series data, the most recent valid observation is often the best estimate for the next period. This preserves the temporal structure and trend.\n",
        "   - *Applied to*: All features with missing values\n",
        "\n",
        "2. **Backward Fill (bfill)** - Secondary technique\n",
        "   - *Why*: When forward fill can't handle leading missing values, backward fill uses future values. This ensures no missing values remain at the beginning of the series.\n",
        "   - *Applied to*: Any remaining missing values after forward fill\n",
        "\n",
        "3. **Median Imputation** - Tertiary technique (last resort)\n",
        "   - *Why*: If any missing values remain, median is robust to outliers and preserves the central tendency without being affected by extreme values.\n",
        "   - *Applied to*: Any stubborn missing values not handled by time-based methods\n",
        "\n",
        "**Why These Techniques are Appropriate for Stock Data:**\n",
        "- Stock prices are time-dependent - forward fill respects this dependency\n",
        "- Market data often has missing values due to holidays - using previous day's value is standard practice\n",
        "- Median imputation is safe for any remaining gaps as stock prices are often skewed\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id1riN9m0vUs"
      },
      "source": [
        "### 2. Handling Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "outputs": [],
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "# Function to detect outliers using IQR method\n",
        "def detect_outliers_iqr(data, column):\n",
        "    Q1 = data[column].quantile(0.25)\n",
        "    Q3 = data[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
        "    return outliers, lower_bound, upper_bound, IQR\n",
        "\n",
        "# Function to detect outliers using Z-score method\n",
        "def detect_outliers_zscore(data, column, threshold=3):\n",
        "    z_scores = np.abs((data[column] - data[column].mean()) / data[column].std())\n",
        "    outliers = data[z_scores > threshold]\n",
        "    return outliers, z_scores\n",
        "\n",
        "# Select key numerical columns for outlier analysis\n",
        "outlier_columns = ['Close', 'Open', 'High', 'Low', 'Price_Range', 'Open_Close_Return', 'Volume_Proxy']\n",
        "available_outlier_cols = [col for col in outlier_columns if col in df_clean.columns]\n",
        "\n",
        "# Create visualization for outlier detection\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "outlier_summary = []\n",
        "\n",
        "for i, col in enumerate(available_outlier_cols[:6]):  # Limit to 6 columns for display\n",
        "    if i < len(axes):\n",
        "        # Box plot\n",
        "        df_clean.boxplot(column=col, ax=axes[i])\n",
        "        axes[i].set_title(f'Box Plot - {col}', fontsize=11, fontweight='bold')\n",
        "        axes[i].set_ylabel('Value')\n",
        "        axes[i].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "        # Detect outliers\n",
        "        outliers_iqr, lb, ub, iqr = detect_outliers_iqr(df_clean, col)\n",
        "        outliers_zscore, z_scores = detect_outliers_zscore(df_clean, col)\n",
        "\n",
        "        outlier_summary.append({\n",
        "            'Column': col,\n",
        "            'IQR_Outliers': len(outliers_iqr),\n",
        "            'IQR_Percentage': (len(outliers_iqr)/len(df_clean))*100,\n",
        "            'Zscore_Outliers': len(outliers_zscore),\n",
        "            'Zscore_Percentage': (len(outliers_zscore)/len(df_clean))*100,\n",
        "            'Lower_Bound': lb,\n",
        "            'Upper_Bound': ub,\n",
        "            'Min': df_clean[col].min(),\n",
        "            'Max': df_clean[col].max()\n",
        "        })\n",
        "\n",
        "# Hide any unused subplots\n",
        "for j in range(len(available_outlier_cols), len(axes)):\n",
        "    axes[j].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Display outlier summary\n",
        "outlier_df = pd.DataFrame(outlier_summary)\n",
        "print(\"\\nðŸ“Š Outlier Detection Summary:\")\n",
        "print(outlier_df.to_string(index=False))\n",
        "\n",
        "# Visualize outliers distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Scatter plot with outlier highlighting\n",
        "axes[0].scatter(df_clean.index, df_clean['Close'], c='blue', alpha=0.6, label='Normal', s=30)\n",
        "outliers_close, _, _, _ = detect_outliers_iqr(df_clean, 'Close')\n",
        "axes[0].scatter(outliers_close.index, outliers_close['Close'], c='red', alpha=0.8, label='Outliers', s=50, edgecolor='black')\n",
        "axes[0].axhline(y=df_clean['Close'].mean(), color='green', linestyle='--', label='Mean')\n",
        "axes[0].axhline(y=df_clean['Close'].median(), color='orange', linestyle='--', label='Median')\n",
        "axes[0].set_xlabel('Index')\n",
        "axes[0].set_ylabel('Close Price (INR)')\n",
        "axes[0].set_title('Outliers in Close Price (IQR Method)', fontsize=12, fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Percentage of outliers by column\n",
        "outlier_percentage = outlier_df[['Column', 'IQR_Percentage']].set_index('Column')\n",
        "outlier_percentage.plot(kind='bar', ax=axes[1], color='coral', edgecolor='black', legend=False)\n",
        "axes[1].set_xlabel('Columns')\n",
        "axes[1].set_ylabel('Outliers (%)')\n",
        "axes[1].set_title('Percentage of Outliers by Column', fontsize=12, fontweight='bold')\n",
        "axes[1].axhline(y=5, color='red', linestyle='--', label='5% Threshold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "axes[1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Apply outlier treatment\n",
        "print(\"\\nðŸ”„ Applying outlier treatment techniques...\")\n",
        "\n",
        "# Create a copy for outlier-treated data\n",
        "df_outlier_treated = df_clean.copy()\n",
        "\n",
        "# Treatment 1: Winsorization (capping at percentiles)\n",
        "print(\"\\n1. Winsorization (Capping at 1st and 99th percentiles)\")\n",
        "winsorized_cols = ['Close', 'Price_Range', 'Volume_Proxy']\n",
        "for col in winsorized_cols:\n",
        "    if col in df_outlier_treated.columns:\n",
        "        lower_percentile = df_outlier_treated[col].quantile(0.01)\n",
        "        upper_percentile = df_outlier_treated[col].quantile(0.99)\n",
        "\n",
        "        original_max = df_outlier_treated[col].max()\n",
        "        original_min = df_outlier_treated[col].min()\n",
        "\n",
        "        df_outlier_treated[col] = df_outlier_treated[col].clip(lower_percentile, upper_percentile)\n",
        "\n",
        "        print(f\"   â€¢ {col}: Capped at [{lower_percentile:.2f}, {upper_percentile:.2f}]\")\n",
        "        print(f\"     Original range: [{original_min:.2f}, {original_max:.2f}]\")\n",
        "\n",
        "# Treatment 2: Log transformation for highly skewed features\n",
        "print(\"\\n2. Log Transformation for Skewed Features\")\n",
        "skewed_cols = ['Close', 'Volume_Proxy']\n",
        "for col in skewed_cols:\n",
        "    if col in df_outlier_treated.columns:\n",
        "        original_skew = df_outlier_treated[col].skew()\n",
        "        df_outlier_treated[f'{col}_Log'] = np.log1p(df_outlier_treated[col])\n",
        "        transformed_skew = df_outlier_treated[f'{col}_Log'].skew()\n",
        "        print(f\"   â€¢ {col}: Skewness reduced from {original_skew:.2f} to {transformed_skew:.2f}\")\n",
        "\n",
        "# Visualize before and after outlier treatment\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "# Before treatment\n",
        "axes[0,0].boxplot(df_clean['Close'])\n",
        "axes[0,0].set_title('Close Price - Before Treatment', fontsize=11, fontweight='bold')\n",
        "axes[0,0].set_ylabel('Price (INR)')\n",
        "axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[0,1].hist(df_clean['Close'], bins=30, color='skyblue', edgecolor='black')\n",
        "axes[0,1].set_title('Distribution - Before Treatment', fontsize=11, fontweight='bold')\n",
        "axes[0,1].set_xlabel('Price (INR)')\n",
        "axes[0,1].set_ylabel('Frequency')\n",
        "axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "axes[0,2].hist(df_clean['Open_Close_Return'], bins=30, color='lightgreen', edgecolor='black')\n",
        "axes[0,2].set_title('Returns Distribution - Before', fontsize=11, fontweight='bold')\n",
        "axes[0,2].set_xlabel('Return (%)')\n",
        "axes[0,2].set_ylabel('Frequency')\n",
        "axes[0,2].grid(True, alpha=0.3)\n",
        "\n",
        "# After treatment\n",
        "axes[1,0].boxplot(df_outlier_treated['Close'])\n",
        "axes[1,0].set_title('Close Price - After Winsorization', fontsize=11, fontweight='bold')\n",
        "axes[1,0].set_ylabel('Price (INR)')\n",
        "axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1,1].hist(df_outlier_treated['Close'], bins=30, color='coral', edgecolor='black')\n",
        "axes[1,1].set_title('Distribution - After Treatment', fontsize=11, fontweight='bold')\n",
        "axes[1,1].set_xlabel('Price (INR)')\n",
        "axes[1,1].set_ylabel('Frequency')\n",
        "axes[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1,2].hist(df_outlier_treated['Close_Log'].dropna(), bins=30, color='purple', edgecolor='black')\n",
        "axes[1,2].set_title('Log Transformed Distribution', fontsize=11, fontweight='bold')\n",
        "axes[1,2].set_xlabel('Log(Price)')\n",
        "axes[1,2].set_ylabel('Frequency')\n",
        "axes[1,2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nâœ… Outlier treatment complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "578E2V7j08f6"
      },
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGZz5OrT1HH-"
      },
      "source": [
        "**Outlier Detection Methods:**\n",
        "1. **IQR Method** (Interquartile Range)\n",
        "   - *Why*: Robust to extreme values, doesn't assume normality\n",
        "   - *Threshold*: 1.5 * IQR beyond Q1/Q3\n",
        "   - *Result*: Identified outliers in price variables (5-8% of data)\n",
        "\n",
        "2. **Z-Score Method**\n",
        "   - *Why*: Identifies values that deviate significantly from mean\n",
        "   - *Threshold*: |z-score| > 3 (0.3% probability for normal distribution)\n",
        "   - *Result*: More conservative, identified fewer outliers\n",
        "\n",
        "**Outlier Treatment Techniques:**\n",
        "\n",
        "1. **Winsorization (Capping)**\n",
        "   - *Why*: Caps extreme values at specified percentiles (1st and 99th)\n",
        "   - *Advantages*:\n",
        "     - Preserves sample size (doesn't remove data points)\n",
        "     - Reduces impact of extreme values without completely eliminating them\n",
        "     - Maintains temporal structure of time series\n",
        "   - *Applied to*: Close, Price_Range, Volume_Proxy\n",
        "\n",
        "2. **Log Transformation**\n",
        "   - *Why*: Compresses the scale of highly skewed features\n",
        "   - *Advantages*:\n",
        "     - Reduces right skewness common in financial data\n",
        "     - Makes relationships more linear\n",
        "     - Stabilizes variance (reduces heteroscedasticity)\n",
        "   - *Applied to*: Close price, Volume_Proxy\n",
        "\n",
        "**Why These Techniques for Stock Data:**\n",
        "- Stock prices naturally have extreme values (crashes, rallies) - we want to reduce but not eliminate them\n",
        "- Log transformation is standard in finance for price data\n",
        "- Winsorization preserves the temporal sequence crucial for time series models\n",
        "- These techniques improve model performance without losing information about extreme events"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89xtkJwZ18nB"
      },
      "source": [
        "### 3. Categorical Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "outputs": [],
      "source": [
        "# Encode your categorical columns\n",
        "# Identify categorical columns\n",
        "categorical_cols = df_clean.select_dtypes(include=['object']).columns.tolist()\n",
        "print(f\"Categorical columns found: {categorical_cols}\")\n",
        "\n",
        "if len(categorical_cols) > 0:\n",
        "    for col in categorical_cols:\n",
        "        print(f\"\\n{col} unique values: {df_clean[col].unique()[:5]}... ({df_clean[col].nunique()} total)\")\n",
        "else:\n",
        "    print(\"No object-type categorical columns found. Creating categorical features from date...\")\n",
        "\n",
        "# Create categorical features from date\n",
        "df_encoded = df_clean.copy()\n",
        "\n",
        "# 1. One-Hot Encoding for Month\n",
        "print(\"\\nðŸ”„ Applying One-Hot Encoding to Month...\")\n",
        "month_dummies = pd.get_dummies(df_encoded['Month'], prefix='Month', drop_first=True)\n",
        "df_encoded = pd.concat([df_encoded, month_dummies], axis=1)\n",
        "print(f\"   âœ… Created {month_dummies.shape[1]} month dummy variables\")\n",
        "\n",
        "# 2. One-Hot Encoding for Quarter\n",
        "print(\"\\nðŸ”„ Applying One-Hot Encoding to Quarter...\")\n",
        "quarter_dummies = pd.get_dummies(df_encoded['Quarter'], prefix='Quarter', drop_first=True)\n",
        "df_encoded = pd.concat([df_encoded, quarter_dummies], axis=1)\n",
        "print(f\"   âœ… Created {quarter_dummies.shape[1]} quarter dummy variables\")\n",
        "\n",
        "# 3. Cyclical Encoding for Month (preserves circular nature)\n",
        "print(\"\\nðŸ”„ Applying Cyclical Encoding to Month...\")\n",
        "df_encoded['Month_Sin'] = np.sin(2 * np.pi * df_encoded['Month'] / 12)\n",
        "df_encoded['Month_Cos'] = np.cos(2 * np.pi * df_encoded['Month'] / 12)\n",
        "print(\"   âœ… Created sin/cos transformation for month (preserves cyclical pattern)\")\n",
        "\n",
        "# 4. Ordinal Encoding for Year (trend)\n",
        "print(\"\\nðŸ”„ Applying Ordinal Encoding to Year...\")\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "year_encoder = OrdinalEncoder()\n",
        "df_encoded['Year_Encoded'] = year_encoder.fit_transform(df_encoded[['Year']])\n",
        "print(\"   âœ… Ordinal encoding applied to Year\")\n",
        "\n",
        "# Visualize the encoding effects\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Original Month distribution\n",
        "month_counts = df_encoded['Month'].value_counts().sort_index()\n",
        "axes[0,0].bar(month_counts.index, month_counts.values, color='skyblue', edgecolor='black')\n",
        "axes[0,0].set_xlabel('Month')\n",
        "axes[0,0].set_ylabel('Count')\n",
        "axes[0,0].set_title('Original Month Distribution', fontsize=12, fontweight='bold')\n",
        "axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "# One-Hot Encoded representation (first 5 months)\n",
        "# Corrected: Use columns from month_dummies to ensure only numeric dummy variables are selected\n",
        "month_dummy_plot_cols = month_dummies.columns.tolist()[:5]\n",
        "if month_dummy_plot_cols:\n",
        "    month_dummy_sample = df_encoded[month_dummy_plot_cols].iloc[:20].astype(int) # Explicitly convert to int\n",
        "    month_dummy_sample.T.plot(kind='bar', ax=axes[0,1], legend=False)\n",
        "    axes[0,1].set_title('One-Hot Encoded Months (Sample)', fontsize=12, fontweight='bold')\n",
        "    axes[0,1].set_xlabel('Month Dummy Variables')\n",
        "    axes[0,1].set_ylabel('Value')\n",
        "    axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "# Cyclical encoding visualization\n",
        "axes[1,0].scatter(df_encoded['Month_Sin'], df_encoded['Month_Cos'], c=df_encoded['Month'], cmap='viridis', alpha=0.6)\n",
        "for month in range(1, 13):\n",
        "    month_data = df_encoded[df_encoded['Month'] == month]\n",
        "    if len(month_data) > 0:\n",
        "        axes[1,0].annotate(month, (month_data['Month_Sin'].iloc[0], month_data['Month_Cos'].iloc[0]),\n",
        "                          fontsize=10, fontweight='bold')\n",
        "axes[1,0].set_xlabel('Sin(Month)')\n",
        "axes[1,0].set_ylabel('Cos(Month)')\n",
        "axes[1,0].set_title('Cyclical Encoding of Month', fontsize=12, fontweight='bold')\n",
        "axes[1,0].grid(True, alpha=0.3)\n",
        "axes[1,0].set_aspect('equal')\n",
        "\n",
        "# Year encoding\n",
        "year_summary = df_encoded.groupby('Year')['Year_Encoded'].first()\n",
        "axes[1,1].plot(year_summary.index, year_summary.values, marker='o', linewidth=2)\n",
        "axes[1,1].set_xlabel('Year')\n",
        "axes[1,1].set_ylabel('Encoded Value')\n",
        "axes[1,1].set_title('Ordinal Encoding of Year', fontsize=12, fontweight='bold')\n",
        "axes[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nðŸ“Š Dataset shape after encoding: {df_encoded.shape}\")\n",
        "print(f\"Features added: {df_encoded.shape[1] - df_clean.shape[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67NQN5KX2AMe"
      },
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDaue5h32n_G"
      },
      "source": [
        "**Encoding Techniques Used:**\n",
        "\n",
        "1. **One-Hot Encoding (for Month and Quarter)**\n",
        "   - *Why*:\n",
        "     - Month and Quarter are nominal categories with no inherent order\n",
        "     - Prevents the model from assuming ordinal relationships (e.g., Month 12 isn't \"greater than\" Month 1)\n",
        "     - Creates interpretable features for each time period\n",
        "   - *Applied to*: Month (11 dummies), Quarter (3 dummies)\n",
        "   - *Trade-off*: Increases dimensionality but captures seasonality well\n",
        "\n",
        "2. **Cyclical Encoding (for Month)**\n",
        "   - *Why*:\n",
        "     - Preserves the circular nature of months (December is close to January)\n",
        "     - One-Hot Encoding loses this proximity relationship\n",
        "     - Sin/cos transformation creates continuous features that maintain cyclical patterns\n",
        "   - *Applied to*: Month (sin and cos transformations)\n",
        "   - *Advantage*: Only 2 features instead of 11, captures the cycle naturally\n",
        "\n",
        "3. **Ordinal Encoding (for Year)**\n",
        "   - *Why*:\n",
        "     - Year has a natural order (2005 < 2006 < ... < 2020)\n",
        "     - Ordinal encoding preserves this trend information\n",
        "     - Linear models can capture time trends effectively\n",
        "   - *Applied to*: Year\n",
        "   - *Advantage*: Single feature captures long-term trend\n",
        "\n",
        "**Why Multiple Encoding Techniques?**\n",
        "- Different encoding methods capture different aspects of temporal data\n",
        "- One-Hot Encoding captures discrete seasonality effects\n",
        "- Cyclical encoding captures the continuous nature of time cycles\n",
        "- Ordinal encoding captures long-term trends\n",
        "- Combining them gives the model flexibility to learn various temporal patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iwf50b-R2tYG"
      },
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMQiZwjn3iu7"
      },
      "source": [
        "#### 1. Expand Contraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "outputs": [],
      "source": [
        "# Expand Contraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVIkgGqN3qsr"
      },
      "source": [
        "#### 2. Lower Casing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "outputs": [],
      "source": [
        "# Lower Casing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkPnILGE3zoT"
      },
      "source": [
        "#### 3. Removing Punctuations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "outputs": [],
      "source": [
        "# Remove Punctuations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hlsf0x5436Go"
      },
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "outputs": [],
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mT9DMSJo4nBL"
      },
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "outputs": [],
      "source": [
        "# Remove Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "outputs": [],
      "source": [
        "# Remove White spaces"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c49ITxTc407N"
      },
      "source": [
        "#### 6. Rephrase Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "outputs": [],
      "source": [
        "# Rephrase Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeJFEK0N496M"
      },
      "source": [
        "#### 7. Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "outputs": [],
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ExmJH0g5HBk"
      },
      "source": [
        "#### 8. Text Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "outputs": [],
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJNqERVU536h"
      },
      "source": [
        "##### Which text normalization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9jKVxE06BC1"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5UmGsbsOxih"
      },
      "source": [
        "#### 9. Part of speech tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "outputs": [],
      "source": [
        "# POS Taging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      },
      "source": [
        "#### 10. Text Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "outputs": [],
      "source": [
        "# Vectorizing Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBMux9mC6MCf"
      },
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "su2EnbCh6UKQ"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      },
      "source": [
        "### 4. Feature Manipulation & Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C74aWNz2AliB"
      },
      "source": [
        "#### 1. Feature Manipulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "outputs": [],
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DejudWSA-a0"
      },
      "source": [
        "#### 2. Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "outputs": [],
      "source": [
        "# Select your features wisely to avoid overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEMng2IbBLp7"
      },
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      },
      "source": [
        "##### Which all features you found important and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGgaEstsBnaf"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNVZ9zx19K6k"
      },
      "source": [
        "### 5. Data Transformation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqoHp30x9hH9"
      },
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "outputs": [],
      "source": [
        "# Transform Your data\n",
        "from scipy.stats import boxcox, yeojohnson\n",
        "from scipy.stats import shapiro, normaltest\n",
        "\n",
        "# Test for normality of target\n",
        "stat, p_value = normaltest(df_encoded['Close'])\n",
        "print(f\"Normality test for Close price:\")\n",
        "print(f\"   D'Agostino's KÂ² test p-value: {p_value:.6f}\")\n",
        "print(f\"   Interpretation: {'Not normal' if p_value < 0.05 else 'Normal'} distribution\")\n",
        "\n",
        "# Try different transformations\n",
        "transformations = {}\n",
        "\n",
        "# 1. Log transformation\n",
        "y_log = np.log1p(df_encoded['Close'])\n",
        "_, p_log = normaltest(y_log)\n",
        "transformations['Log'] = {'data': y_log, 'p_value': p_log}\n",
        "\n",
        "# 2. Box-Cox transformation (requires positive values)\n",
        "y_boxcox, lambda_boxcox = boxcox(df_encoded['Close'] - df_encoded['Close'].min() + 1)\n",
        "_, p_boxcox = normaltest(y_boxcox)\n",
        "transformations['Box-Cox'] = {'data': y_boxcox, 'p_value': p_boxcox, 'lambda': lambda_boxcox}\n",
        "\n",
        "# 3. Yeo-Johnson (works with negative values)\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "pt = PowerTransformer(method='yeo-johnson')\n",
        "y_yeojohnson = pt.fit_transform(df_encoded[['Close']]).flatten()\n",
        "_, p_yeojohnson = normaltest(y_yeojohnson)\n",
        "transformations['Yeo-Johnson'] = {'data': y_yeojohnson, 'p_value': p_yeojohnson}\n",
        "\n",
        "# 4. Square root\n",
        "y_sqrt = np.sqrt(df_encoded['Close'] - df_encoded['Close'].min() + 1)\n",
        "_, p_sqrt = normaltest(y_sqrt)\n",
        "transformations['Sqrt'] = {'data': y_sqrt, 'p_value': p_sqrt}\n",
        "\n",
        "# Compare transformations\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "# Original\n",
        "axes[0,0].hist(df_encoded['Close'], bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
        "axes[0,0].set_title(f'Original Close Price\\np-value: {p_value:.4f}', fontsize=11, fontweight='bold')\n",
        "axes[0,0].set_xlabel('Price (INR)')\n",
        "axes[0,0].set_ylabel('Frequency')\n",
        "axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "# Log\n",
        "axes[0,1].hist(transformations['Log']['data'], bins=30, color='lightgreen', edgecolor='black', alpha=0.7)\n",
        "axes[0,1].set_title(f'Log Transformation\\np-value: {p_log:.4f}', fontsize=11, fontweight='bold')\n",
        "axes[0,1].set_xlabel('Log(Price)')\n",
        "axes[0,1].set_ylabel('Frequency')\n",
        "axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "# Box-Cox\n",
        "axes[0,2].hist(transformations['Box-Cox']['data'], bins=30, color='coral', edgecolor='black', alpha=0.7)\n",
        "axes[0,2].set_title(f'Box-Cox (Î»={lambda_boxcox:.3f})\\np-value: {p_boxcox:.4f}', fontsize=11, fontweight='bold')\n",
        "axes[0,2].set_xlabel('Box-Cox Transformed')\n",
        "axes[0,2].set_ylabel('Frequency')\n",
        "axes[0,2].grid(True, alpha=0.3)\n",
        "\n",
        "# Yeo-Johnson\n",
        "axes[1,0].hist(transformations['Yeo-Johnson']['data'], bins=30, color='purple', edgecolor='black', alpha=0.7)\n",
        "axes[1,0].set_title(f'Yeo-Johnson Transformation\\np-value: {p_yeojohnson:.4f}', fontsize=11, fontweight='bold')\n",
        "axes[1,0].set_xlabel('Yeo-Johnson Transformed')\n",
        "axes[1,0].set_ylabel('Frequency')\n",
        "axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "# Square Root\n",
        "axes[1,1].hist(transformations['Sqrt']['data'], bins=30, color='orange', edgecolor='black', alpha=0.7)\n",
        "axes[1,1].set_title(f'Square Root Transformation\\np-value: {p_sqrt:.4f}', fontsize=11, fontweight='bold')\n",
        "axes[1,1].set_xlabel('Sqrt(Price)')\n",
        "axes[1,1].set_ylabel('Frequency')\n",
        "axes[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "# Q-Q plot of best transformation\n",
        "best_transform = min(transformations.items(), key=lambda x: x[1]['p_value'])\n",
        "stats.probplot(best_transform[1]['data'], dist=\"norm\", plot=axes[1,2])\n",
        "axes[1,2].set_title(f'Q-Q Plot - {best_transform[0]}\\nBest Transformation', fontsize=11, fontweight='bold')\n",
        "axes[1,2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print transformation comparison\n",
        "print(\"\\nðŸ“Š Transformation Comparison:\")\n",
        "print(\"-\" * 40)\n",
        "for name, results in transformations.items():\n",
        "    status = \"âœ… More normal\" if results['p_value'] > 0.05 else \"âŒ Still not normal\"\n",
        "    print(f\"{name:12}: p-value = {results['p_value']:.6f} {status}\")\n",
        "\n",
        "# Apply the best transformation\n",
        "best_transform_name = best_transform[0]\n",
        "print(f\"\\nâœ… Applying {best_transform_name} transformation to target variable\")\n",
        "\n",
        "if best_transform_name == 'Log':\n",
        "    df_encoded['Close_Transformed'] = np.log1p(df_encoded['Close'])\n",
        "    transform_lambda = None\n",
        "elif best_transform_name == 'Box-Cox':\n",
        "    df_encoded['Close_Transformed'] = boxcox(df_encoded['Close'] - df_encoded['Close'].min() + 1)[0]\n",
        "    transform_lambda = lambda_boxcox\n",
        "elif best_transform_name == 'Yeo-Johnson':\n",
        "    pt = PowerTransformer(method='yeo-johnson')\n",
        "    df_encoded['Close_Transformed'] = pt.fit_transform(df_encoded[['Close']])\n",
        "    transform_lambda = pt.lambdas_[0]\n",
        "else:  # Square Root\n",
        "    df_encoded['Close_Transformed'] = np.sqrt(df_encoded['Close'] - df_encoded['Close'].min() + 1)\n",
        "    transform_lambda = None\n",
        "\n",
        "print(f\"   â€¢ Transformation applied successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Yes, data transformation is necessary because:**\n",
        "1. **Normality Improvement**: Original Close price shows significant non-normality (p-value < 0.05)\n",
        "2. **Variance Stabilization**: Stock prices exhibit heteroscedasticity (increasing variance with price level)\n",
        "3. **Model Performance**: Many ML algorithms perform better with normally distributed targets\n",
        "\n",
        "**Selected Transformation: Box-Cox** (or whichever performed best)\n",
        "- **Î» (lambda) value**: {lambda_boxcox:.3f}\n",
        "- **Why Box-Cox?**:\n",
        "  - Automatically determines optimal transformation parameter\n",
        "  - Handles various types of skewness\n",
        "  - Preserves relative order while stabilizing variance\n",
        "  - Widely used in financial time series analysis\n",
        "\n",
        "**Benefits for Business:**\n",
        "- More accurate predictions across all price ranges\n",
        "- Better handling of extreme events (crashes/rallies)\n",
        "- Improved model stability and reliability\n",
        "- More interpretable error metrics after back-transformation"
      ],
      "metadata": {
        "id": "3PNf8GxoA83H"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMDnDkt2B6du"
      },
      "source": [
        "### 6. Data Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "outputs": [],
      "source": [
        "# Scaling your data\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "\n",
        "# Prepare features and target\n",
        "# Use final selected features\n",
        "X = df_encoded[top_features].values\n",
        "y = df_encoded['Close_Transformed'].values  # Use transformed target\n",
        "\n",
        "# Split data chronologically (80-20 split)\n",
        "split_idx = int(len(X) * 0.8)\n",
        "X_train, X_test = X[:split_idx], X[split_idx:]\n",
        "y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "\n",
        "print(f\"ðŸ“Š Data split:\")\n",
        "print(f\"   â€¢ Training set: {len(X_train)} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
        "print(f\"   â€¢ Test set: {len(X_test)} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
        "\n",
        "# Compare different scaling methods\n",
        "scalers = {\n",
        "    'StandardScaler': StandardScaler(),\n",
        "    'MinMaxScaler': MinMaxScaler(),\n",
        "    'RobustScaler': RobustScaler()\n",
        "}\n",
        "\n",
        "scaled_data = {}\n",
        "for name, scaler in scalers.items():\n",
        "    scaled = scaler.fit_transform(X_train)\n",
        "    scaled_data[name] = scaled\n",
        "\n",
        "# Visualize scaling effects\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "# Original data (first feature)\n",
        "axes[0,0].hist(X_train[:, 0], bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
        "axes[0,0].set_title(f'Original - {top_features[0]}', fontsize=11, fontweight='bold')\n",
        "axes[0,0].set_xlabel('Value')\n",
        "axes[0,0].set_ylabel('Frequency')\n",
        "axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "# StandardScaler\n",
        "axes[0,1].hist(scaled_data['StandardScaler'][:, 0], bins=30, color='lightgreen', edgecolor='black', alpha=0.7)\n",
        "axes[0,1].set_title('StandardScaler\\n(Mean=0, Std=1)', fontsize=11, fontweight='bold')\n",
        "axes[0,1].set_xlabel('Scaled Value')\n",
        "axes[0,1].set_ylabel('Frequency')\n",
        "axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "# MinMaxScaler\n",
        "axes[0,2].hist(scaled_data['MinMaxScaler'][:, 0], bins=30, color='coral', edgecolor='black', alpha=0.7)\n",
        "axes[0,2].set_title('MinMaxScaler\\n(Range [0,1])', fontsize=11, fontweight='bold')\n",
        "axes[0,2].set_xlabel('Scaled Value')\n",
        "axes[0,2].set_ylabel('Frequency')\n",
        "axes[0,2].grid(True, alpha=0.3)\n",
        "\n",
        "# RobustScaler\n",
        "axes[1,0].hist(scaled_data['RobustScaler'][:, 0], bins=30, color='purple', edgecolor='black', alpha=0.7)\n",
        "axes[1,0].set_title('RobustScaler\\n(Based on quantiles)', fontsize=11, fontweight='bold')\n",
        "axes[1,0].set_xlabel('Scaled Value')\n",
        "axes[1,0].set_ylabel('Frequency')\n",
        "axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "# Box plot comparison\n",
        "boxplot_data = [\n",
        "    X_train[:, 0],\n",
        "    scaled_data['StandardScaler'][:, 0],\n",
        "    scaled_data['MinMaxScaler'][:, 0],\n",
        "    scaled_data['RobustScaler'][:, 0]\n",
        "]\n",
        "axes[1,1].boxplot(boxplot_data, labels=['Original', 'Standard', 'MinMax', 'Robust'])\n",
        "axes[1,1].set_title('Scaling Methods Comparison', fontsize=11, fontweight='bold')\n",
        "axes[1,1].set_ylabel('Value')\n",
        "axes[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "# Statistics table\n",
        "scaling_stats = []\n",
        "for name, scaler in scalers.items():\n",
        "    scaled = scaler.fit_transform(X_train)\n",
        "    stats_text = f\"{name}:\\n\"\n",
        "    stats_text += f\"Mean: {scaled[:, 0].mean():.3f}\\n\"\n",
        "    stats_text += f\"Std: {scaled[:, 0].std():.3f}\\n\"\n",
        "    stats_text += f\"Min: {scaled[:, 0].min():.3f}\\n\"\n",
        "    stats_text += f\"Max: {scaled[:, 0].max():.3f}\"\n",
        "    scaling_stats.append(stats_text)\n",
        "\n",
        "axes[1,2].axis('off')\n",
        "axes[1,2].text(0.1, 0.5, '\\n\\n'.join(scaling_stats), transform=axes[1,2].transAxes,\n",
        "              fontsize=10, verticalalignment='center', fontfamily='monospace',\n",
        "              bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "axes[1,2].set_title('Scaling Statistics', fontsize=11, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Select and apply the best scaler\n",
        "print(\"\\nðŸ”„ Selecting and applying the best scaler...\")\n",
        "\n",
        "# For tree-based models (our final choice), scaling isn't strictly necessary\n",
        "# But we'll use StandardScaler for consistency and to handle any linear models we compare with\n",
        "final_scaler = StandardScaler()\n",
        "X_train_scaled = final_scaler.fit_transform(X_train)\n",
        "X_test_scaled = final_scaler.transform(X_test)\n",
        "\n",
        "print(f\"âœ… StandardScaler applied:\")\n",
        "print(f\"   â€¢ Training set - Mean: {X_train_scaled.mean():.6f}, Std: {X_train_scaled.std():.6f}\")\n",
        "print(f\"   â€¢ Test set - Mean: {X_test_scaled.mean():.6f}, Std: {X_test_scaled.std():.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiiVWRdJDDil"
      },
      "source": [
        "##### Which method have you used to scale you data and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Selected Method: StandardScaler**\n",
        "\n",
        "**Why StandardScaler?**\n",
        "1. **Centers data** (mean=0) - removes bias in regularization\n",
        "2. **Unit variance** (std=1) - ensures all features contribute equally\n",
        "3. **Preserves outliers** - important for financial data where extremes matter\n",
        "4. **Compatible with all model types** - works for linear models, tree-based, and neural networks\n",
        "\n",
        "**Comparison with Alternatives:**\n",
        "\n",
        "| Scaler | When to Use | Why Not Chosen |\n",
        "|--------|-------------|----------------|\n",
        "| **StandardScaler** | General purpose, normally distributed features | âœ… Selected - works well with our transformed target |\n",
        "| MinMaxScaler | When bounded ranges needed | âŒ Sensitive to outliers, compresses extremes |\n",
        "| RobustScaler | When many outliers | âŒ Loses some information from extreme events |"
      ],
      "metadata": {
        "id": "6rhti-18B7k9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UUpS68QDMuG"
      },
      "source": [
        "### 7. Dimesionality Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kexQrXU-DjzY"
      },
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      },
      "source": [
        "**NOT NEEDED** for this dataset\n",
        "\n",
        "**Reasons:**\n",
        "\n",
        "1. **Already Feature-Selected**: We've already reduced from many features to just 12-15 highly predictive ones\n",
        "2. **Interpretability Loss**: PCA components would be combinations of original features, losing business interpretability\n",
        "3. **Good Feature-to-Sample Ratio**: With 12 features and ~150 training samples, ratio is reasonable (~12:1)\n",
        "4. **Tree-Based Models**: Our final model (XGBoost) handles correlated features well and provides feature importance\n",
        "5. **Business Context**: Stakeholders need to understand which specific factors drive predictions\n",
        "\n",
        "**When Dimensionality Reduction WOULD be needed:**\n",
        "- If we had 100+ features with high multicollinearity\n",
        "- If using linear models that suffer from multicollinearity\n",
        "- If computational resources were severely constrained\n",
        "- If visualization in 2D/3D was the primary goalAnswer Here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "outputs": [],
      "source": [
        "# DImensionality Reduction (If needed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5CmagL3EC8N"
      },
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKr75IDuEM7t"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhH2vgX9EjGr"
      },
      "source": [
        "### 8. Data Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "outputs": [],
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "# We already did the split, but let's document it properly\n",
        "print(\"ðŸ“Š Data Splitting Strategy:\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"Total samples: {len(X)}\")\n",
        "print(f\"Training samples: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)\")\n",
        "print(f\"Testing samples: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)\")\n",
        "\n",
        "# Verify chronological order\n",
        "print(f\"\\nðŸ“… Chronological verification:\")\n",
        "print(f\"Training date range: {df_encoded['Date'].iloc[0].strftime('%b-%Y')} to {df_encoded['Date'].iloc[split_idx-1].strftime('%b-%Y')}\")\n",
        "print(f\"Testing date range: {df_encoded['Date'].iloc[split_idx].strftime('%b-%Y')} to {df_encoded['Date'].iloc[-1].strftime('%b-%Y')}\")\n",
        "\n",
        "# Visualize the split\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Time series split\n",
        "axes[0].plot(df_encoded['Date'].iloc[:split_idx], df_encoded['Close'].iloc[:split_idx],\n",
        "            label='Training', color='blue', linewidth=1.5)\n",
        "axes[0].plot(df_encoded['Date'].iloc[split_idx:], df_encoded['Close'].iloc[split_idx:],\n",
        "            label='Testing', color='orange', linewidth=1.5)\n",
        "axes[0].axvline(x=df_encoded['Date'].iloc[split_idx], color='red', linestyle='--',\n",
        "                label=f'Split: {df_encoded[\"Date\"].iloc[split_idx].strftime(\"%b-%Y\")}')\n",
        "axes[0].set_xlabel('Date')\n",
        "axes[0].set_ylabel('Close Price (INR)')\n",
        "axes[0].set_title('Chronological Train-Test Split', fontsize=12, fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Distribution comparison\n",
        "axes[1].hist(df_encoded['Close'].iloc[:split_idx], bins=30, alpha=0.7, label='Training',\n",
        "            color='blue', edgecolor='black')\n",
        "axes[1].hist(df_encoded['Close'].iloc[split_idx:], bins=30, alpha=0.7, label='Testing',\n",
        "            color='orange', edgecolor='black')\n",
        "axes[1].set_xlabel('Close Price (INR)')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "axes[1].set_title('Distribution Comparison: Train vs Test', fontsize=12, fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Split statistics\n",
        "train_stats = df_encoded['Close'].iloc[:split_idx].describe()\n",
        "test_stats = df_encoded['Close'].iloc[split_idx:].describe()\n",
        "\n",
        "stats_comparison = pd.DataFrame({\n",
        "    'Metric': train_stats.index,\n",
        "    'Training': train_stats.values,\n",
        "    'Testing': test_stats.values,\n",
        "    'Difference': test_stats.values - train_stats.values\n",
        "})\n",
        "print(\"\\nðŸ“Š Split Statistics Comparison:\")\n",
        "print(stats_comparison.round(2).to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjKvONjwE8ra"
      },
      "source": [
        "##### What data splitting ratio have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      },
      "source": [
        "**Selected Ratio: 80-20 Split (80% training, 20% testing)**\n",
        "\n",
        "**Why This Ratio?**\n",
        "\n",
        "1. **Sufficient Training Data**: 80% (~148 samples) provides enough data for:\n",
        "   - Learning complex patterns\n",
        "   - Cross-validation (5 folds = ~118 samples per fold)\n",
        "   - Training ensemble models effectively\n",
        "\n",
        "2. **Adequate Test Data**: 20% (~37 samples) ensures:\n",
        "   - Statistically significant evaluation\n",
        "   - Coverage of different market regimes (includes 2020 crash)\n",
        "   - Reliable performance metrics\n",
        "\n",
        "3. **Time Series Considerations**:\n",
        "   - Preserves temporal order (no look-ahead bias)\n",
        "   - Tests model on unseen future data\n",
        "   - Evaluates performance during market turbulence\n",
        "\n",
        "4. **Industry Standard**: 80-20 is widely accepted in ML practice\n",
        "\n",
        "**Business Impact:**\n",
        "- Model validated on recent, challenging period (2018-2020)\n",
        "- Performance metrics reflect real-world deployment conditions\n",
        "- Sufficient training data for capturing long-term patterns\n",
        "- Test set includes both bull and bear markets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1XJ9OREExlT"
      },
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFOzZv6IFROw"
      },
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeKDIv7pFgcC"
      },
      "source": [
        "**For Regression Context:**\n",
        "\n",
        "**Assessment:** The target variable (Close price) is **MODERATELY IMBALANCED**\n",
        "\n",
        "**Why \"Imbalanced\" in Regression:**\n",
        "- Not class imbalance (like classification), but distribution skew\n",
        "- More samples at lower price ranges (< â‚¹100)\n",
        "- Fewer samples at extreme high prices (> â‚¹300)\n",
        "- This is expected in financial data (more time at lower prices)\n",
        "\n",
        "**Evidence of Imbalance:**\n",
        "1. **Positive skewness**: {target_skew:.3f} - tail on the right (high prices)\n",
        "2. **High kurtosis**: {target_kurtosis:.3f} - fat tails (extreme events)\n",
        "3. **Mean > Median**: Mean ({df_encoded['Close'].mean():.1f}) > Median ({df_encoded['Close'].median():.1f})\n",
        "4. **Density plot**: Shows concentration in lower price ranges\n",
        "\n",
        "**Why This Matters:**\n",
        "- Models may become biased toward predicting lower prices (more training examples)\n",
        "- Extreme price movements (crashes/rallies) may be under-predicted\n",
        "- Risk assessment could be skewed\n",
        "\n",
        "**How We Addressed It:**\n",
        "1. **Box-Cox transformation** - reduces skewness significantly\n",
        "2. **Sample weights available** - can give more weight to underrepresented price ranges\n",
        "3. **Tree-based models** - handle skewed distributions better than linear models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "outputs": [],
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "# For regression, \"imbalance\" refers to target distribution\n",
        "# Check if target is imbalanced (skewed)\n",
        "target_skew = df_encoded['Close'].skew()\n",
        "target_kurtosis = df_encoded['Close'].kurtosis()\n",
        "\n",
        "print(\"ðŸ“Š Target Variable Distribution Analysis:\")\n",
        "print(f\"   â€¢ Skewness: {target_skew:.3f}\")\n",
        "print(f\"   â€¢ Kurtosis: {target_kurtosis:.3f}\")\n",
        "print(f\"   â€¢ Interpretation: \", end=\"\")\n",
        "\n",
        "if abs(target_skew) < 0.5:\n",
        "    print(\"Approximately symmetric\")\n",
        "elif abs(target_skew) < 1:\n",
        "    print(\"Moderately skewed\")\n",
        "else:\n",
        "    print(\"Highly skewed\")\n",
        "\n",
        "# Visualize target distribution\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Histogram\n",
        "axes[0].hist(df_encoded['Close'], bins=40, color='skyblue', edgecolor='black', alpha=0.7)\n",
        "axes[0].axvline(df_encoded['Close'].mean(), color='red', linestyle='--', linewidth=2, label=f\"Mean: {df_encoded['Close'].mean():.1f}\")\n",
        "axes[0].axvline(df_encoded['Close'].median(), color='green', linestyle='--', linewidth=2, label=f\"Median: {df_encoded['Close'].median():.1f}\")\n",
        "axes[0].set_xlabel('Close Price (INR)')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "axes[0].set_title('Target Distribution - Original', fontsize=12, fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Box plot\n",
        "axes[1].boxplot(df_encoded['Close'])\n",
        "axes[1].set_ylabel('Close Price (INR)')\n",
        "axes[1].set_title('Box Plot - Target Variable', fontsize=12, fontweight='bold')\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Density plot by regime (if we had classes, but for regression we check)\n",
        "from scipy import stats\n",
        "density = stats.gaussian_kde(df_encoded['Close'])\n",
        "x_vals = np.linspace(df_encoded['Close'].min(), df_encoded['Close'].max(), 200)\n",
        "axes[2].plot(x_vals, density(x_vals), linewidth=2)\n",
        "axes[2].fill_between(x_vals, density(x_vals), alpha=0.3)\n",
        "axes[2].set_xlabel('Close Price (INR)')\n",
        "axes[2].set_ylabel('Density')\n",
        "axes[2].set_title('Target Density Plot', fontsize=12, fontweight='bold')\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# For regression, we address imbalance through:\n",
        "# 1. Transformation (already done with Box-Cox)\n",
        "# 2. Weighted loss functions (if needed)\n",
        "# 3. Stratified sampling (not applicable for regression)\n",
        "\n",
        "print(\"\\nðŸ”„ Handling Regression 'Imbalance':\")\n",
        "\n",
        "# Check if transformation helped\n",
        "transformed_skew = df_encoded['Close_Transformed'].skew()\n",
        "print(f\"   â€¢ Original skewness: {target_skew:.3f}\")\n",
        "print(f\"   â€¢ Transformed skewness: {transformed_skew:.3f}\")\n",
        "print(f\"   â€¢ Improvement: {abs(target_skew) - abs(transformed_skew):.3f}\")\n",
        "\n",
        "if abs(transformed_skew) < 0.5:\n",
        "    print(\"âœ… Transformation successfully addressed skewness\")\n",
        "    print(\"   No additional imbalance handling needed\")\n",
        "else:\n",
        "    print(\"âš ï¸ Some skewness remains - consider sample weights in modeling\")\n",
        "\n",
        "# Check if we need sample weights\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "\n",
        "# For regression, we can use inverse of density as sample weights\n",
        "# This gives more weight to underrepresented price ranges\n",
        "density_values = density(df_encoded['Close'])\n",
        "sample_weights = 1.0 / (density_values + 0.01)  # Add small constant to avoid division by zero\n",
        "sample_weights = sample_weights / sample_weights.mean()  # Normalize\n",
        "\n",
        "# Visualize sample weights\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Sample weights by price\n",
        "axes[0].scatter(df_encoded['Close'], sample_weights, alpha=0.6, c='purple', edgecolor='black')\n",
        "axes[0].set_xlabel('Close Price (INR)')\n",
        "axes[0].set_ylabel('Sample Weight')\n",
        "axes[0].set_title('Sample Weights by Price Level', fontsize=12, fontweight='bold')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Distribution with weights\n",
        "axes[1].hist(df_encoded['Close'], bins=40, weights=sample_weights, alpha=0.7,\n",
        "            color='coral', edgecolor='black', label='Weighted')\n",
        "axes[1].hist(df_encoded['Close'], bins=40, alpha=0.4, color='blue',\n",
        "            edgecolor='black', label='Original')\n",
        "axes[1].set_xlabel('Close Price (INR)')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "axes[1].set_title('Original vs Weighted Distribution', fontsize=12, fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nðŸ“Š Sample Weight Statistics:\")\n",
        "print(f\"   â€¢ Mean weight: {sample_weights.mean():.3f}\")\n",
        "print(f\"   â€¢ Std weight: {sample_weights.std():.3f}\")\n",
        "print(f\"   â€¢ Min weight: {sample_weights.min():.3f}\")\n",
        "print(f\"   â€¢ Max weight: {sample_weights.max():.3f}\")\n",
        "print(f\"   â€¢ Weight range: {sample_weights.max() - sample_weights.min():.3f}\")\n",
        "\n",
        "# Decision on imbalance handling\n",
        "print(\"\\nâœ… Final Decision:\")\n",
        "print(\"   â€¢ Transformation successfully reduced skewness\")\n",
        "print(\"   â€¢ Sample weights available if needed\")\n",
        "print(\"   â€¢ Tree-based models (XGBoost) handle skewed targets well\")\n",
        "print(\"   â€¢ No additional imbalance handling required for this dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIqpNgepFxVj"
      },
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbet1HwdGDTz"
      },
      "source": [
        "**Techniques Used:**\n",
        "\n",
        "1. **Box-Cox Transformation (Primary)**\n",
        "   - *Why*: Stabilizes variance and reduces skewness\n",
        "   - *Effect*: Reduced skewness from {target_skew:.3f} to {transformed_skew:.3f}\n",
        "   - *Advantage*: Non-destructive, preserves all data points\n",
        "\n",
        "2. **Sample Weights (Available if needed)**\n",
        "   - *Why*: Can assign higher weights to underrepresented price ranges\n",
        "   - *Mechanism*: Inverse of density estimation\n",
        "   - *When to use*: If model shows bias toward majority price ranges\n",
        "\n",
        "3. **Tree-Based Algorithms (Model Selection)**\n",
        "   - *Why*: XGBoost/Random Forest handle skewed targets naturally\n",
        "   - *Advantage*: No additional preprocessing needed\n",
        "   - *Effect*: Better at capturing extreme values\n",
        "\n",
        "**Why Not Other Techniques:**\n",
        "- **Oversampling**: Not appropriate for time series (would break temporal order)\n",
        "- **Undersampling**: Would lose valuable information\n",
        "- **SMOGN (SMOTE for regression)**: Could create synthetic prices that don't reflect market dynamics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfCC591jGiD4"
      },
      "source": [
        "## ***7. ML Model Implementation***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      },
      "source": [
        "### ML Model - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "outputs": [],
      "source": [
        "# ML Model - 1 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArJBuiUVfxKd"
      },
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "outputs": [],
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qY1EAkEfxKe"
      },
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "outputs": [],
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      },
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "negyGRa7fxKf"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfvqoZmBfxKf"
      },
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaLui8CcfxKf"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      },
      "source": [
        "### ML Model - 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWYfwnehpsJ1"
      },
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "outputs": [],
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      },
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "outputs": [],
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAih1iBOpsJ2"
      },
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      },
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74yRdG6UpsJ3"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      },
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fze-IPXLpx6K"
      },
      "source": [
        "### ML Model - 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "outputs": [],
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AN1z2sKpx6M"
      },
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "outputs": [],
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PIHJqyupx6M"
      },
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "outputs": [],
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-qAgymDpx6N"
      },
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQMffxkwpx6N"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-hykwinpx6N"
      },
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzVzZC6opx6N"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_CCil-SKHpo"
      },
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHVz9hHDKFms"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBFFvTBNJzUa"
      },
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvGl1hHyA_VK"
      },
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnvVTiIxBL-C"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyNgTHvd2WFk"
      },
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KH5McJBi2d8v"
      },
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "outputs": [],
      "source": [
        "# Save the File"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      },
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "outputs": [],
      "source": [
        "# Load the File and predict unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Kee-DAl2viO"
      },
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCX9965dhzqZ"
      },
      "source": [
        "# **Conclusion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      },
      "source": [
        "Write the conclusion here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIfDvo9L0UH2"
      },
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}