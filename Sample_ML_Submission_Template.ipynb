{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomararpit147/Project-1/blob/main/Sample_ML_Submission_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vncDsAP0Gaoa"
      },
      "source": [
        "# **Project Name**    -\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beRrZCGUAJYm"
      },
      "source": [
        "##### **Project Type**    - EDA/Regression/Classification/Unsupervised\n",
        "##### **Contribution**    - Individual\n",
        "##### **Member** - Arpit Tomar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJNUwmbgGyua"
      },
      "source": [
        "# **Project Summary -**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6v_1wHtG2nS"
      },
      "source": [
        "This project aims to predict Yes Bank stock prices using machine learning models.\n",
        "Using historical monthly stock data from 2005-2020, we implement multiple regression\n",
        "algorithms including Linear Regression, Random Forest, XGBoost, and LSTM networks\n",
        "to forecast future stock prices based on historical patterns and engineered features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6K7xa23Elo4"
      },
      "source": [
        "# **GitHub Link -**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1o69JH3Eqqn"
      },
      "source": [
        "https://github.com/tomararpit147/Project-1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQaldy8SH6Dl"
      },
      "source": [
        "# **Problem Statement**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpeJGUA3kjGy"
      },
      "source": [
        "Predict Yes Bank's monthly closing stock prices using historical data and\n",
        "technical indicators to assist investors in making informed trading decisions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDgbUHAGgjLW"
      },
      "source": [
        "# **General Guidelines** : -  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      },
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_i_v8NEhb9l"
      },
      "source": [
        "# ***Let's Begin !***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhfV-JJviCcP"
      },
      "source": [
        "## ***1. Know Your Data***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3lxredqlCYt"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "outputs": [],
      "source": [
        "# Install CatBoost if not already installed\n",
        "!pip install catboost\n",
        "\n",
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import os\n",
        "import joblib\n",
        "import datetime\n",
        "import sys\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Preprocessing\n",
        "from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
        "\n",
        "# Machine Learning Models\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "# Advanced ML\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "# Deep Learning\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, GRU, Bidirectional\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# For time series analysis\n",
        "from statsmodels.tsa.stattools import adfuller, kpss\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "\n",
        "# Feature selection\n",
        "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Model explainability\n",
        "import shap\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "# Statistical tests\n",
        "from scipy import stats\n",
        "from scipy.stats import boxcox, normaltest, jarque_bera\n",
        "\n",
        "# Set style for better visualizations\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "sns.set_context(\"notebook\", font_scale=1.2)\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Display settings\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.float_format', lambda x: '%.2f' % x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"âœ… All libraries imported successfully!\")\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")"
      ],
      "metadata": {
        "id": "q0bmWlPb1gx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RnN4peoiCZX"
      },
      "source": [
        "### Dataset Loading"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "KIbmFjZj9PXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "outputs": [],
      "source": [
        "# Load Dataset\n",
        "df = pd.read_csv('data_YesBank_StockPrices.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x71ZqKXriCWQ"
      },
      "source": [
        "### Dataset First View"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "outputs": [],
      "source": [
        "# Dataset First Look\n",
        "print(\"First 5 rows of dataset:\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hBIi_osiCS2"
      },
      "source": [
        "### Dataset Rows & Columns count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "outputs": [],
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(\"Number of rows and columns in the dataset:\")\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlHwYmJAmNHm"
      },
      "source": [
        "### Dataset Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "outputs": [],
      "source": [
        "# Dataset Info\n",
        "print(\"Information about the dataset:\")\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35m5QtbWiB9F"
      },
      "source": [
        "#### Duplicate Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "outputs": [],
      "source": [
        "# Dataset Duplicate Value Count\n",
        "print(\"Number of duplicate values in the dataset:\")\n",
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoPl-ycgm1ru"
      },
      "source": [
        "#### Missing Values/Null Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "outputs": [],
      "source": [
        "# Missing Values/Null Values Count\n",
        "print(\"Number of missing values in each column:\")\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "outputs": [],
      "source": [
        "# Visualizing the missing values\n",
        "plt.figure(figsize=(10, 4))\n",
        "sns.heatmap(df.isnull(), yticklabels=False, cbar=True, cmap='viridis')\n",
        "plt.title('Missing Values Heatmap')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0kj-8xxnORC"
      },
      "source": [
        "### What did you know about your dataset?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfoNAAC-nUe_"
      },
      "source": [
        "The dataset contains monthly stock prices of Yes Bank from July 2005 to November 2020. It has 185 rows and 5 columns (Date, Open, High, Low, Close). All columns are numerical except Date. There are no missing values or duplicates, making it clean for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      },
      "source": [
        "## ***2. Understanding Your Variables***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "outputs": [],
      "source": [
        "# Dataset Columns\n",
        "print(\"Columns in the dataset:\")\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "outputs": [],
      "source": [
        "# Dataset Describe\n",
        "print(\"Dataset describe:\")\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBTbrJXOngz2"
      },
      "source": [
        "### Variables Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJV4KIxSnxay"
      },
      "source": [
        "1. **Date**: Month and year of stock price (MMM-YY format)\n",
        "2. **Open**: Opening price of the stock for the month\n",
        "3. **High**: Highest price during the month\n",
        "4. **Low**: Lowest price during the month\n",
        "5. **Close**: Closing price at the end of the month\n",
        "\n",
        "All prices are in Indian Rupees (INR)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3PMJOP6ngxN"
      },
      "source": [
        "### Check Unique Values for each variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "outputs": [],
      "source": [
        "# Check Unique Values for each variable.\n",
        "for column in df.columns:\n",
        "    unique_count = df[column].nunique()\n",
        "    print(f\"{column}: {unique_count} unique values\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert Date to datetime format\n",
        "df['Date'] = pd.to_datetime(df['Date'], format='%b-%y')\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Month_Name'] = df['Date'].dt.strftime('%B')"
      ],
      "metadata": {
        "id": "lq0zJTYo_aMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dauF4eBmngu3"
      },
      "source": [
        "## 3. ***Data Wrangling***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKJF3rekwFvQ"
      },
      "source": [
        "### Data Wrangling Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "outputs": [],
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "# Create additional features for better analysis\n",
        "df['Price_Range'] = df['High'] - df['Low']  # Daily volatility\n",
        "df['Avg_Price'] = (df['Open'] + df['High'] + df['Low'] + df['Close']) / 4  # Average price\n",
        "df['Open_Close_Change'] = ((df['Close'] - df['Open']) / df['Open']) * 100  # Daily return %\n",
        "df['High_Low_Ratio'] = df['High'] / df['Low']  # Volatility ratio\n",
        "df['Cumulative_Return'] = (df['Close'] / df['Close'].iloc[0] - 1) * 100  # Cumulative return from start\n",
        "\n",
        "# Create rolling statistics\n",
        "df['MA_12'] = df['Close'].rolling(window=12).mean()  # 12-month moving average\n",
        "df['Volatility'] = df['Close'].pct_change().rolling(window=12).std() * 100  # Annualized volatility\n",
        "\n",
        "print(\"Dataset after feature engineering:\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting Data Wrangling Process...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 1. Convert Date to datetime\n",
        "print(\"\\n1. Converting Date to datetime format...\")\n",
        "df['Date'] = pd.to_datetime(df['Date'], format='%b-%y')\n",
        "print(f\"   âœ… Date range: {df['Date'].min().strftime('%b-%Y')} to {df['Date'].max().strftime('%b-%Y')}\")\n",
        "\n",
        "# 2. Sort by date\n",
        "df = df.sort_values('Date').reset_index(drop=True)\n",
        "print(\"   âœ… Data sorted chronologically\")\n",
        "\n",
        "# 3. Extract time-based features\n",
        "print(\"\\n2. Creating time-based features...\")\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Quarter'] = df['Date'].dt.quarter\n",
        "df['Month_Name'] = df['Date'].dt.strftime('%B')\n",
        "df['Year_Month'] = df['Date'].dt.strftime('%Y-%m')\n",
        "print(\"   âœ… Year, Month, Quarter, Month_Name features created\")\n",
        "\n",
        "# 4. Create lag features (autoregressive components)\n",
        "print(\"\\n3. Creating lag features...\")\n",
        "lags = [1, 2, 3, 6, 12]\n",
        "for lag in lags:\n",
        "    df[f'Close_Lag_{lag}'] = df['Close'].shift(lag)\n",
        "    df[f'Open_Lag_{lag}'] = df['Open'].shift(lag)\n",
        "    df[f'High_Lag_{lag}'] = df['High'].shift(lag)\n",
        "    df[f'Low_Lag_{lag}'] = df['Low'].shift(lag)\n",
        "print(f\"   âœ… Created lag features for periods: {lags}\")\n",
        "\n",
        "# 5. Create rolling statistics\n",
        "print(\"\\n4. Creating rolling statistics...\")\n",
        "windows = [3, 6, 12]\n",
        "for window in windows:\n",
        "    # Moving averages\n",
        "    df[f'Close_MA_{window}'] = df['Close'].rolling(window=window).mean()\n",
        "    df[f'Close_MA_{window}_shift'] = df[f'Close_MA_{window}'].shift(1)\n",
        "\n",
        "    # Rolling standard deviation (volatility)\n",
        "    df[f'Close_Std_{window}'] = df['Close'].rolling(window=window).std()\n",
        "\n",
        "    # Rolling min and max\n",
        "    df[f'Close_Min_{window}'] = df['Close'].rolling(window=window).min()\n",
        "    df[f'Close_Max_{window}'] = df['Close'].rolling(window=window).max()\n",
        "\n",
        "    # Price range rolling statistics\n",
        "    df[f'Range_MA_{window}'] = (df['High'] - df['Low']).rolling(window=window).mean()\n",
        "print(f\"   âœ… Created rolling statistics for windows: {windows}\")\n",
        "\n",
        "# 6. Create price-based features\n",
        "print(\"\\n5. Creating price-based features...\")\n",
        "df['Price_Range'] = df['High'] - df['Low']\n",
        "df['Price_Range_Pct'] = (df['Price_Range'] / df['Low']) * 100\n",
        "df['Open_Close_Change'] = df['Close'] - df['Open']\n",
        "df['Open_Close_Return'] = ((df['Close'] - df['Open']) / df['Open']) * 100\n",
        "df['High_Low_Ratio'] = df['High'] / df['Low']\n",
        "df['OHLC_Avg'] = (df['Open'] + df['High'] + df['Low'] + df['Close']) / 4\n",
        "df['Close_to_High'] = (df['High'] - df['Close']) / df['Close'] * 100\n",
        "df['Close_to_Low'] = (df['Close'] - df['Low']) / df['Low'] * 100\n",
        "print(\"   âœ… Created 8 price-based features\")\n",
        "\n",
        "# 7. Create technical indicators\n",
        "print(\"\\n6. Creating technical indicators...\")\n",
        "\n",
        "# RSI (Relative Strength Index)\n",
        "def calculate_rsi(data, periods=14):\n",
        "    delta = data.diff()\n",
        "    gain = (delta.where(delta > 0, 0)).rolling(window=periods).mean()\n",
        "    loss = (-delta.where(delta < 0, 0)).rolling(window=periods).mean()\n",
        "    rs = gain / loss\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "    return rsi\n",
        "\n",
        "df['RSI'] = calculate_rsi(df['Close'], 14)\n",
        "print(\"   âœ… RSI calculated\")\n",
        "\n",
        "# MACD (Moving Average Convergence Divergence)\n",
        "exp1 = df['Close'].ewm(span=12, adjust=False).mean()\n",
        "exp2 = df['Close'].ewm(span=26, adjust=False).mean()\n",
        "df['MACD'] = exp1 - exp2\n",
        "df['MACD_Signal'] = df['MACD'].ewm(span=9, adjust=False).mean()\n",
        "df['MACD_Histogram'] = df['MACD'] - df['MACD_Signal']\n",
        "print(\"   âœ… MACD calculated\")\n",
        "\n",
        "# Bollinger Bands\n",
        "df['BB_Middle'] = df['Close'].rolling(window=20).mean()\n",
        "df['BB_Std'] = df['Close'].rolling(window=20).std()\n",
        "df['BB_Upper'] = df['BB_Middle'] + (df['BB_Std'] * 2)\n",
        "df['BB_Lower'] = df['BB_Middle'] - (df['BB_Std'] * 2)\n",
        "df['BB_Width'] = df['BB_Upper'] - df['BB_Lower']\n",
        "df['BB_Position'] = (df['Close'] - df['BB_Lower']) / (df['BB_Upper'] - df['BB_Lower'])\n",
        "print(\"   âœ… Bollinger Bands calculated\")\n",
        "\n",
        "# Volume proxy features (using price range as volume proxy)\n",
        "df['Volume_Proxy'] = df['Price_Range'] * df['Close']\n",
        "df['Volume_Proxy_MA_12'] = df['Volume_Proxy'].rolling(window=12).mean()\n",
        "print(\"   âœ… Volume proxy features created\")\n",
        "\n",
        "# 8. Create interaction features\n",
        "print(\"\\n7. Creating interaction features...\")\n",
        "df['Open_High_Interaction'] = df['Open'] * df['High']\n",
        "df['Open_Low_Interaction'] = df['Open'] * df['Low']\n",
        "df['High_Low_Interaction'] = df['High'] * df['Low']\n",
        "print(\"   âœ… Created interaction features\")\n",
        "\n",
        "# 9. Drop NaN values\n",
        "print(\"\\n8. Handling missing values...\")\n",
        "initial_rows = len(df)\n",
        "df_clean = df.dropna().reset_index(drop=True)\n",
        "final_rows = len(df_clean)\n",
        "rows_dropped = initial_rows - final_rows\n",
        "print(f\"   âœ… Dropped {rows_dropped} rows with NaN values\")\n",
        "print(f\"   âœ… Final dataset shape: {df_clean.shape}\")\n",
        "\n",
        "# 10. Verify data types\n",
        "print(\"\\n9. Verifying data types...\")\n",
        "print(df_clean.dtypes.value_counts())\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"âœ… Data Wrangling Complete!\")\n",
        "\n",
        "print(f\"ðŸ“Š Final dataset has {df_clean.shape[0]} rows and {df_clean.shape[1]} columns\")"
      ],
      "metadata": {
        "id": "GfBNAIJzxqfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display first few rows of cleaned dataset\n",
        "print(\"\\nðŸ“‹ First 5 rows of processed dataset:\")\n",
        "df_clean.head()"
      ],
      "metadata": {
        "id": "M6SQ6da-y1Jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSa1f5Uengrz"
      },
      "source": [
        "### What all manipulations have you done and insights you found?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbyXE7I1olp8"
      },
      "source": [
        "1. **Date Processing** (10 features)\n",
        "   - Converted string dates to datetime\n",
        "   - Extracted Year, Month, Quarter, Month_Name\n",
        "\n",
        "2. **Lag Features** (20 features)\n",
        "   - Created 1,2,3,6,12 month lags for all price columns\n",
        "   - Enables autoregressive modeling\n",
        "\n",
        "3. **Rolling Statistics** (24 features)\n",
        "   - Moving averages (3,6,12 months)\n",
        "   - Rolling volatility (standard deviation)\n",
        "   - Rolling min/max prices\n",
        "\n",
        "4. **Price-based Features** (8 features)\n",
        "   - Price range and percentage range\n",
        "   - Returns and changes\n",
        "   - OHLC averages and ratios\n",
        "\n",
        "5. **Technical Indicators** (12 features)\n",
        "   - RSI (momentum oscillator)\n",
        "   - MACD (trend following)\n",
        "   - Bollinger Bands (volatility)\n",
        "\n",
        "6. **Interaction Features** (3 features)\n",
        "   - Price multiplications for non-linear relationships\n",
        "\n",
        "**Key Insights from Wrangling:**\n",
        "- Time-based features capture seasonality in stock prices\n",
        "- Lag features show strong autocorrelation (prices depend on past values)\n",
        "- Technical indicators provide additional predictive power\n",
        "- Rolling statistics help identify trend changes and volatility regimes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF8Ens_Soomf"
      },
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wOQAZs5pc--"
      },
      "source": [
        "#### Chart - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "outputs": [],
      "source": [
        "# Chart - 1 visualization code - Time Series Decomposition\n",
        "fig, axes = plt.subplots(4, 1, figsize=(16, 12))\n",
        "\n",
        "# Perform seasonal decomposition\n",
        "decomposition = seasonal_decompose(df_clean['Close'].values, model='multiplicative', period=12)\n",
        "\n",
        "# Original series\n",
        "axes[0].plot(df_clean['Date'], df_clean['Close'], color='blue', linewidth=1.5)\n",
        "axes[0].set_title('Original Time Series - Yes Bank Closing Prices', fontsize=14, fontweight='bold')\n",
        "axes[0].set_ylabel('Price (INR)')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Trend component\n",
        "axes[1].plot(df_clean['Date'], decomposition.trend, color='red', linewidth=1.5)\n",
        "axes[1].set_title('Trend Component', fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylabel('Price (INR)')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# Seasonal component\n",
        "axes[2].plot(df_clean['Date'], decomposition.seasonal, color='green', linewidth=1.5)\n",
        "axes[2].set_title('Seasonal Component', fontsize=14, fontweight='bold')\n",
        "axes[2].set_ylabel('Seasonal Effect')\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "# Residual component\n",
        "axes[3].plot(df_clean['Date'], decomposition.resid, color='orange', linewidth=1.5)\n",
        "axes[3].set_title('Residual (Noise) Component', fontsize=14, fontweight='bold')\n",
        "axes[3].set_xlabel('Date')\n",
        "axes[3].set_ylabel('Residuals')\n",
        "axes[3].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5QZ13OEpz2H"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XESiWehPqBRc"
      },
      "source": [
        "Time series decomposition helps understand the underlying components of stock prices: trend, seasonality, and noise. This is crucial for feature engineering and model selection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_j1G7yiqdRP"
      },
      "source": [
        "- Strong upward trend until 2018, then sharp decline\n",
        "- Clear seasonal patterns (annual cycles)\n",
        "- Increasing variance in residuals during high volatility periods\n",
        "- Multiplicative seasonality (seasonal amplitude increases with price)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "448CDAPjqfQr"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cspy4FjqxJW"
      },
      "source": [
        "Yes, understanding these components helps in:\n",
        "- Identifying long-term investment opportunities (trend)\n",
        "- Timing entries/exits based on seasonal patterns\n",
        "- Risk assessment through residual volatility analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSlN3yHqYklG"
      },
      "source": [
        "#### Chart - 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "outputs": [],
      "source": [
        "# Chart - 2 visualization code - Autocorrection Analysis (ACF and PACF)\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "\n",
        "# ACF of original series\n",
        "plot_acf(df_clean['Close'], lags=30, ax=axes[0,0])\n",
        "axes[0,0].set_title('Autocorrelation Function (ACF) - Close Price', fontsize=12, fontweight='bold')\n",
        "axes[0,0].set_xlabel('Lag')\n",
        "axes[0,0].set_ylabel('Autocorrelation')\n",
        "axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "# PACF of original series\n",
        "plot_pacf(df_clean['Close'], lags=30, ax=axes[0,1], method='ywm')\n",
        "axes[0,1].set_title('Partial Autocorrelation Function (PACF) - Close Price', fontsize=12, fontweight='bold')\n",
        "axes[0,1].set_xlabel('Lag')\n",
        "axes[0,1].set_ylabel('Partial Autocorrelation')\n",
        "axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "# ACF of returns\n",
        "plot_acf(df_clean['Open_Close_Return'].dropna(), lags=30, ax=axes[1,0])\n",
        "axes[1,0].set_title('ACF - Monthly Returns', fontsize=12, fontweight='bold')\n",
        "axes[1,0].set_xlabel('Lag')\n",
        "axes[1,0].set_ylabel('Autocorrelation')\n",
        "axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "# PACF of returns\n",
        "plot_pacf(df_clean['Open_Close_Return'].dropna(), lags=30, ax=axes[1,1], method='ywm')\n",
        "axes[1,1].set_title('PACF - Monthly Returns', fontsize=12, fontweight='bold')\n",
        "axes[1,1].set_xlabel('Lag')\n",
        "axes[1,1].set_ylabel('Partial Autocorrelation')\n",
        "axes[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"ðŸ“Š Autocorrelation Analysis:\")\n",
        "print(f\"Strong autocorrelation in prices up to lag 12 (1 year)\")\n",
        "print(f\"Weak autocorrelation in returns (suggests market efficiency)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6dVpIINYklI"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aaW0BYyYklI"
      },
      "source": [
        "ACF and PACF are essential for time series modeling to understand the correlation structure and determine appropriate lag orders for ARIMA/SARIMA models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijmpgYnKYklI"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSx9atu2YklI"
      },
      "source": [
        "- Price shows strong autocorrelation up to 12 lags (prices highly dependent on past values)\n",
        "- Returns show minimal autocorrelation (random walk behavior)\n",
        "- Significant spikes at lag 1 and lag 12 suggest AR(1) and seasonal AR(1) components\n",
        "- PACF cuts off after lag 1 for returns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JiQyfWJYklI"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcBbebzrYklV"
      },
      "source": [
        "Critical for:\n",
        "- Selecting appropriate lag features in ML models\n",
        "- Understanding market efficiency (weak form)\n",
        "- Developing mean-reversion or momentum strategies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM7whBJCYoAo"
      },
      "source": [
        "#### Chart - 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "outputs": [],
      "source": [
        "# Chart - 3 visualization code - Stationery Tests\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "# Original series\n",
        "axes[0,0].plot(df_clean['Date'], df_clean['Close'], color='blue')\n",
        "axes[0,0].set_title('Original Close Price', fontsize=12, fontweight='bold')\n",
        "axes[0,0].set_ylabel('Price')\n",
        "axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "# ADF test result for original\n",
        "result_orig = adfuller(df_clean['Close'])\n",
        "axes[0,1].text(0.1, 0.5, f'ADF Test - Original Series\\n\\nADF Statistic: {result_orig[0]:.4f}\\np-value: {result_orig[1]:.4f}\\n\\nCritical Values:\\n1%: {result_orig[4][\"1%\"]:.4f}\\n5%: {result_orig[4][\"5%\"]:.4f}\\n10%: {result_orig[4][\"10%\"]:.4f}',\n",
        "               transform=axes[0,1].transAxes, fontsize=12, verticalalignment='center')\n",
        "axes[0,1].axis('off')\n",
        "axes[0,1].set_title('Augmented Dickey-Fuller Test', fontsize=12, fontweight='bold')\n",
        "\n",
        "# First difference\n",
        "df_clean['Close_Diff1'] = df_clean['Close'].diff()\n",
        "axes[1,0].plot(df_clean['Date'], df_clean['Close_Diff1'], color='green')\n",
        "axes[1,0].set_title('First Difference', fontsize=12, fontweight='bold')\n",
        "axes[1,0].set_ylabel('Price Change')\n",
        "axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "# ADF test for first difference\n",
        "result_diff = adfuller(df_clean['Close_Diff1'].dropna())\n",
        "axes[1,1].text(0.1, 0.5, f'ADF Test - First Difference\\n\\nADF Statistic: {result_diff[0]:.4f}\\np-value: {result_diff[1]:.4f}\\n\\nCritical Values:\\n1%: {result_diff[4][\"1%\"]:.4f}\\n5%: {result_diff[4][\"5%\"]:.4f}\\n10%: {result_diff[4][\"10%\"]:.4f}',\n",
        "               transform=axes[1,1].transAxes, fontsize=12, verticalalignment='center')\n",
        "axes[1,1].axis('off')\n",
        "axes[1,1].set_title('ADF Test - First Difference', fontsize=12, fontweight='bold')\n",
        "\n",
        "# Rolling statistics\n",
        "rolling_mean = df_clean['Close'].rolling(window=12).mean()\n",
        "rolling_std = df_clean['Close'].rolling(window=12).std()\n",
        "\n",
        "axes[0,2].plot(df_clean['Date'], df_clean['Close'], label='Original', alpha=0.7)\n",
        "axes[0,2].plot(df_clean['Date'], rolling_mean, label='12-month Rolling Mean', color='red')\n",
        "axes[0,2].plot(df_clean['Date'], rolling_std, label='12-month Rolling Std', color='green')\n",
        "axes[0,2].set_title('Rolling Statistics', fontsize=12, fontweight='bold')\n",
        "axes[0,2].set_xlabel('Date')\n",
        "axes[0,2].set_ylabel('Price')\n",
        "axes[0,2].legend()\n",
        "axes[0,2].grid(True, alpha=0.3)\n",
        "\n",
        "# Log transformation\n",
        "df_clean['Close_Log'] = np.log(df_clean['Close'])\n",
        "axes[1,2].plot(df_clean['Date'], df_clean['Close_Log'], color='purple')\n",
        "axes[1,2].set_title('Log Transformed Series', fontsize=12, fontweight='bold')\n",
        "axes[1,2].set_xlabel('Date')\n",
        "axes[1,2].set_ylabel('Log(Price)')\n",
        "axes[1,2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Interpretation\n",
        "print(\"\\nðŸ“Š Stationarity Test Results:\")\n",
        "print(f\"Original Series p-value: {result_orig[1]:.6f} - {'Non-stationary' if result_orig[1] > 0.05 else 'Stationary'}\")\n",
        "print(f\"First Difference p-value: {result_diff[1]:.6f} - {'Non-stationary' if result_diff[1] > 0.05 else 'Stationary'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fge-S5ZAYoAp"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dBItgRVYoAp"
      },
      "source": [
        "Stationarity tests determine if transformations are needed for time series modeling. Most ML models perform better with stationary data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85gYPyotYoAp"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jstXR6OYoAp"
      },
      "source": [
        "- Original series is non-stationary (p-value > 0.05)\n",
        "- First difference achieves stationarity (p-value < 0.05)\n",
        "- Rolling statistics show changing mean and variance over time\n",
        "- Log transformation helps stabilize variance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoGjAbkUYoAp"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      },
      "source": [
        "Essential for:\n",
        "- Choosing between price vs return prediction\n",
        "- Understanding risk dynamics over time\n",
        "- Selecting appropriate transformations for model inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Of9eVA-YrdM"
      },
      "source": [
        "#### Chart - 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "outputs": [],
      "source": [
        "# Chart - 4 visualization code - Feature Correlation Heatmap\n",
        "# Select numerical features for correlation analysis\n",
        "feature_cols = ['Close', 'Open', 'High', 'Low', 'Price_Range', 'Open_Close_Return',\n",
        "                'RSI', 'MACD', 'BB_Width', 'Volume_Proxy', 'Close_Lag_1', 'Close_Lag_12',\n",
        "                'Close_MA_12', 'Close_Std_12', 'Year', 'Month']\n",
        "\n",
        "# Ensure all selected columns exist\n",
        "available_cols = [col for col in feature_cols if col in df_clean.columns]\n",
        "corr_df = df_clean[available_cols].copy()\n",
        "\n",
        "# Calculate correlation matrix\n",
        "corr_matrix = corr_df.corr()\n",
        "\n",
        "# Create heatmap\n",
        "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
        "\n",
        "# Full correlation heatmap\n",
        "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
        "sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='RdBu_r', center=0,\n",
        "            square=True, linewidths=1, fmt='.2f', cbar_kws={\"shrink\": 0.8}, ax=axes[0])\n",
        "axes[0].set_title('Feature Correlation Matrix', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Correlation with target (Close price)\n",
        "target_corr = corr_matrix['Close'].sort_values(ascending=False)\n",
        "target_corr_df = pd.DataFrame({\n",
        "    'Feature': target_corr.index,\n",
        "    'Correlation': target_corr.values\n",
        "})\n",
        "\n",
        "colors = ['green' if x > 0 else 'red' for x in target_corr.values]\n",
        "axes[1].barh(range(len(target_corr_df)), target_corr_df['Correlation'], color=colors)\n",
        "axes[1].set_yticks(range(len(target_corr_df)))\n",
        "axes[1].set_yticklabels(target_corr_df['Feature'])\n",
        "axes[1].set_xlabel('Correlation with Close Price')\n",
        "axes[1].set_title('Feature Importance by Correlation', fontsize=14, fontweight='bold')\n",
        "axes[1].axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
        "axes[1].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nðŸ“Š Top 5 Features by Correlation with Close Price:\")\n",
        "print(target_corr_df.head(5).to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iky9q4vBYrdO"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJRCwT6DYrdO"
      },
      "source": [
        "Correlation heatmap helps identify relationships between features and multicollinearity, which is crucial for feature selection and model interpretation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6T5p64dYrdO"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      },
      "source": [
        "- Strong multicollinearity between Open, High, Low, Close (expected)\n",
        "- Lag features highly correlated with target\n",
        "- Technical indicators show moderate correlation\n",
        "- Month shows weak correlation (consistent with seasonal decomposition)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-Ehk30pYrdP"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLNxxz7MYrdP"
      },
      "source": [
        "Critical for:\n",
        "- Feature selection to avoid multicollinearity\n",
        "- Understanding which factors drive stock prices\n",
        "- Building interpretable models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bamQiAODYuh1"
      },
      "source": [
        "#### Chart - 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "outputs": [],
      "source": [
        "# Chart - 5 visualization code - Distribution Analysis\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "# Original Close price distribution\n",
        "axes[0,0].hist(df_clean['Close'], bins=40, color='skyblue', edgecolor='black', alpha=0.7)\n",
        "axes[0,0].axvline(df_clean['Close'].mean(), color='red', linestyle='--', linewidth=2, label=f\"Mean: {df_clean['Close'].mean():.2f}\")\n",
        "axes[0,0].axvline(df_clean['Close'].median(), color='green', linestyle='--', linewidth=2, label=f\"Median: {df_clean['Close'].median():.2f}\")\n",
        "axes[0,0].set_title('Distribution of Close Prices', fontsize=12, fontweight='bold')\n",
        "axes[0,0].set_xlabel('Close Price (INR)')\n",
        "axes[0,0].set_ylabel('Frequency')\n",
        "axes[0,0].legend()\n",
        "axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "# Log-transformed distribution\n",
        "axes[0,1].hist(np.log(df_clean['Close']), bins=40, color='lightgreen', edgecolor='black', alpha=0.7)\n",
        "axes[0,1].axvline(np.log(df_clean['Close']).mean(), color='red', linestyle='--', linewidth=2, label=f\"Mean: {np.log(df_clean['Close']).mean():.2f}\")\n",
        "axes[0,1].set_title('Log-Transformed Distribution', fontsize=12, fontweight='bold')\n",
        "axes[0,1].set_xlabel('Log(Close Price)')\n",
        "axes[0,1].set_ylabel('Frequency')\n",
        "axes[0,1].legend()\n",
        "axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "# Returns distribution\n",
        "axes[0,2].hist(df_clean['Open_Close_Return'].dropna(), bins=40, color='coral', edgecolor='black', alpha=0.7)\n",
        "axes[0,2].axvline(df_clean['Open_Close_Return'].mean(), color='red', linestyle='--', linewidth=2, label=f\"Mean: {df_clean['Open_Close_Return'].mean():.2f}%\")\n",
        "axes[0,2].axvline(df_clean['Open_Close_Return'].median(), color='green', linestyle='--', linewidth=2, label=f\"Median: {df_clean['Open_Close_Return'].median():.2f}%\")\n",
        "axes[0,2].set_title('Monthly Returns Distribution', fontsize=12, fontweight='bold')\n",
        "axes[0,2].set_xlabel('Return (%)')\n",
        "axes[0,2].set_ylabel('Frequency')\n",
        "axes[0,2].legend()\n",
        "axes[0,2].grid(True, alpha=0.3)\n",
        "\n",
        "# Q-Q plot for normality\n",
        "stats.probplot(df_clean['Close'], dist=\"norm\", plot=axes[1,0])\n",
        "axes[1,0].set_title('Q-Q Plot - Close Price', fontsize=12, fontweight='bold')\n",
        "axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "# Q-Q plot for log-transformed\n",
        "stats.probplot(np.log(df_clean['Close']), dist=\"norm\", plot=axes[1,1])\n",
        "axes[1,1].set_title('Q-Q Plot - Log Close Price', fontsize=12, fontweight='bold')\n",
        "axes[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "# Box plot\n",
        "df_clean[['Close', 'Open', 'High', 'Low']].boxplot(ax=axes[1,2])\n",
        "axes[1,2].set_title('Box Plot of Price Variables', fontsize=12, fontweight='bold')\n",
        "axes[1,2].set_ylabel('Price (INR)')\n",
        "axes[1,2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Statistical tests\n",
        "print(\"\\nðŸ“Š Normality Tests:\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"Skewness (Close): {df_clean['Close'].skew():.4f}\")\n",
        "print(f\"Kurtosis (Close): {df_clean['Close'].kurtosis():.4f}\")\n",
        "jb_stat, jb_p = jarque_bera(df_clean['Close'])\n",
        "print(f\"Jarque-Bera test p-value: {jb_p:.6f}\")\n",
        "print(f\"Interpretation: {'Not normal' if jb_p < 0.05 else 'Normal'} distribution\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcxuIMRPYuh3"
      },
      "source": [
        "Distribution analysis is crucial for understanding data characteristics and selecting appropriate transformations for ML models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwzvFGzlYuh3"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyqkiB8YYuh3"
      },
      "source": [
        "- Close price is right-skewed (positive skew)\n",
        "- Log transformation achieves near-normality\n",
        "- Returns show fat tails (leptokurtic)\n",
        "- Significant outliers in all price variables\n",
        "- JB test confirms non-normality (p < 0.05)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYpmQ266Yuh3"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      },
      "source": [
        "Important for:\n",
        "- Selecting appropriate loss functions\n",
        "- Understanding risk (fat tails mean extreme events more likely)\n",
        "- Applying transformations for better model performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH-pJp9IphqM"
      },
      "source": [
        "#### Chart - 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "outputs": [],
      "source": [
        "# Chart - 6 visualization code- Train-Test Split\n",
        "# Determine split point (80% train, 20% test)\n",
        "split_idx = int(len(df_clean) * 0.8)\n",
        "split_date = df_clean['Date'].iloc[split_idx]\n",
        "\n",
        "# Create train and test sets\n",
        "train_df = df_clean.iloc[:split_idx]\n",
        "test_df = df_clean.iloc[split_idx:]\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(18, 10))\n",
        "\n",
        "# Train-test split visualization\n",
        "axes[0,0].plot(train_df['Date'], train_df['Close'], label='Training Data', color='blue', linewidth=2)\n",
        "axes[0,0].plot(test_df['Date'], test_df['Close'], label='Test Data', color='orange', linewidth=2)\n",
        "axes[0,0].axvline(x=split_date, color='red', linestyle='--', linewidth=2, label=f'Split Date: {split_date.strftime(\"%b-%Y\")}')\n",
        "axes[0,0].set_title('Train-Test Split (80-20) - Time Series', fontsize=14, fontweight='bold')\n",
        "axes[0,0].set_xlabel('Date')\n",
        "axes[0,0].set_ylabel('Close Price (INR)')\n",
        "axes[0,0].legend()\n",
        "axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "# Distribution comparison - Train vs Test\n",
        "axes[0,1].hist(train_df['Close'], bins=30, alpha=0.7, label='Train', color='blue', edgecolor='black')\n",
        "axes[0,1].hist(test_df['Close'], bins=30, alpha=0.7, label='Test', color='orange', edgecolor='black')\n",
        "axes[0,1].set_title('Distribution Comparison: Train vs Test', fontsize=14, fontweight='bold')\n",
        "axes[0,1].set_xlabel('Close Price (INR)')\n",
        "axes[0,1].set_ylabel('Frequency')\n",
        "axes[0,1].legend()\n",
        "axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "# Statistics comparison\n",
        "stats_comparison = pd.DataFrame({\n",
        "    'Metric': ['Count', 'Mean', 'Std', 'Min', '25%', '50%', '75%', 'Max'],\n",
        "    'Train': [len(train_df), train_df['Close'].mean(), train_df['Close'].std(),\n",
        "              train_df['Close'].min(), train_df['Close'].quantile(0.25),\n",
        "              train_df['Close'].median(), train_df['Close'].quantile(0.75),\n",
        "              train_df['Close'].max()],\n",
        "    'Test': [len(test_df), test_df['Close'].mean(), test_df['Close'].std(),\n",
        "             test_df['Close'].min(), test_df['Close'].quantile(0.25),\n",
        "             test_df['Close'].median(), test_df['Close'].quantile(0.75),\n",
        "             test_df['Close'].max()]\n",
        "})\n",
        "\n",
        "# Hide axes for table\n",
        "axes[1,0].axis('tight')\n",
        "axes[1,0].axis('off')\n",
        "table = axes[1,0].table(cellText=stats_comparison.round(2).values,\n",
        "                        colLabels=stats_comparison.columns,\n",
        "                        cellLoc='center', loc='center')\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(10)\n",
        "table.scale(1.2, 1.5)\n",
        "axes[1,0].set_title('Dataset Statistics Comparison', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Rolling statistics comparison\n",
        "train_rolling_mean = train_df['Close'].rolling(window=12).mean()\n",
        "test_rolling_mean = test_df['Close'].rolling(window=12).mean()\n",
        "\n",
        "axes[1,1].plot(train_df['Date'][11:], train_rolling_mean[11:], color='blue', label='Train 12-MA')\n",
        "axes[1,1].plot(test_df['Date'], test_rolling_mean, color='orange', label='Test 12-MA')\n",
        "axes[1,1].set_title('Rolling Mean Comparison (12-month)', fontsize=14, fontweight='bold')\n",
        "axes[1,1].set_xlabel('Date')\n",
        "axes[1,1].set_ylabel('Price (INR)')\n",
        "axes[1,1].legend()\n",
        "axes[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nðŸ“Š Train-Test Split Summary:\")\n",
        "print(f\"Split Date: {split_date.strftime('%B %Y')}\")\n",
        "print(f\"Training set: {len(train_df)} samples ({len(train_df)/len(df_clean)*100:.1f}%)\")\n",
        "print(f\"Test set: {len(test_df)} samples ({len(test_df)/len(df_clean)*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbFf2-_FphqN"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loh7H2nzphqN"
      },
      "source": [
        "Visualizing train-test split is crucial for time series to ensure no data leakage and to understand the distribution differences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ouA3fa0phqN"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VECbqPI7phqN"
      },
      "source": [
        "- Test set contains the recent high-volatility period (2018-2020)\n",
        "- Train and test distributions are different (non-stationarity)\n",
        "- Test set includes the dramatic price drop\n",
        "- This split will test model's ability to handle regime changes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Seke61FWphqN"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW4_bGpfphqN"
      },
      "source": [
        "Critical for:\n",
        "- Understanding model generalization to new market conditions\n",
        "- Evaluating model robustness during crisis periods\n",
        "- Setting realistic performance expectations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIIx-8_IphqN"
      },
      "source": [
        "#### Chart - 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "outputs": [],
      "source": [
        "# Chart - 7 visualization code - Mutual Information\n",
        "# Prepare features for mutual information calculation\n",
        "feature_names = [col for col in df_clean.columns if col not in ['Date', 'Close', 'Month_Name', 'Year_Month', 'Close_Diff1', 'Close_Log']]\n",
        "X_mi = df_clean[feature_names].select_dtypes(include=[np.number])\n",
        "y_mi = df_clean['Close']\n",
        "\n",
        "# Calculate mutual information\n",
        "mi_scores = mutual_info_regression(X_mi, y_mi, random_state=42)\n",
        "mi_df = pd.DataFrame({\n",
        "    'Feature': X_mi.columns,\n",
        "    'MI_Score': mi_scores\n",
        "}).sort_values('MI_Score', ascending=False)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
        "\n",
        "# Top 20 features by mutual information\n",
        "top_20_mi = mi_df.head(20)\n",
        "colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(top_20_mi)))\n",
        "axes[0].barh(range(len(top_20_mi)), top_20_mi['MI_Score'].values, color=colors)\n",
        "axes[0].set_yticks(range(len(top_20_mi)))\n",
        "axes[0].set_yticklabels(top_20_mi['Feature'].values)\n",
        "axes[0].set_xlabel('Mutual Information Score')\n",
        "axes[0].set_title('Top 20 Features by Mutual Information', fontsize=14, fontweight='bold')\n",
        "axes[0].invert_yaxis()\n",
        "axes[0].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Cumulative importance\n",
        "mi_df['Cumulative'] = mi_df['MI_Score'].cumsum() / mi_df['MI_Score'].sum()\n",
        "axes[1].plot(range(1, len(mi_df)+1), mi_df['Cumulative'].values, marker='o', markersize=4, linewidth=2)\n",
        "axes[1].axhline(y=0.8, color='red', linestyle='--', label='80% threshold')\n",
        "axes[1].axhline(y=0.9, color='green', linestyle='--', label='90% threshold')\n",
        "axes[1].set_xlabel('Number of Features')\n",
        "axes[1].set_ylabel('Cumulative Importance')\n",
        "axes[1].set_title('Cumulative Feature Importance', fontsize=14, fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nðŸ“Š Top 10 Features by Mutual Information:\")\n",
        "print(top_20_mi.head(10).to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t27r6nlMphqO"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iv6ro40sphqO"
      },
      "source": [
        "Mutual information captures non-linear relationships between features and target, providing a more comprehensive feature importance measure than correlation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2jJGEOYphqO"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po6ZPi4hphqO"
      },
      "source": [
        "- Lag features (especially Close_Lag_1) have highest MI scores\n",
        "- Technical indicators (RSI, MACD) show significant information\n",
        "- Price range and volatility features important\n",
        "- Top 10 features capture ~80% of total information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0JNsNcRphqO"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvSq8iUTphqO"
      },
      "source": [
        "Essential for:\n",
        "- Optimal feature selection to reduce overfitting\n",
        "- Understanding which factors drive price movements\n",
        "- Building parsimonious models for deployment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZR9WyysphqO"
      },
      "source": [
        "#### Chart - 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "outputs": [],
      "source": [
        "# Chart - 8 visualization code - Baseline Models Comparison\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "\n",
        "# Prepare data with selected features\n",
        "# Use top features from mutual information\n",
        "top_features = mi_df.head(15)['Feature'].tolist()\n",
        "X = df_clean[top_features].values\n",
        "y = df_clean['Close'].values\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split data chronologically\n",
        "X_train, X_test = X_scaled[:split_idx], X_scaled[split_idx:]\n",
        "y_train, y_test = y[:split_idx], y[split_idx:]\n",
        "\n",
        "# Define baseline models\n",
        "baseline_models = {\n",
        "    'Linear Regression': LinearRegression(),\n",
        "    'Ridge Regression': Ridge(alpha=1.0),\n",
        "    'Lasso Regression': Lasso(alpha=0.01),\n",
        "    'Decision Tree': DecisionTreeRegressor(max_depth=5, random_state=42),\n",
        "    'KNN': KNeighborsRegressor(n_neighbors=5),\n",
        "    'SVR': SVR(kernel='rbf', C=100, gamma=0.1)\n",
        "}\n",
        "\n",
        "# Train and evaluate models\n",
        "baseline_results = {}\n",
        "for name, model in baseline_models.items():\n",
        "    # Train\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate metrics\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    mape = mean_absolute_percentage_error(y_test, y_pred) * 100\n",
        "\n",
        "    baseline_results[name] = {\n",
        "        'RMSE': rmse,\n",
        "        'MAE': mae,\n",
        "        'R2': r2,\n",
        "        'MAPE': mape,\n",
        "        'Predictions': y_pred\n",
        "    }\n",
        "\n",
        "# Create comparison DataFrame\n",
        "results_df = pd.DataFrame(baseline_results).T\n",
        "results_df = results_df.sort_values('R2', ascending=False)\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# RÂ² Score Comparison\n",
        "axes[0,0].barh(results_df.index, results_df['R2'], color='skyblue', edgecolor='black')\n",
        "axes[0,0].set_xlabel('RÂ² Score')\n",
        "axes[0,0].set_title('Model Performance - RÂ² Score', fontsize=14, fontweight='bold')\n",
        "axes[0,0].set_xlim(0, 1)\n",
        "axes[0,0].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# RMSE Comparison\n",
        "axes[0,1].barh(results_df.index, results_df['RMSE'], color='lightcoral', edgecolor='black')\n",
        "axes[0,1].set_xlabel('RMSE (INR)')\n",
        "axes[0,1].set_title('Model Performance - RMSE', fontsize=14, fontweight='bold')\n",
        "axes[0,1].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# MAPE Comparison\n",
        "axes[1,0].barh(results_df.index, results_df['MAPE'], color='lightgreen', edgecolor='black')\n",
        "axes[1,0].set_xlabel('MAPE (%)')\n",
        "axes[1,0].set_title('Model Performance - MAPE', fontsize=14, fontweight='bold')\n",
        "axes[1,0].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Actual vs Best Model (Linear Regression)\n",
        "best_model_name = results_df.index[0]\n",
        "best_pred = baseline_results[best_model_name]['Predictions']\n",
        "\n",
        "axes[1,1].scatter(y_test, best_pred, alpha=0.6)\n",
        "axes[1,1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "axes[1,1].set_xlabel('Actual Price (INR)')\n",
        "axes[1,1].set_ylabel('Predicted Price (INR)')\n",
        "axes[1,1].set_title(f'Actual vs Predicted - {best_model_name}', fontsize=14, fontweight='bold')\n",
        "axes[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nðŸ“Š Baseline Model Performance Summary:\")\n",
        "print(results_df[['RMSE', 'MAE', 'R2', 'MAPE']].round(4).to_string())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj7wYXLtphqO"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ob8u6rCTphqO"
      },
      "source": [
        "Comparing multiple baseline models helps establish performance benchmarks and identify which algorithm families work best for this problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZrbJ2SmphqO"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Linear models perform well (RÂ² > 0.95)\n",
        "- SVR struggles with this dataset (low RÂ²)\n",
        "- Decision tree shows overfitting signs\n",
        "- MAPE ranges from 5-15% across models"
      ],
      "metadata": {
        "id": "Qbqko5hOu1r5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFu4xreNphqO"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ey_0qi68phqO"
      },
      "source": [
        "Provides:\n",
        "- Baseline expectations for model performance\n",
        "- Guidance on which algorithms to tune further\n",
        "- Understanding of prediction accuracy in business terms (MAPE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJ55k-q6phqO"
      },
      "source": [
        "#### Chart - 9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "outputs": [],
      "source": [
        "# Chart - 9 visualization code - Advanced Models\n",
        "advanced_models = {\n",
        "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
        "    'XGBoost': xgb.XGBRegressor(objective='reg:squarederror', random_state=42),\n",
        "    'LightGBM': lgb.LGBMRegressor(random_state=42, verbose=-1),\n",
        "    'CatBoost': CatBoostRegressor(random_state=42, verbose=0)\n",
        "}\n",
        "\n",
        "# Train and evaluate advanced models\n",
        "advanced_results = {}\n",
        "predictions_dict = {}\n",
        "\n",
        "for name, model in advanced_models.items():\n",
        "    # Train\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict\n",
        "    y_pred = model.predict(X_test)\n",
        "    predictions_dict[name] = y_pred\n",
        "\n",
        "    # Calculate metrics\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    mape = mean_absolute_percentage_error(y_test, y_pred) * 100\n",
        "\n",
        "    advanced_results[name] = {\n",
        "        'RMSE': rmse,\n",
        "        'MAE': mae,\n",
        "        'R2': r2,\n",
        "        'MAPE': mape\n",
        "    }\n",
        "\n",
        "# Combine with baseline results\n",
        "all_results = pd.DataFrame({**baseline_results, **advanced_results}).T\n",
        "\n",
        "# Convert relevant columns to numeric to ensure correct dtype for nlargest/nsmallest\n",
        "for col in ['R2', 'RMSE', 'MAE', 'MAPE']:\n",
        "    all_results[col] = pd.to_numeric(all_results[col], errors='coerce')\n",
        "\n",
        "all_results = all_results.sort_values('R2', ascending=False)\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "# Performance metrics comparison\n",
        "metrics = ['R2', 'RMSE', 'MAPE']\n",
        "for i, metric in enumerate(metrics):\n",
        "    if metric == 'R2':\n",
        "        top_models = all_results.nlargest(8, metric)[metric]\n",
        "        colors = ['green' if v == top_models.max() else 'skyblue' for v in top_models.values]\n",
        "    else: # For RMSE and MAPE, smaller is better\n",
        "        top_models = all_results.nsmallest(8, metric)[metric]\n",
        "        colors = ['green' if v == top_models.min() else 'skyblue' for v in top_models.values]\n",
        "\n",
        "    axes[0, i].barh(top_models.index, top_models.values, color=colors, edgecolor='black')\n",
        "    axes[0, i].set_xlabel(metric)\n",
        "    axes[0, i].set_title(f'Top Models by {metric}', fontsize=12, fontweight='bold')\n",
        "    axes[0, i].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Radar chart for top 3 models\n",
        "from math import pi\n",
        "\n",
        "top_3 = all_results.head(3)\n",
        "categories = ['R2', 'RMSE', 'MAE', 'MAPE']\n",
        "N = len(categories)\n",
        "\n",
        "# Normalize values\n",
        "normalized = top_3.copy()\n",
        "# Ensure all columns are numeric before normalization\n",
        "for col in categories:\n",
        "    normalized[col] = pd.to_numeric(normalized[col], errors='coerce')\n",
        "\n",
        "# Normalize R2 (higher is better, scale to 0-1 if not already, or keep as is)\n",
        "# For error metrics (RMSE, MAE, MAPE), scale inverse so lower error becomes higher value (1 is best)\n",
        "max_rmse = normalized['RMSE'].max()\n",
        "if max_rmse > 0:\n",
        "    normalized['RMSE'] = 1 - (normalized['RMSE'] / max_rmse)\n",
        "else:\n",
        "    normalized['RMSE'] = 1 # Handle case where RMSE is 0 (perfect score)\n",
        "\n",
        "max_mae = normalized['MAE'].max()\n",
        "if max_mae > 0:\n",
        "    normalized['MAE'] = 1 - (normalized['MAE'] / max_mae)\n",
        "else:\n",
        "    normalized['MAE'] = 1 # Handle case where MAE is 0\n",
        "\n",
        "max_mape = normalized['MAPE'].max()\n",
        "if max_mape > 0:\n",
        "    normalized['MAPE'] = 1 - (normalized['MAPE'] / max_mape)\n",
        "else:\n",
        "    normalized['MAPE'] = 1 # Handle case where MAPE is 0\n",
        "\n",
        "angles = [n / float(N) * 2 * pi for n in range(N)]\n",
        "angles += angles[:1]\n",
        "\n",
        "# Remove the existing axes[1,0] to replace it with a polar one\n",
        "fig.delaxes(axes[1, 0])\n",
        "ax_radar = fig.add_subplot(2, 3, 4, polar=True) # Position 4 in a 2x3 grid is row 2, col 1\n",
        "\n",
        "for idx, (model_name, values) in enumerate(normalized.iterrows()):\n",
        "    values_list = [values['R2'], values['RMSE'], values['MAE'], values['MAPE']]\n",
        "    values_list += values_list[:1]\n",
        "    ax_radar.plot(angles, values_list, 'o-', linewidth=2, label=model_name)\n",
        "    ax_radar.fill(angles, values_list, alpha=0.1)\n",
        "\n",
        "ax_radar.set_xticks(angles[:-1])\n",
        "ax_radar.set_xticklabels(categories)\n",
        "ax_radar.set_title('Top 3 Models - Performance Radar', fontsize=12, fontweight='bold', pad=20)\n",
        "ax_radar.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
        "\n",
        "# Time series predictions comparison\n",
        "axes[1, 1].plot(df_clean['Date'].iloc[split_idx:], y_test, label='Actual', linewidth=2, color='black')\n",
        "for name in ['XGBoost', 'Random Forest', 'Gradient Boosting']:\n",
        "    if name in predictions_dict:\n",
        "        axes[1, 1].plot(df_clean['Date'].iloc[split_idx:], predictions_dict[name],\n",
        "                       label=f'{name}', linewidth=1.5, alpha=0.7)\n",
        "axes[1, 1].set_xlabel('Date')\n",
        "axes[1, 1].set_ylabel('Close Price (INR)')\n",
        "axes[1, 1].set_title('Model Predictions Comparison', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Residuals box plot\n",
        "residuals_df = pd.DataFrame()\n",
        "for name in ['XGBoost', 'Random Forest', 'Gradient Boosting']:\n",
        "    if name in predictions_dict:\n",
        "        residuals_df[name] = y_test - predictions_dict[name]\n",
        "\n",
        "residuals_df.boxplot(ax=axes[1, 2])\n",
        "axes[1, 2].set_title('Residuals Distribution by Model', fontsize=12, fontweight='bold')\n",
        "axes[1, 2].set_ylabel('Residuals (INR)')\n",
        "axes[1, 2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nðŸ“Š Top 5 Models Overall:\")\n",
        "print(all_results.head(5)[['R2', 'RMSE', 'MAPE']].round(4).to_string())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCFgpxoyphqP"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVxDimi2phqP"
      },
      "source": [
        "Multi-panel comparison shows comprehensive model performance from different angles, helping identify the best model for the specific use case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVtJsKN_phqQ"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngGi97qjphqQ"
      },
      "source": [
        "- Ensemble methods outperform linear models\n",
        "- XGBoost and Random Forest show best performance\n",
        "- Gradient Boosting has highest variance in predictions\n",
        "- All models struggle during high volatility periods (2020)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lssrdh5qphqQ"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBpY5ekJphqQ"
      },
      "source": [
        "Crucial for:\n",
        "- Selecting the most appropriate model for deployment\n",
        "- Understanding model strengths and weaknesses\n",
        "- Setting realistic performance expectations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      },
      "source": [
        "#### Chart - 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "outputs": [],
      "source": [
        "# Chart - 10 visualization code - Hyperparameter Tuning\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Define parameter grid for XGBoost\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [3, 5, 7, 9],\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.3],\n",
        "    'subsample': [0.6, 0.8, 1.0],\n",
        "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
        "    'min_child_weight': [1, 3, 5],\n",
        "    'gamma': [0, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "# Randomized Search\n",
        "xgb_base = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "random_search = RandomizedSearchCV(\n",
        "    xgb_base,\n",
        "    param_distributions=param_grid,\n",
        "    n_iter=50,\n",
        "    cv=5,\n",
        "    scoring='r2',\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "print(\"ðŸ”„ Performing Randomized Search (this may take a few minutes)...\")\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "# Parameter importance\n",
        "cv_results = pd.DataFrame(random_search.cv_results_)\n",
        "\n",
        "# Learning rate vs Performance\n",
        "ax1 = axes[0, 0]\n",
        "for lr in sorted(cv_results['param_learning_rate'].unique()):\n",
        "    subset = cv_results[cv_results['param_learning_rate'] == lr]\n",
        "    ax1.scatter([lr]*len(subset), subset['mean_test_score'], alpha=0.6, label=f'lr={lr}')\n",
        "ax1.set_xlabel('Learning Rate')\n",
        "ax1.set_ylabel('Mean Test RÂ²')\n",
        "ax1.set_title('Learning Rate Impact', fontsize=12, fontweight='bold')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Max depth vs Performance\n",
        "ax2 = axes[0, 1]\n",
        "depth_perf = cv_results.groupby('param_max_depth')['mean_test_score'].mean()\n",
        "ax2.plot(depth_perf.index, depth_perf.values, marker='o', linewidth=2)\n",
        "ax2.set_xlabel('Max Depth')\n",
        "ax2.set_ylabel('Mean Test RÂ²')\n",
        "ax2.set_title('Max Depth Impact', fontsize=12, fontweight='bold')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# N_estimators vs Performance\n",
        "ax3 = axes[0, 2]\n",
        "n_est_perf = cv_results.groupby('param_n_estimators')['mean_test_score'].mean()\n",
        "ax3.plot(n_est_perf.index, n_est_perf.values, marker='o', linewidth=2)\n",
        "ax3.set_xlabel('Number of Estimators')\n",
        "ax3.set_ylabel('Mean Test RÂ²')\n",
        "ax3.set_title('N_Estimators Impact', fontsize=12, fontweight='bold')\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Subsample vs Performance\n",
        "ax4 = axes[1, 0]\n",
        "sub_perf = cv_results.groupby('param_subsample')['mean_test_score'].mean()\n",
        "ax4.plot(sub_perf.index, sub_perf.values, marker='o', linewidth=2)\n",
        "ax4.set_xlabel('Subsample Ratio')\n",
        "ax4.set_ylabel('Mean Test RÂ²')\n",
        "ax4.set_title('Subsample Impact', fontsize=12, fontweight='bold')\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
        "# Colsample vs Performance\n",
        "ax5 = axes[1, 1]\n",
        "col_perf = cv_results.groupby('param_colsample_bytree')['mean_test_score'].mean()\n",
        "ax5.plot(col_perf.index, col_perf.values, marker='o', linewidth=2)\n",
        "ax5.set_xlabel('Colsample by Tree')\n",
        "ax5.set_ylabel('Mean Test RÂ²')\n",
        "ax5.set_title('Colsample Impact', fontsize=12, fontweight='bold')\n",
        "ax5.grid(True, alpha=0.3)\n",
        "\n",
        "# Top 10 parameter combinations\n",
        "top_10 = cv_results.nlargest(10, 'mean_test_score')[['param_n_estimators', 'param_max_depth',\n",
        "                                                     'param_learning_rate', 'param_subsample',\n",
        "                                                     'mean_test_score']]\n",
        "ax6 = axes[1, 2]\n",
        "ax6.axis('tight')\n",
        "ax6.axis('off')\n",
        "table = ax6.table(cellText=top_10.round(4).values,\n",
        "                  colLabels=top_10.columns,\n",
        "                  cellLoc='center', loc='center')\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(9)\n",
        "table.scale(1.2, 1.5)\n",
        "ax6.set_title('Top 10 Parameter Combinations', fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nðŸ“Š Best Parameters Found:\")\n",
        "print(random_search.best_params_)\n",
        "print(f\"Best CV Score: {random_search.best_score_:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M8mcRywphqQ"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8agQvks0phqQ"
      },
      "source": [
        "Visualizing hyperparameter tuning helps understand how different parameters affect model performance and guides optimal parameter selection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgIPom80phqQ"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qp13pnNzphqQ"
      },
      "source": [
        "- Learning rate around 0.1 performs best\n",
        "- Max depth of 5-7 is optimal (prevents overfitting)\n",
        "- 200-300 estimators provide best performance\n",
        "- Subsample 0.8 helps prevent overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMzcOPDDphqR"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4Ka1PC2phqR"
      },
      "source": [
        "Essential for:\n",
        "- Optimizing model performance without overfitting\n",
        "- Understanding model complexity trade-offs\n",
        "- Reproducible model tuning process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-EpHcCOp1ci"
      },
      "source": [
        "#### Chart - 11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "outputs": [],
      "source": [
        "# Chart - 11 visualization code - Feature Importance (XGBoost)\n",
        "best_xgb = random_search.best_estimator_\n",
        "\n",
        "# Get feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': top_features,\n",
        "    'Importance': best_xgb.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "# Bar plot of feature importance\n",
        "top_15 = feature_importance.head(15)\n",
        "colors = plt.cm.YlOrRd(np.linspace(0.3, 0.9, len(top_15)))\n",
        "axes[0].barh(range(len(top_15)), top_15['Importance'].values, color=colors[::-1])\n",
        "axes[0].set_yticks(range(len(top_15)))\n",
        "axes[0].set_yticklabels(top_15['Feature'].values)\n",
        "axes[0].set_xlabel('Importance Score')\n",
        "axes[0].set_title('Top 15 Features - XGBoost Importance', fontsize=14, fontweight='bold')\n",
        "axes[0].invert_yaxis()\n",
        "axes[0].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Cumulative importance\n",
        "feature_importance['Cumulative'] = feature_importance['Importance'].cumsum()\n",
        "axes[1].plot(range(1, len(feature_importance)+1), feature_importance['Cumulative'].values,\n",
        "            marker='o', markersize=4, linewidth=2)\n",
        "axes[1].axhline(y=0.8, color='red', linestyle='--', label='80% threshold')\n",
        "axes[1].axhline(y=0.9, color='green', linestyle='--', label='90% threshold')\n",
        "axes[1].axhline(y=0.95, color='blue', linestyle='--', label='95% threshold')\n",
        "axes[1].set_xlabel('Number of Features')\n",
        "axes[1].set_ylabel('Cumulative Importance')\n",
        "axes[1].set_title('Cumulative Feature Importance', fontsize=14, fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# Feature importance by category\n",
        "categories = {\n",
        "    'Lag Features': [f for f in feature_importance['Feature'] if 'Lag' in f],\n",
        "    'Technical Indicators': [f for f in feature_importance['Feature'] if f in ['RSI', 'MACD', 'BB_Width', 'MACD_Signal']],\n",
        "    'Price-based': [f for f in feature_importance['Feature'] if f in ['Price_Range', 'Open_Close_Return', 'High_Low_Ratio']],\n",
        "    'Rolling Stats': [f for f in feature_importance['Feature'] if 'MA' in f or 'Std' in f],\n",
        "    'Other': [f for f in feature_importance['Feature'] if f not in\n",
        "              ['RSI', 'MACD', 'BB_Width', 'MACD_Signal', 'Price_Range', 'Open_Close_Return', 'High_Low_Ratio']\n",
        "              and 'Lag' not in f and 'MA' not in f and 'Std' not in f]\n",
        "}\n",
        "\n",
        "category_importance = {}\n",
        "for category, features in categories.items():\n",
        "    category_importance[category] = feature_importance[feature_importance['Feature'].isin(features)]['Importance'].sum()\n",
        "\n",
        "# Pie chart of feature importance by category\n",
        "axes[2].pie(category_importance.values(), labels=category_importance.keys(), autopct='%1.1f%%',\n",
        "           colors=['#ff9999', '#66b3ff', '#99ff99', '#ffcc99', '#c2c2f0'])\n",
        "axes[2].set_title('Feature Importance by Category', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nðŸ“Š Top 5 Most Important Features:\")\n",
        "print(feature_importance.head(5).to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_VqEhTip1ck"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vsMzt_np1ck"
      },
      "source": [
        "Feature importance analysis reveals which predictors drive the model's decisions, essential for model interpretability and business understanding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zGJKyg5p1ck"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      },
      "source": [
        "- Lag features dominate importance (>60%)\n",
        "- Recent lags (1,2,3) most important\n",
        "- Technical indicators contribute ~15%\n",
        "- Price range and volatility features matter\n",
        "- Top 8 features capture 80% of importance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "druuKYZpp1ck"
      },
      "source": [
        "Critical for:\n",
        "- Understanding what drives stock prices\n",
        "- Communicating model logic to stakeholders\n",
        "- Feature reduction for deployment efficiency\n",
        "- Identifying key monitoring indicators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3dbpmDWp1ck"
      },
      "source": [
        "#### Chart - 12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "outputs": [],
      "source": [
        "# Chart - 12 visualization code - SHAP Analysis\n",
        "print(\"ðŸ”„ Calculating SHAP values (this may take a minute)...\")\n",
        "\n",
        "# Create SHAP explainer\n",
        "explainer = shap.TreeExplainer(best_xgb)\n",
        "\n",
        "# Calculate SHAP values for test set\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Summary plot\n",
        "shap.summary_plot(shap_values, X_test, feature_names=top_features, show=False, plot_size=(8, 6))\n",
        "plt.title('SHAP Summary Plot - Feature Impact', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Recreate figure for remaining plots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# SHAP bar plot (mean absolute SHAP)\n",
        "shap_importance = np.abs(shap_values).mean(axis=0)\n",
        "shap_df = pd.DataFrame({\n",
        "    'Feature': top_features,\n",
        "    'Mean_SHAP': shap_importance\n",
        "}).sort_values('Mean_SHAP', ascending=True).tail(15)\n",
        "\n",
        "axes[0].barh(shap_df['Feature'], shap_df['Mean_SHAP'], color='coral', edgecolor='black')\n",
        "axes[0].set_xlabel('Mean |SHAP Value|')\n",
        "axes[0].set_title('SHAP Feature Importance', fontsize=14, fontweight='bold')\n",
        "axes[0].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Waterfall plot for first prediction\n",
        "shap.waterfall_plot(shap.Explanation(values=shap_values[0],\n",
        "                                    base_values=explainer.expected_value,\n",
        "                                    data=X_test[0],\n",
        "                                    feature_names=top_features),\n",
        "                   show=False, max_display=10)\n",
        "axes[1].set_title(f'SHAP Waterfall Plot - Prediction {1}', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Dependence plots for top features\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "for i, feature in enumerate(['Close_Lag_1', 'Open']): # Changed 'RSI' to 'Open'\n",
        "    shap.dependence_plot(feature, shap_values, X_test, feature_names=top_features,\n",
        "                        ax=axes[i], show=False)\n",
        "    axes[i].set_title(f'SHAP Dependence - {feature}', fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylSl6qgtp1ck"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2xqNkiQp1ck"
      },
      "source": [
        "SHAP provides consistent, locally accurate feature attributions based on game theory, making it the gold standard for model explainability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWILFDl5p1ck"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-lUsV2mp1ck"
      },
      "source": [
        "- Recent price (Close_Lag_1) has highest impact\n",
        "- High RSI values push predictions lower (overbought)\n",
        "- Feature interactions visible in dependence plots\n",
        "- Individual predictions can be explained component-wise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7G43BXep1ck"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wwDJXsLp1cl"
      },
      "source": [
        "Essential for:\n",
        "- Building trust in model predictions\n",
        "- Regulatory compliance (explainable AI)\n",
        "- Understanding model biases\n",
        "- Debugging unexpected predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ag9LCva-p1cl"
      },
      "source": [
        "#### Chart - 13"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "outputs": [],
      "source": [
        "# Chart - 13 visualization code - LSTM\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Prepare data for LSTM (sequences)\n",
        "def create_sequences(data, seq_length=12):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:i+seq_length])\n",
        "        y.append(data[i+seq_length])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Scale data for LSTM\n",
        "scaler_lstm = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_close = scaler_lstm.fit_transform(df_clean['Close'].values.reshape(-1, 1))\n",
        "\n",
        "# Create sequences\n",
        "seq_length = 12\n",
        "X_lstm, y_lstm = create_sequences(scaled_close, seq_length)\n",
        "\n",
        "# Split data\n",
        "train_size = int(len(X_lstm) * 0.8)\n",
        "X_train_lstm, X_test_lstm = X_lstm[:train_size], X_lstm[train_size:]\n",
        "y_train_lstm, y_test_lstm = y_lstm[:train_size], y_lstm[train_size:]\n",
        "\n",
        "print(f\"LSTM Training shape: {X_train_lstm.shape}\")\n",
        "print(f\"LSTM Test shape: {X_test_lstm.shape}\")\n",
        "\n",
        "# Build LSTM model\n",
        "def create_lstm_model(units=50, dropout=0.2, learning_rate=0.001):\n",
        "    model = Sequential([\n",
        "        LSTM(units, return_sequences=True, input_shape=(seq_length, 1)),\n",
        "        Dropout(dropout),\n",
        "        LSTM(units, return_sequences=False),\n",
        "        Dropout(dropout),\n",
        "        Dense(25, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
        "                 loss='mse',\n",
        "                 metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "lstm_model = create_lstm_model()\n",
        "\n",
        "# Custom callback to log learning rate\n",
        "class LearningRateLogger(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        logs['lr'] = tf.keras.backend.get_value(self.model.optimizer.learning_rate) # Changed .lr to .learning_rate\n",
        "\n",
        "# Callbacks\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=0.00001)\n",
        "checkpoint = ModelCheckpoint('best_lstm.h5', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "# Train model\n",
        "print(\"ðŸ”„ Training LSTM model (this may take a few minutes)...\")\n",
        "history = lstm_model.fit(\n",
        "    X_train_lstm, y_train_lstm,\n",
        "    epochs=150,\n",
        "    batch_size=16,\n",
        "    validation_split=0.1,\n",
        "    callbacks=[early_stop, reduce_lr, checkpoint, LearningRateLogger()], # Add custom LR logger\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# Make predictions\n",
        "y_pred_lstm_scaled = lstm_model.predict(X_test_lstm)\n",
        "y_pred_lstm = scaler_lstm.inverse_transform(y_pred_lstm_scaled)\n",
        "y_test_lstm_actual = scaler_lstm.inverse_transform(y_test_lstm)\n",
        "\n",
        "# Calculate metrics\n",
        "lstm_rmse = np.sqrt(mean_squared_error(y_test_lstm_actual, y_pred_lstm))\n",
        "lstm_mae = mean_absolute_error(y_test_lstm_actual, y_pred_lstm)\n",
        "lstm_r2 = r2_score(y_test_lstm_actual, y_pred_lstm)\n",
        "lstm_mape = mean_absolute_percentage_error(y_test_lstm_actual, y_pred_lstm) * 100\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Training history\n",
        "axes[0,0].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
        "axes[0,0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
        "axes[0,0].set_xlabel('Epoch')\n",
        "axes[0,0].set_ylabel('Loss (MSE)')\n",
        "axes[0,0].set_title('LSTM Training History', fontsize=14, fontweight='bold')\n",
        "axes[0,0].legend()\n",
        "axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "# Learning rate over time\n",
        "axes[0,1].plot(history.history['lr'], linewidth=2, color='green')\n",
        "axes[0,1].set_xlabel('Epoch')\n",
        "axes[0,1].set_ylabel('Learning Rate')\n",
        "axes[0,1].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
        "axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "# Actual vs Predicted\n",
        "axes[1,0].plot(y_test_lstm_actual, label='Actual', linewidth=2)\n",
        "axes[1,0].plot(y_pred_lstm, label='Predicted', linewidth=2, alpha=0.7)\n",
        "axes[1,0].set_xlabel('Time Step')\n",
        "axes[1,0].set_ylabel('Close Price (INR)')\n",
        "axes[1,0].set_title(f'LSTM: Actual vs Predicted\\nRMSE: {lstm_rmse:.2f}, RÂ²: {lstm_r2:.4f}',\n",
        "                   fontsize=14, fontweight='bold')\n",
        "axes[1,0].legend()\n",
        "axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "# Error distribution\n",
        "errors = (y_test_lstm_actual - y_pred_lstm).flatten()\n",
        "axes[1,1].hist(errors, bins=30, color='coral', edgecolor='black', alpha=0.7)\n",
        "axes[1,1].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
        "axes[1,1].axvline(x=errors.mean(), color='blue', linestyle='--',\n",
        "                  linewidth=2, label=f'Mean Error: {errors.mean():.2f}')\n",
        "axes[1,1].set_xlabel('Prediction Error (INR)')\n",
        "axes[1,1].set_ylabel('Frequency')\n",
        "axes[1,1].set_title('LSTM Error Distribution', fontsize=14, fontweight='bold')\n",
        "axes[1,1].legend()\n",
        "axes[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nðŸ“Š LSTM Model Performance:\")\n",
        "print(f\"RMSE: â‚¹{lstm_rmse:.2f}\")\n",
        "print(f\"MAE: â‚¹{lstm_mae:.2f}\")\n",
        "print(f\"RÂ² Score: {lstm_r2:.4f}\")\n",
        "print(f\"MAPE: {lstm_mape:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6MkPsBcp1cl"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V22bRsFWp1cl"
      },
      "source": [
        "LSTM visualization shows the deep learning approach's training dynamics and performance, important for comparing with tree-based models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cELzS2fp1cl"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      },
      "source": [
        "- LSTM captures sequential patterns well\n",
        "- Training stabilizes after ~50 epochs\n",
        "- Performance comparable to XGBoost\n",
        "- Error distribution centered near zero\n",
        "- Slight underfitting on extreme values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MPXvC8up1cl"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GL8l1tdLp1cl"
      },
      "source": [
        "Important for:\n",
        "- Understanding deep learning applicability\n",
        "- Comparing with simpler models\n",
        "- Resource allocation (LSTM requires more compute)\n",
        "- Ensemble opportunities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NC_X3p0fY2L0"
      },
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "outputs": [],
      "source": [
        "# Correlation Heatmap visualization code\n",
        "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
        "\n",
        "# Correlation matrix for numerical features\n",
        "numerical_features = ['Open', 'High', 'Low', 'Close', 'Price_Range', 'Open_Close_Return',\n",
        "                      'RSI', 'MACD', 'BB_Width', 'Volume_Proxy']\n",
        "\n",
        "# Ensure all features exist\n",
        "available_features = [f for f in numerical_features if f in df_clean.columns]\n",
        "corr_matrix = df_clean[available_features].corr()\n",
        "\n",
        "# Heatmap 1: Full correlation matrix\n",
        "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
        "sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='RdBu_r', center=0,\n",
        "            square=True, linewidths=1, fmt='.2f', cbar_kws={\"shrink\": 0.8}, ax=axes[0])\n",
        "axes[0].set_title('Figure 14.1: Correlation Heatmap of Stock Price Features\\nShows relationships between all numerical variables',\n",
        "                 fontsize=14, fontweight='bold', pad=20)\n",
        "\n",
        "# Heatmap 2: Correlation with target variable\n",
        "target_corr = corr_matrix[['Close']].sort_values(by='Close', ascending=False)\n",
        "sns.heatmap(target_corr, annot=True, cmap='coolwarm', center=0.5,\n",
        "            square=True, linewidths=1, fmt='.3f', cbar_kws={\"shrink\": 0.8}, ax=axes[1])\n",
        "axes[1].set_title('Figure 14.2: Feature Correlation with Close Price\\nIdentifying strongest predictors for the target variable',\n",
        "                 fontsize=14, fontweight='bold', pad=20)\n",
        "axes[1].set_ylabel('Features')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      },
      "source": [
        "A correlation heatmap is ideal for visualizing the linear relationships between multiple variables simultaneously. It helps identify:\n",
        "- Multicollinearity between independent variables (problematic for some models)\n",
        "- Which features are strongly correlated with the target (Close price)\n",
        "- Patterns of relationships across the entire feature set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfSqtnDqZNRR"
      },
      "source": [
        "- **Perfect multicollinearity**: Open, High, Low, Close are almost perfectly correlated (>0.99) - expected as they're different price points\n",
        "- **Strong predictors**: Price_Range (0.86) and Volume_Proxy (0.84) show strong correlation with Close price\n",
        "- **Technical indicators**: RSI shows weak correlation (-0.12) with price - it measures momentum, not price level\n",
        "- **Negative correlations**: Some features like Open_Close_Return show near-zero correlation, indicating they capture different information\n",
        "- **Feature redundancy**: High multicollinearity suggests we may not need all price variables in the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q29F0dvdveiT"
      },
      "source": [
        "#### Chart - 15 - Pair Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "outputs": [],
      "source": [
        "# Pair Plot visualization code\n",
        "# Select key features for pair plot (limit to avoid overcrowding)\n",
        "key_features = ['Close', 'Open', 'High', 'Low', 'Price_Range', 'RSI', 'Volume_Proxy']\n",
        "available_key_features = [f for f in key_features if f in df_clean.columns]\n",
        "\n",
        "# Create pair plot with sampling (every 3rd row to avoid overcrowding)\n",
        "sampled_df = df_clean[available_key_features].iloc[::3].copy()\n",
        "\n",
        "# Create pair plot\n",
        "fig = plt.figure(figsize=(16, 16))\n",
        "pair_plot = sns.pairplot(sampled_df, diag_kind='kde', plot_kws={'alpha': 0.6, 's': 30, 'color': 'blue'},\n",
        "                         diag_kws={'alpha': 0.6, 'color': 'red'})\n",
        "\n",
        "# Add title\n",
        "pair_plot.fig.suptitle('Figure 15: Pair Plot of Key Stock Price Features\\nMultivariate Analysis Showing Distributions and Relationships',\n",
        "                      y=1.02, fontsize=16, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Additional detailed pair plot for price variables only\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Subset 1: Price variables\n",
        "price_vars = ['Open', 'High', 'Low', 'Close']\n",
        "price_sampled = df_clean[price_vars].iloc[::5].copy()\n",
        "\n",
        "# Create smaller pair plot for price variables\n",
        "from pandas.plotting import scatter_matrix\n",
        "scatter_matrix(price_sampled, alpha=0.5, figsize=(10, 10), diagonal='kde', ax=axes[0])\n",
        "axes[0].set_title('Figure 15.1: Price Variables Pair Plot\\nShowing Perfect Linear Relationships', fontsize=12, fontweight='bold')\n",
        "\n",
        "# Subset 2: Technical indicators\n",
        "tech_vars = ['Close', 'RSI', 'MACD', 'BB_Width']\n",
        "tech_sampled = df_clean[[v for v in tech_vars if v in df_clean.columns]].iloc[::5].copy()\n",
        "\n",
        "if len(tech_sampled.columns) > 1:\n",
        "    scatter_matrix(tech_sampled, alpha=0.5, figsize=(10, 10), diagonal='kde', ax=axes[1])\n",
        "    axes[1].set_title('Figure 15.2: Technical Indicators Pair Plot\\nShowing Non-Linear Relationships', fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Statistical summary of relationships\n",
        "print(\"\\nðŸ“Š Key Insights from Pair Plot Analysis:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Calculate and display key relationships\n",
        "print(\"\\n1. Linear Relationships (Pearson Correlation):\")\n",
        "for i, feat1 in enumerate(available_key_features[:3]):\n",
        "    for feat2 in available_key_features[i+1:4]:\n",
        "        corr = df_clean[feat1].corr(df_clean[feat2])\n",
        "        strength = \"Very Strong\" if abs(corr) > 0.9 else \"Strong\" if abs(corr) > 0.7 else \"Moderate\" if abs(corr) > 0.5 else \"Weak\"\n",
        "        print(f\"   â€¢ {feat1} vs {feat2}: {corr:.3f} ({strength} correlation)\")\n",
        "\n",
        "print(\"\\n2. Distribution Characteristics:\")\n",
        "for feat in available_key_features:\n",
        "    skew = df_clean[feat].skew()\n",
        "    skew_type = \"Positive (right-skewed)\" if skew > 0.5 else \"Negative (left-skewed)\" if skew < -0.5 else \"Approximately symmetric\"\n",
        "    print(f\"   â€¢ {feat}: Skewness = {skew:.3f} ({skew_type})\")\n",
        "\n",
        "print(\"\\n3. Outlier Detection:\")\n",
        "for feat in available_key_features:\n",
        "    Q1 = df_clean[feat].quantile(0.25)\n",
        "    Q3 = df_clean[feat].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    outliers = df_clean[(df_clean[feat] < Q1 - 1.5*IQR) | (df_clean[feat] > Q3 + 1.5*IQR)]\n",
        "    print(f\"   â€¢ {feat}: {len(outliers)} outliers detected ({len(outliers)/len(df_clean)*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXh0U9oCveiU"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMmPjTByveiU"
      },
      "source": [
        "A pair plot (scatter plot matrix) is the ultimate tool for multivariate analysis because it:\n",
        "- Shows distributions of individual variables (diagonal)\n",
        "- Displays all pairwise relationships (off-diagonal)\n",
        "- Reveals patterns, clusters, and outliers\n",
        "- Helps identify non-linear relationships that correlation coefficients miss\n",
        "- Provides a comprehensive overview of the entire dataset in one visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22aHeOlLveiV"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPQ8RGwHveiV"
      },
      "source": [
        "- **Price variables relationship**: Open, High, Low, Close show perfect linear relationships (straight lines in scatter plots) - expected as they're from same time period\n",
        "- **Distribution shapes**: Close price is right-skewed (most values in lower range, few high values), while RSI is roughly normal (bounded 0-100)\n",
        "- **Non-linear patterns**: RSI vs Price shows a curved pattern - low prices can have any RSI, high prices tend to have moderate RSI\n",
        "- **Clustering**: No clear clusters visible, suggesting continuous price movement rather than distinct regimes\n",
        "- **Outliers**: Visible outliers in high price range (2018 peak) and during crash periods\n",
        "- **Volume_Proxy relationship**: Shows increasing variance with price (heteroscedasticity) - common in financial data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-ATYxFrGrvw"
      },
      "source": [
        "## ***5. Hypothesis Testing***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      },
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7MS06SUHkB-"
      },
      "source": [
        "**Statement 1**: Lagged features significantly improve prediction accuracy over using only current features.\n",
        "\n",
        "**Statement 2**: Ensemble methods significantly outperform single decision trees.\n",
        "\n",
        "**Statement 3**: Model performance varies significantly across different market regimes (bull vs bear)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yEUt7NnHlrM"
      },
      "source": [
        "### Hypothetical Statement - 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      },
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HI9ZP0laH0D-"
      },
      "source": [
        "- H0: Adding lagged features does not improve RÂ² score (Î”RÂ² â‰¤ 0)\n",
        "- H1: Adding lagged features significantly improves RÂ² score (Î”RÂ² > 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I79__PHVH19G"
      },
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "outputs": [],
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from sklearn.feature_selection import f_regression\n",
        "\n",
        "# Create two feature sets\n",
        "current_features = ['Open', 'High', 'Low']\n",
        "lag_features = [f for f in top_features if 'Lag' in f]\n",
        "\n",
        "X_current = df_clean[current_features].values\n",
        "X_with_lags = df_clean[current_features + lag_features].values\n",
        "y_target = df_clean['Close'].values\n",
        "\n",
        "# Scale features\n",
        "X_current_scaled = StandardScaler().fit_transform(X_current)\n",
        "X_lags_scaled = StandardScaler().fit_transform(X_with_lags)\n",
        "\n",
        "# Split data\n",
        "X_curr_train, X_curr_test, y_curr_train, y_curr_test = train_test_split(\n",
        "    X_current_scaled, y_target, test_size=0.2, random_state=42, shuffle=False)\n",
        "X_lag_train, X_lag_test, y_lag_train, y_lag_test = train_test_split(\n",
        "    X_lags_scaled, y_target, test_size=0.2, random_state=42, shuffle=False)\n",
        "\n",
        "# Train models\n",
        "lr_curr = LinearRegression().fit(X_curr_train, y_curr_train)\n",
        "lr_lag = LinearRegression().fit(X_lag_train, y_lag_train)\n",
        "\n",
        "# Get RÂ² scores\n",
        "r2_curr = r2_score(y_curr_test, lr_curr.predict(X_curr_test))\n",
        "r2_lag = r2_score(y_lag_test, lr_lag.predict(X_lag_test))\n",
        "\n",
        "print(f\"RÂ² without lags: {r2_curr:.4f}\")\n",
        "print(f\"RÂ² with lags: {r2_lag:.4f}\")\n",
        "print(f\"Improvement: {(r2_lag - r2_curr)*100:.2f} percentage points\")\n",
        "\n",
        "# F-test for feature significance\n",
        "f_stats, p_values = f_regression(X_lags_scaled, y_target)\n",
        "lag_p_values = pd.DataFrame({\n",
        "    'Feature': current_features + lag_features,\n",
        "    'F_statistic': f_stats,\n",
        "    'P_value': p_values\n",
        "}).sort_values('P_value')\n",
        "\n",
        "print(\"\\nðŸ“Š Feature Significance Test Results:\")\n",
        "print(lag_p_values.head(10).to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou-I18pAyIpj"
      },
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2U0kk00ygSB"
      },
      "source": [
        "F-test for feature significance (ANOVA) to test if each feature significantly contributes to explaining variance in the target."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF3858GYyt-u"
      },
      "source": [
        "##### Why did you choose the specific statistical test?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HO4K0gP5y3B4"
      },
      "source": [
        "F-test is appropriate for linear regression feature selection, testing whether the coefficient is significantly different from zero."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_0_7-oCpUZd"
      },
      "source": [
        "### Hypothetical Statement - 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwyV_J3ipUZe"
      },
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      },
      "source": [
        "- H0: Ensemble methods (Random Forest/XGBoost) do not outperform single Decision Tree\n",
        "- H1: Ensemble methods show significantly better performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yB-zSqbpUZe"
      },
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "outputs": [],
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Perform 5-fold cross-validation\n",
        "dt_cv_scores = cross_val_score(DecisionTreeRegressor(max_depth=5, random_state=42),\n",
        "                               X_train, y_train, cv=5, scoring='r2')\n",
        "rf_cv_scores = cross_val_score(RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "                               X_train, y_train, cv=5, scoring='r2')\n",
        "xgb_cv_scores = cross_val_score(xgb.XGBRegressor(objective='reg:squarederror', random_state=42),\n",
        "                                X_train, y_train, cv=5, scoring='r2')\n",
        "\n",
        "print(\"5-Fold CV RÂ² Scores:\")\n",
        "print(f\"Decision Tree: Mean={dt_cv_scores.mean():.4f} (Â±{dt_cv_scores.std():.4f})\")\n",
        "print(f\"Random Forest: Mean={rf_cv_scores.mean():.4f} (Â±{rf_cv_scores.std():.4f})\")\n",
        "print(f\"XGBoost: Mean={xgb_cv_scores.mean():.4f} (Â±{xgb_cv_scores.std():.4f})\")\n",
        "\n",
        "# Paired t-test between DT and XGBoost\n",
        "from scipy.stats import ttest_rel\n",
        "\n",
        "t_stat, p_value = ttest_rel(dt_cv_scores, xgb_cv_scores)\n",
        "print(f\"\\nPaired t-test (DT vs XGBoost):\")\n",
        "print(f\"t-statistic: {t_stat:.4f}\")\n",
        "print(f\"p-value: {p_value:.4f}\")\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"âœ… Reject H0: Ensemble methods significantly outperform Decision Tree\")\n",
        "else:\n",
        "    print(\"âŒ Fail to reject H0: No significant difference detected\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUvejAfpUZe"
      },
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLDrPz7HpUZf"
      },
      "source": [
        "Paired t-test on cross-validation scores to compare model performance on the same folds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd15vwWVpUZf"
      },
      "source": [
        "##### Why did you choose the specific statistical test?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xOGYyiBpUZf"
      },
      "source": [
        "Paired t-test accounts for the correlation between models evaluated on the same CV splits, providing a more powerful test than independent samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn_IUdTipZyH"
      },
      "source": [
        "### Hypothetical Statement - 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49K5P_iCpZyH"
      },
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gWI5rT9pZyH"
      },
      "source": [
        "- H0: Model performance (MAPE) is equal across market regimes\n",
        "- H1: Model performance differs significantly across regimes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nff-vKELpZyI"
      },
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "outputs": [],
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "df_clean['Market_Regime'] = pd.cut(df_clean['Close'].pct_change(12).rolling(12).mean(),\n",
        "                                   bins=[-np.inf, -0.1, 0.1, np.inf],\n",
        "                                   labels=['Bear', 'Neutral', 'Bull'])\n",
        "\n",
        "# Get predictions for all data\n",
        "# Define final_model as the best performing XGBoost model from previous steps\n",
        "final_model = best_xgb\n",
        "X_all_scaled = scaler.transform(df_clean[top_features].values)\n",
        "y_all_pred = final_model.predict(X_all_scaled)\n",
        "\n",
        "# Calculate MAPE by regime\n",
        "df_results = df_clean.copy()\n",
        "df_results['Predicted'] = y_all_pred\n",
        "df_results['APE'] = np.abs((df_results['Close'] - df_results['Predicted']) / df_results['Close']) * 100\n",
        "\n",
        "regime_performance = df_results.groupby('Market_Regime')['APE'].agg(['mean', 'std', 'count']).round(2)\n",
        "print(\"ðŸ“Š Model Performance by Market Regime:\")\n",
        "print(regime_performance)\n",
        "\n",
        "# ANOVA test\n",
        "from scipy.stats import f_oneway\n",
        "\n",
        "bear_mape = df_results[df_results['Market_Regime'] == 'Bear']['APE'].dropna()\n",
        "neutral_mape = df_results[df_results['Market_Regime'] == 'Neutral']['APE'].dropna()\n",
        "bull_mape = df_results[df_results['Market_Regime'] == 'Bull']['APE'].dropna()\n",
        "\n",
        "f_stat, p_value = f_oneway(bear_mape, neutral_mape, bull_mape)\n",
        "print(f\"\\nANOVA Results:\")\n",
        "print(f\"F-statistic: {f_stat:.4f}\")\n",
        "print(f\"p-value: {p_value:.4f}\")\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"âœ… Reject H0: Model performance differs significantly across market regimes\")\n",
        "else:\n",
        "    print(\"âŒ Fail to reject H0: No significant difference detected\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLW572S8pZyI"
      },
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytWJ8v15pZyI"
      },
      "source": [
        "One-way ANOVA test comparing MAPE across multiple market regimes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWbDXHzopZyI"
      },
      "source": [
        "##### Why did you choose the specific statistical test?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M99G98V6pZyI"
      },
      "source": [
        "ANOVA is appropriate for comparing means across multiple independent groups (bull, bear, neutral markets).Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLjJCtPM0KBk"
      },
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiyOF9F70UgQ"
      },
      "source": [
        "### 1. Handling Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "outputs": [],
      "source": [
        "# Handling Missing Values & Missing Value Imputation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wuGOrhz0itI"
      },
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ixusLtI0pqI"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id1riN9m0vUs"
      },
      "source": [
        "### 2. Handling Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "outputs": [],
      "source": [
        "# Handling Outliers & Outlier treatments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "578E2V7j08f6"
      },
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGZz5OrT1HH-"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89xtkJwZ18nB"
      },
      "source": [
        "### 3. Categorical Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "outputs": [],
      "source": [
        "# Encode your categorical columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67NQN5KX2AMe"
      },
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDaue5h32n_G"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iwf50b-R2tYG"
      },
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMQiZwjn3iu7"
      },
      "source": [
        "#### 1. Expand Contraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "outputs": [],
      "source": [
        "# Expand Contraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVIkgGqN3qsr"
      },
      "source": [
        "#### 2. Lower Casing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "outputs": [],
      "source": [
        "# Lower Casing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkPnILGE3zoT"
      },
      "source": [
        "#### 3. Removing Punctuations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "outputs": [],
      "source": [
        "# Remove Punctuations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hlsf0x5436Go"
      },
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "outputs": [],
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mT9DMSJo4nBL"
      },
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "outputs": [],
      "source": [
        "# Remove Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "outputs": [],
      "source": [
        "# Remove White spaces"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c49ITxTc407N"
      },
      "source": [
        "#### 6. Rephrase Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "outputs": [],
      "source": [
        "# Rephrase Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeJFEK0N496M"
      },
      "source": [
        "#### 7. Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "outputs": [],
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ExmJH0g5HBk"
      },
      "source": [
        "#### 8. Text Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "outputs": [],
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJNqERVU536h"
      },
      "source": [
        "##### Which text normalization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9jKVxE06BC1"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5UmGsbsOxih"
      },
      "source": [
        "#### 9. Part of speech tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "outputs": [],
      "source": [
        "# POS Taging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      },
      "source": [
        "#### 10. Text Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "outputs": [],
      "source": [
        "# Vectorizing Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBMux9mC6MCf"
      },
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "su2EnbCh6UKQ"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      },
      "source": [
        "### 4. Feature Manipulation & Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C74aWNz2AliB"
      },
      "source": [
        "#### 1. Feature Manipulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "outputs": [],
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DejudWSA-a0"
      },
      "source": [
        "#### 2. Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "outputs": [],
      "source": [
        "# Select your features wisely to avoid overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEMng2IbBLp7"
      },
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      },
      "source": [
        "##### Which all features you found important and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGgaEstsBnaf"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNVZ9zx19K6k"
      },
      "source": [
        "### 5. Data Transformation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqoHp30x9hH9"
      },
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "outputs": [],
      "source": [
        "# Transform Your data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMDnDkt2B6du"
      },
      "source": [
        "### 6. Data Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "outputs": [],
      "source": [
        "# Scaling your data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiiVWRdJDDil"
      },
      "source": [
        "##### Which method have you used to scale you data and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UUpS68QDMuG"
      },
      "source": [
        "### 7. Dimesionality Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kexQrXU-DjzY"
      },
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "outputs": [],
      "source": [
        "# DImensionality Reduction (If needed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5CmagL3EC8N"
      },
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKr75IDuEM7t"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhH2vgX9EjGr"
      },
      "source": [
        "### 8. Data Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "outputs": [],
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjKvONjwE8ra"
      },
      "source": [
        "##### What data splitting ratio have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1XJ9OREExlT"
      },
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFOzZv6IFROw"
      },
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeKDIv7pFgcC"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "outputs": [],
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIqpNgepFxVj"
      },
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbet1HwdGDTz"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfCC591jGiD4"
      },
      "source": [
        "## ***7. ML Model Implementation***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      },
      "source": [
        "### ML Model - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "outputs": [],
      "source": [
        "# ML Model - 1 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArJBuiUVfxKd"
      },
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "outputs": [],
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qY1EAkEfxKe"
      },
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "outputs": [],
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      },
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "negyGRa7fxKf"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfvqoZmBfxKf"
      },
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaLui8CcfxKf"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      },
      "source": [
        "### ML Model - 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWYfwnehpsJ1"
      },
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "outputs": [],
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      },
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "outputs": [],
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAih1iBOpsJ2"
      },
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      },
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74yRdG6UpsJ3"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      },
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fze-IPXLpx6K"
      },
      "source": [
        "### ML Model - 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "outputs": [],
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AN1z2sKpx6M"
      },
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "outputs": [],
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PIHJqyupx6M"
      },
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "outputs": [],
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-qAgymDpx6N"
      },
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQMffxkwpx6N"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-hykwinpx6N"
      },
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzVzZC6opx6N"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_CCil-SKHpo"
      },
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHVz9hHDKFms"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBFFvTBNJzUa"
      },
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvGl1hHyA_VK"
      },
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnvVTiIxBL-C"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyNgTHvd2WFk"
      },
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KH5McJBi2d8v"
      },
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "outputs": [],
      "source": [
        "# Save the File"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      },
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "outputs": [],
      "source": [
        "# Load the File and predict unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Kee-DAl2viO"
      },
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCX9965dhzqZ"
      },
      "source": [
        "# **Conclusion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      },
      "source": [
        "Write the conclusion here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIfDvo9L0UH2"
      },
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}